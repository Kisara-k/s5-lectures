## 4. Optimization

## Questions

#### 1. Which of the following statements about turning points in optimization are true?  
A) The first derivative of the function at a turning point is zero.  
B) The second derivative at a turning point determines if it is a maximum or minimum.  
C) A turning point always corresponds to a global optimum.  
D) The function value at a turning point is always greater than at the endpoints.


#### 2. In unconstrained optimization, the design variables are:  
A) Variables that are fixed and cannot be changed.  
B) Variables that can be adjusted freely to optimize the objective function.  
C) Variables that must satisfy equality constraints.  
D) Variables that are restricted by inequality constraints.


#### 3. Which of the following correctly describe the difference between local and global optima?  
A) A local optimum is the best value in the entire domain.  
B) A global optimum is the best value over the entire domain.  
C) A function with multiple optima is called multimodal.  
D) A unimodal function can have multiple local maxima.


#### 4. For the function $f(x)$, if $f'(x) = 0$ and $f''(x) < 0$, then:  
A) $x$ is a local minimum.  
B) $x$ is a local maximum.  
C) $x$ could be a saddle point.  
D) The function is concave down at $x$.


#### 5. The gradient vector $\nabla f(x, y)$ points in the direction of:  
A) The steepest decrease of the function.  
B) The steepest increase of the function.  
C) The direction where the function value remains constant.  
D) The direction perpendicular to the level curves of the function.


#### 6. In the steepest ascent method, which of the following steps are necessary?  
A) Calculate the gradient at the current point.  
B) Move in the opposite direction of the gradient.  
C) Find the step size that maximizes the function along the gradient direction.  
D) Repeat the process until the gradient is zero.


#### 7. Which of the following statements about constrained optimization are true?  
A) Constraints can be equality or inequality conditions on design variables.  
B) The objective function must be linear.  
C) The solution must satisfy all constraints to be feasible.  
D) Constraints restrict the domain of the design variables.


#### 8. In linear programming, the objective function and constraints are:  
A) Always nonlinear.  
B) Always linear.  
C) Sometimes nonlinear, depending on the problem.  
D) Linear inequalities or equalities.


#### 9. Consider the function $f(x, y) = 1 - x^2 - y^2$. Which of the following are true?  
A) The function has a maximum at $(0, 0)$.  
B) The gradient at $(0, 0)$ is zero.  
C) The function is unimodal.  
D) The function has multiple local maxima.


#### 10. When optimizing a function with multiple local maxima, which of the following is true?  
A) The global maximum is always the first local maximum found.  
B) Care must be taken to distinguish between local and global maxima.  
C) The function is called multimodal.  
D) The steepest ascent method guarantees finding the global maximum.


#### 11. Which of the following correctly describe the role of the second derivative in optimization?  
A) It confirms whether a stationary point is a maximum or minimum.  
B) If the second derivative is zero, the point is always a saddle point.  
C) A positive second derivative indicates a local minimum.  
D) A negative second derivative indicates a local maximum.


#### 12. In the context of the Energy Savers Inc. example, which of the following statements are correct?  
A) The constraints represent machine time limitations.  
B) The objective function is nonlinear.  
C) The optimal solution lies at the intersection of constraints.  
D) The feasible region is unbounded.


#### 13. Which of the following are true about the steepest descent method?  
A) It is used to find the maximum of a function.  
B) It moves in the direction opposite to the gradient.  
C) It is the counterpart to the steepest ascent method.  
D) It requires the second derivative of the function.


#### 14. Which of the following statements about the gradient are true?  
A) The gradient is a scalar quantity.  
B) The gradient is a vector of partial derivatives.  
C) The magnitude of the gradient indicates the rate of change of the function.  
D) The gradient is zero at local maxima, minima, and saddle points.


#### 15. In unconstrained optimization, the condition $f'(x) = 0$ is:  
A) Sufficient to guarantee a global optimum.  
B) Necessary to find turning points.  
C) Always guarantees a minimum.  
D) Always guarantees a maximum.


#### 16. Which of the following are characteristics of multimodal functions?  
A) They have multiple local maxima or minima.  
B) They have exactly one global optimum.  
C) Optimization algorithms may get stuck in local optima.  
D) They are always linear functions.


#### 17. In linear programming, the graphical method is most suitable when:  
A) There are many design variables (more than 3).  
B) There are only two design variables.  
C) The objective function is nonlinear.  
D) The constraints are linear inequalities.


#### 18. Which of the following statements about the feasibility region in constrained optimization are true?  
A) It is the set of all points that satisfy the constraints.  
B) The optimal solution must lie inside or on the boundary of the feasibility region.  
C) The feasibility region can be empty if constraints are contradictory.  
D) The feasibility region always includes the origin.


#### 19. Which of the following are true about the process of moving along the gradient direction in optimization?  
A) The slope of the function remains constant along the gradient direction.  
B) The gradient direction changes from point to point.  
C) Taking very large steps along the gradient always improves convergence.  
D) Re-evaluating the gradient after each step is necessary for accuracy.


#### 20. Which of the following statements about the objective function in optimization are true?  
A) It is also called the merit function.  
B) It represents the quantity to be optimized.  
C) It must always be differentiable.  
D) It can be either maximized or minimized depending on the problem.



<br>

## Answers

#### 1. Which of the following statements about turning points in optimization are true?  
A) ✓ The first derivative is zero at turning points.  
B) ✓ The second derivative indicates max or min.  
C) ✗ Turning points can be local, not always global optima.  
D) ✗ Function value at turning point may be less than at endpoints.

**Correct:** A, B


#### 2. In unconstrained optimization, the design variables are:  
A) ✗ They are not fixed; they can be changed.  
B) ✓ They are free to adjust to optimize the function.  
C) ✗ No equality constraints in unconstrained optimization.  
D) ✗ No restrictions in unconstrained optimization.

**Correct:** B


#### 3. Which of the following correctly describe the difference between local and global optima?  
A) ✗ Local optimum is not necessarily best overall.  
B) ✓ Global optimum is best over entire domain.  
C) ✓ Multiple optima mean function is multimodal.  
D) ✗ Unimodal means only one optimum, so no multiple local maxima.

**Correct:** B, C


#### 4. For the function $f(x)$, if $f'(x) = 0$ and $f''(x) < 0$, then:  
A) ✗ Negative second derivative means maximum, not minimum.  
B) ✓ Negative second derivative means local maximum.  
C) ✗ Saddle points have zero second derivative or mixed signs.  
D) ✓ Concave down means negative second derivative.

**Correct:** B, D


#### 5. The gradient vector $\nabla f(x, y)$ points in the direction of:  
A) ✗ Gradient points to increase, not decrease.  
B) ✓ Gradient points to steepest increase.  
C) ✗ Direction of constant function is perpendicular to gradient.  
D) ✓ Gradient is perpendicular to level curves.

**Correct:** B, D


#### 6. In the steepest ascent method, which of the following steps are necessary?  
A) ✓ Calculate gradient at current point.  
B) ✗ Move in gradient direction, not opposite.  
C) ✓ Find step size that maximizes function along gradient.  
D) ✓ Repeat until gradient is zero.

**Correct:** A, C, D


#### 7. Which of the following statements about constrained optimization are true?  
A) ✓ Constraints can be equalities or inequalities.  
B) ✗ Objective function need not be linear.  
C) ✓ Solution must satisfy all constraints to be feasible.  
D) ✓ Constraints restrict design variable domain.

**Correct:** A, C, D


#### 8. In linear programming, the objective function and constraints are:  
A) ✗ They are linear, not nonlinear.  
B) ✓ Both objective and constraints are linear.  
C) ✗ Not sometimes nonlinear; LP requires linearity.  
D) ✓ Constraints are linear inequalities or equalities.

**Correct:** B, D


#### 9. Consider the function $f(x, y) = 1 - x^2 - y^2$. Which of the following are true?  
A) ✓ Maximum at (0,0) since function decreases away.  
B) ✓ Gradient zero at (0,0).  
C) ✓ Function is unimodal with single maximum.  
D) ✗ No multiple local maxima.

**Correct:** A, B, C


#### 10. When optimizing a function with multiple local maxima, which of the following is true?  
A) ✗ Global max is not always first local max found.  
B) ✓ Must distinguish local vs global maxima.  
C) ✓ Function with multiple maxima is multimodal.  
D) ✗ Steepest ascent may get stuck in local max, no guarantee.

**Correct:** B, C


#### 11. Which of the following correctly describe the role of the second derivative in optimization?  
A) ✓ Confirms max or min at stationary points.  
B) ✗ Zero second derivative does not always mean saddle point.  
C) ✓ Positive second derivative indicates local minimum.  
D) ✓ Negative second derivative indicates local maximum.

**Correct:** A, C, D


#### 12. In the context of the Energy Savers Inc. example, which of the following statements are correct?  
A) ✓ Constraints represent machine time limits.  
B) ✗ Objective function is linear, not nonlinear.  
C) ✓ Optimal solution lies at intersection of constraints.  
D) ✗ Feasible region is bounded by constraints.

**Correct:** A, C


#### 13. Which of the following are true about the steepest descent method?  
A) ✗ Used to find minimum, not maximum.  
B) ✓ Moves opposite to gradient.  
C) ✓ Counterpart to steepest ascent.  
D) ✗ Does not require second derivative.

**Correct:** B, C


#### 14. Which of the following statements about the gradient are true?  
A) ✗ Gradient is a vector, not scalar.  
B) ✓ Gradient is vector of partial derivatives.  
C) ✓ Magnitude indicates rate of change.  
D) ✓ Gradient zero at maxima, minima, saddle points.

**Correct:** B, C, D


#### 15. In unconstrained optimization, the condition $f'(x) = 0$ is:  
A) ✗ Not sufficient for global optimum.  
B) ✓ Necessary to find turning points.  
C) ✗ Does not always guarantee minimum.  
D) ✗ Does not always guarantee maximum.

**Correct:** B


#### 16. Which of the following are characteristics of multimodal functions?  
A) ✓ Multiple local maxima or minima.  
B) ✗ May have multiple global optima or one.  
C) ✓ Algorithms may get stuck in local optima.  
D) ✗ Multimodal functions are not always linear.

**Correct:** A, C


#### 17. In linear programming, the graphical method is most suitable when:  
A) ✗ Not suitable for many variables.  
B) ✓ Suitable for two variables.  
C) ✗ Objective function must be linear.  
D) ✓ Constraints are linear inequalities.

**Correct:** B, D


#### 18. Which of the following statements about the feasibility region in constrained optimization are true?  
A) ✓ Feasibility region satisfies all constraints.  
B) ✓ Optimal solution lies inside or on boundary.  
C) ✓ Feasibility region can be empty if constraints conflict.  
D) ✗ Feasibility region does not always include origin.

**Correct:** A, B, C


#### 19. Which of the following are true about the process of moving along the gradient direction in optimization?  
A) ✗ Slope changes along gradient direction.  
B) ✓ Gradient direction changes point to point.  
C) ✗ Large steps can overshoot optimum, harm convergence.  
D) ✓ Re-evaluating gradient after each step is necessary.

**Correct:** B, D


#### 20. Which of the following statements about the objective function in optimization are true?  
A) ✓ Also called merit function.  
B) ✓ Represents quantity to optimize.  
C) ✗ Need not always be differentiable.  
D) ✓ Can be maximized or minimized.

**Correct:** A, B, D