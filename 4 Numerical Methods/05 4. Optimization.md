## 4. Optimization

## Study Notes

### 1. üîç What is Optimization?

Optimization is a fundamental concept in mathematics and engineering that involves finding the best possible value of a function. This "best" value could be either the maximum or minimum of the function, depending on the problem.

- **In simple terms:** Optimization means adjusting certain variables to make a function as large or as small as possible.
- **Geometrically:** The maximum or minimum of a function occurs at points where the graph "turns" ‚Äî these are called turning points ‚Äî or at the boundaries (endpoints) of the domain.
- **Mathematically:** At these turning points, the slope of the function is zero. This means the first derivative of the function, denoted as $f'(x)$, equals zero.
- To determine whether this turning point is a maximum or minimum, we look at the second derivative $f''(x)$:
  - If $f''(x) < 0$, the point is a **maximum** (the curve is concave down).
  - If $f''(x) > 0$, the point is a **minimum** (the curve is concave up).

#### Key Terms:
- **Merit function (or objective function):** The function $f(x)$ that we want to optimize.
- **Design variables:** The variables $x$ that we can change to optimize the merit function.


### 2. üéØ Types of Optimization Problems

Optimization problems can be broadly classified into two categories:

#### Unconstrained Optimization
- There are **no restrictions** on the design variables.
- The goal is simply to find the values of $x$ that maximize or minimize $f(x)$.

#### Constrained Optimization
- There are **restrictions or constraints** on the design variables.
- These constraints can be equalities (e.g., $g_i(x) = 0$) or inequalities (e.g., $h_j(x) \leq 0$).
- The solution must satisfy these constraints while optimizing the objective function.


### 3. üèûÔ∏è Local vs Global Optima

- **Local optimum:** A point where the function is better than all nearby points but not necessarily the best overall.
- **Global optimum:** The absolute best value of the function over the entire domain.
- Some functions have multiple optima (called **multimodal**), meaning they have several local maxima or minima.
- Functions with only one optimum are called **unimodal**.
- In practice, we usually want the **global optimum**, so we must be careful not to mistake a local optimum for the global one.


### 4. ‚¨ÜÔ∏è Multidimensional Unconstrained Optimization and the Gradient

When dealing with functions of multiple variables, such as $f(x, y)$, optimization becomes more complex.

- The **gradient** of a function, denoted $\nabla f(x, y)$, is a vector of partial derivatives:

$$
  \nabla f(x, y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)
$$

- The gradient points in the direction of the **steepest increase** of the function.
- To maximize $f(x, y)$, you move in the direction of the gradient.
- To minimize $f(x, y)$, you move in the opposite direction of the gradient.

#### Example:
For the function $f(x, y) = 1 - x^2 - y^2$, the maximum is at $(0, 0)$ because the gradient is zero there, and the function decreases as you move away from this point.


### 5. üö∂‚Äç‚ôÇÔ∏è Steepest Ascent and Steepest Descent Methods

These are iterative methods used to find maxima or minima of functions when an explicit solution is difficult.

#### Steepest Ascent (for maximization):
- Start at an initial guess $(x_0, y_0)$.
- Calculate the gradient $\nabla f(x_0, y_0)$.
- Move a small step in the direction of the gradient.
- Recalculate the gradient at the new point.
- Repeat until you reach a point where the gradient is zero (a maximum).

#### Steepest Descent (for minimization):
- Same as above, but move in the **opposite** direction of the gradient.

#### Why this works:
- The gradient gives the direction of the fastest increase.
- By taking small steps and recalculating, you "climb" or "descend" the function like walking uphill or downhill.


### 6. üìä Example of Steepest Ascent

Suppose we want to maximize a function starting from $(x_0, y_0) = (-1, 1)$.

- Calculate the gradient at the starting point.
- Find the step size $h_0$ that maximizes the function along the gradient direction.
- Update the point to $(x_1, y_1)$.
- Repeat the process for several iterations until convergence.

This iterative approach gradually leads to the maximum point.


### 7. üõë Constrained Optimization

In many real-world problems, we cannot freely choose any values for the design variables because of constraints.

- Constraints can be **equalities** (e.g., $g_i(x) = 0$) or **inequalities** (e.g., $h_j(x) \leq 0$).
- The goal is to optimize the objective function while satisfying all these constraints.
- This is more complex than unconstrained optimization and often requires specialized methods.


### 8. üìê Linear Programming (LP)

Linear programming is a special type of constrained optimization where:

- The objective function is **linear**:

$$
  z = a_1 x_1 + a_2 x_2 + \cdots + a_n x_n
$$

- The constraints are **linear inequalities** or equalities:

$$
  a_{i1} x_1 + a_{i2} x_2 + \cdots + a_{in} x_n \leq b_i
$$

  or

$$
  a_{j1} x_1 + a_{j2} x_2 + \cdots + a_{jn} x_n \geq b_j
$$


#### Example: Energy Savers, Inc.

- The company produces two types of heaters: S and L.
- Prices: $40 for S, $88 for L.
- Two machines M1 and M2 have time constraints:
  - M1: 2 min per S heater, 8 min per L heater, max 60 min available.
  - M2: 5 min per S heater, 2 min per L heater, max 60 min available.
- Objective: Maximize revenue $z = 40x_1 + 88x_2$, where $x_1$ and $x_2$ are the number of S and L heaters produced per hour.
- Constraints:

$$
  2x_1 + 8x_2 \leq 60 \quad \text{(M1 time)}
$$


$$
  5x_1 + 2x_2 \leq 60 \quad \text{(M2 time)}
$$


#### Solution:
- Graph the constraints and find the feasible region.
- Move the line of constant revenue upwards until it just touches the feasible region.
- The optimal solution is at the intersection of the constraints, here at $(10, 5)$.
- Maximum revenue:

$$
  z_{\max} = 40 \times 10 + 88 \times 5 = 840
$$



### Summary

Optimization is about finding the best values of variables to maximize or minimize a function. It can be unconstrained or constrained, and can involve one or multiple variables. The derivative and gradient are key tools to find turning points. Iterative methods like steepest ascent/descent help find optima when explicit solutions are hard. Linear programming is a powerful technique for optimization problems with linear objectives and constraints, often solved graphically or with algorithms.