## 1.2 Matrix Decomposition

[Study Notes](#study-notes)

[Questions](#questions)



### Key Points

#### 1. üü¶ Diagonally Dominant Matrices  
- A matrix \( A \) is diagonally dominant if for each row \( i \), \( |a_{ii}| \geq \sum_{j \neq i} |a_{ij}| \).  
- \( A \) is strictly diagonally dominant if \( |a_{ii}| > \sum_{j \neq i} |a_{ij}| \) for all \( i \).  
- Strictly diagonally dominant matrices are nonsingular.  
- Gaussian elimination on strictly diagonally dominant matrices can be performed without row or column interchanges and is numerically stable.


#### 2. üü© Positive Definite Matrices  
- A matrix \( A \) is positive definite if it is symmetric and \( x^T A x > 0 \) for all nonzero vectors \( x \).  
- All eigenvalues of a positive definite matrix are positive (non-negative).  
- A symmetric matrix is positive definite if and only if all its leading principal submatrices have positive determinants.  
- Gaussian elimination on positive definite matrices can be done without row interchanges, with all pivots positive, ensuring numerical stability.


#### 3. üìè Vector and Matrix Norms  
- A vector norm \( \|\cdot\| \) satisfies positivity, homogeneity, and the triangle inequality.  
- Common vector norms:  
  - \( l_1 \) norm: sum of absolute values  
  - \( l_2 \) norm: Euclidean norm (square root of sum of squares)  
  - \( l_\infty \) norm: maximum absolute value of components  
- Matrix norms induced by vector norms satisfy \( \|A\| = \sup_{x \neq 0} \frac{\|Ax\|}{\|x\|} \).  
- Induced matrix norms:  
  - \( \|A\|_1 \) = maximum absolute column sum  
  - \( \|A\|_\infty \) = maximum absolute row sum  
  - \( \|A\|_2 \) = spectral radius (maximum absolute eigenvalue)  
- The spectral radius \( \rho(A) \) is the maximum absolute eigenvalue of \( A \).  
- The norms \( \|\cdot\|_1 \), \( \|\cdot\|_2 \), and \( \|\cdot\|_\infty \) are equivalent on \( \mathbb{R}^n \).


#### 4. üî∫ LU Decomposition  
- LU decomposition factors a square matrix \( A \) into \( A = LU \), where \( L \) is lower triangular and \( U \) is upper triangular.  
- \( L \) has zeros above the diagonal; \( U \) has zeros below the diagonal.  
- LU decomposition can be used to solve \( Ax = b \) by solving \( Ly = b \) (forward substitution) and then \( Ux = y \) (backward substitution).  
- If Gaussian elimination can be performed without row interchanges, LU decomposition exists.  
- Doolittle factorization sets diagonal entries of \( L \) to 1; Crout factorization sets diagonal entries of \( U \) to 1.  
- LU decomposition can fail if the pivot element is zero or very small.


#### 5. üîÑ LUP Decomposition (LU with Partial Pivoting)  
- LUP decomposition factors \( A \) as \( PA = LU \), where \( P \) is a permutation matrix, \( L \) is lower triangular with 1's on the diagonal, and \( U \) is upper triangular.  
- \( P \) permutes rows to place the largest pivot element in the top-left position, improving numerical stability.  
- LUP decomposition always exists for any square matrix \( A \).  
- LUP decomposition is more robust than LU decomposition without pivoting and has similar computational cost.  
- To solve \( Ax = b \), solve \( LUx = Pb \) after applying permutation \( P \) to \( b \).


#### 6. ‚ûñ Tridiagonal Systems and Crout Factorization  
- A tridiagonal matrix has nonzero elements only on the main diagonal and the diagonals immediately above and below it.  
- Crout factorization can be applied to tridiagonal matrices, producing \( L \) and \( U \) factors that are also tridiagonal.  
- The factorization proceeds by computing entries of \( L \) and \( U \) using the known entries of \( A \) and previously computed entries.


#### 7. üü© Cholesky Decomposition  
- A matrix \( A \) is positive definite if and only if it can be factored as \( A = LL^T \), where \( L \) is lower triangular with positive diagonal entries.  
- Cholesky decomposition is a special case of LU decomposition for positive definite symmetric matrices.  
- The diagonal entries of \( L \) are computed as \( l_{kk} = \sqrt{a_{kk} - \sum_{j=1}^{k-1} l_{kj}^2} \).  
- Off-diagonal entries are computed as \( l_{ki} = \frac{1}{l_{ii}} \left( a_{ki} - \sum_{j=1}^{i-1} l_{kj} l_{ij} \right) \) for \( i < k \).  
- Cholesky decomposition is more efficient and numerically stable than general LU decomposition for positive definite matrices.



<br>

## Study Notes

### 1. üü¶ Diagonally Dominant Matrices

#### Introduction  
Diagonally dominant matrices are a special class of square matrices where the diagonal elements are "dominant" compared to the other elements in their respective rows. This property is important because it guarantees certain desirable behaviors, such as nonsingularity (invertibility) and numerical stability when solving linear systems.

#### What is Diagonal Dominance?  
Consider an \( n \times n \) matrix \( A = (a_{ij}) \). The matrix \( A \) is **diagonally dominant** if, for every row \( i \), the absolute value of the diagonal element \( |a_{ii}| \) is at least as large as the sum of the absolute values of the other elements in that row:

\[
|a_{ii}| \geq \sum_{j \neq i} |a_{ij}|
\]

If the inequality is strict for every row, i.e.,

\[
|a_{ii}| > \sum_{j \neq i} |a_{ij}|
\]

then \( A \) is called **strictly diagonally dominant**.

#### Why is this important?  
Strict diagonal dominance ensures that the matrix is **nonsingular** (invertible). This means the system \( Ax = b \) has a unique solution. Moreover, Gaussian elimination can be performed without needing to swap rows or columns, which simplifies computations and improves numerical stability.

#### Example  
- Matrix \( A = \begin{bmatrix} 7 & 2 & 0 \\ 3 & 5 & 1 \\ 0 & 5 & 6 \end{bmatrix} \) is strictly diagonally dominant because, for each row, the diagonal element is greater than the sum of the other elements in that row.
- Matrix \( B = \begin{bmatrix} 6 & 4 & 3 \\ 4 & 6 & 5 \\ 3 & 5 & 6 \end{bmatrix} \) is not strictly diagonally dominant because, for example, in the first row, \( |6| < |4| + |3| = 7 \).

#### Theorem  
- **Strictly diagonally dominant matrices are nonsingular.**  
- Gaussian elimination on such matrices is stable and does not require pivoting (row or column interchanges).


### 2. üü© Positive Definite Matrices

#### Introduction  
Positive definite matrices are another important class of matrices, especially in optimization, statistics, and numerical analysis. They have properties that guarantee unique solutions and stability in computations.

#### Definition  
A matrix \( A \) is **positive definite** if:

1. \( A \) is **symmetric** (i.e., \( A = A^T \)), and  
2. For every nonzero vector \( x \in \mathbb{R}^n \), the quadratic form \( x^T A x > 0 \).

This means that when you multiply \( A \) by any nonzero vector \( x \), the result is always a positive scalar.

#### Key Properties  
- All eigenvalues of a positive definite matrix are **positive** (or at least non-negative).
- A symmetric matrix \( A \) is positive definite **if and only if** all its **leading principal submatrices** have positive determinants.  
  - A **leading principal submatrix** is a submatrix formed by taking the first \( k \) rows and columns of \( A \), for \( k = 1, 2, ..., n \).

#### Why does this matter?  
Positive definiteness ensures that certain matrix factorizations (like Cholesky decomposition) exist and are numerically stable. It also guarantees that the system \( Ax = b \) has a unique solution and that Gaussian elimination can be performed without row interchanges, with all pivot elements positive.

#### Example  
Given a symmetric matrix \( A \), you can check positive definiteness by verifying the positivity of determinants of its leading principal submatrices.


### 3. üìè Vector and Matrix Norms

#### Introduction  
Norms provide a way to measure the "size" or "length" of vectors and matrices. They are essential in numerical analysis to understand errors, convergence, and stability.

#### Vector Norms  
A **vector norm** \( \| \cdot \| \) is a function that assigns a non-negative real number to a vector, satisfying:

1. \( \|x\| \geq 0 \) and \( \|x\| = 0 \) if and only if \( x = 0 \).  
2. \( \|\alpha x\| = |\alpha| \|x\| \) for any scalar \( \alpha \).  
3. Triangle inequality: \( \|x + y\| \leq \|x\| + \|y\| \).

Common vector norms include:

- **\( l_1 \) norm**: sum of absolute values of components  
  \[
  \|x\|_1 = \sum_{i=1}^n |x_i|
  \]
- **\( l_2 \) norm (Euclidean norm)**: square root of sum of squares  
  \[
  \|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
  \]
- **\( l_\infty \) norm (max norm)**: maximum absolute value of components  
  \[
  \|x\|_\infty = \max_i |x_i|
  \]

#### Example  
For vector \( v = (-5, 1, 3)^T \):

- \( \|v\|_1 = |{-5}| + |1| + |3| = 9 \)  
- \( \|v\|_2 = \sqrt{(-5)^2 + 1^2 + 3^2} = \sqrt{25 + 1 + 9} = \sqrt{35} \approx 5.92 \)  
- \( \|v\|_\infty = \max(|-5|, |1|, |3|) = 5 \)

#### Matrix Norms  
A **matrix norm** \( \|A\| \) measures the size of a matrix and must satisfy similar properties as vector norms. One important type is the **induced matrix norm**, defined based on a vector norm \( \|\cdot\|_v \):

\[
\|A\| = \sup_{x \neq 0} \frac{\|Ax\|_v}{\|x\|_v}
\]

This means the matrix norm measures the maximum stretching effect of \( A \) on any vector.

#### Common Induced Matrix Norms  
- **\( \|A\|_1 \)**: maximum absolute column sum  
- **\( \|A\|_\infty \)**: maximum absolute row sum  
- **\( \|A\|_2 \)**: related to the largest singular value or spectral radius (largest absolute eigenvalue)

#### Spectral Radius  
The **spectral radius** \( \rho(A) \) of a matrix \( A \) is the maximum absolute value of its eigenvalues. It plays a key role in understanding matrix behavior, especially in iterative methods.


### 4. üî∫ LU Decomposition

#### Introduction  
LU decomposition is a method to factor a square matrix \( A \) into the product of a **lower triangular matrix** \( L \) and an **upper triangular matrix** \( U \):

\[
A = LU
\]

This factorization simplifies solving linear systems, computing determinants, and finding inverses.

#### What are \( L \) and \( U \)?  
- \( L \) is a lower triangular matrix: all entries above the main diagonal are zero.  
- \( U \) is an upper triangular matrix: all entries below the main diagonal are zero.

#### How to use LU decomposition to solve \( Ax = b \)?  
1. Write \( A = LU \), so \( LUx = b \).  
2. Let \( Ux = y \). First solve \( Ly = b \) by **forward substitution** (easy because \( L \) is lower triangular).  
3. Then solve \( Ux = y \) by **backward substitution** (easy because \( U \) is upper triangular).  
4. The solution \( x \) is the solution to the original system.

#### Finding \( L \) and \( U \)  
- If Gaussian elimination can be done without row swaps, \( A \) can be factored as \( LU \).  
- The diagonal entries of \( L \) are often set to 1 (Doolittle factorization). Alternatively, \( U \) can have 1's on the diagonal (Crout factorization).

#### Limitations  
LU decomposition can fail if the pivot element (top-left entry) is zero or very small, leading to numerical instability.


### 5. üîÑ LUP Decomposition (LU with Partial Pivoting)

#### Introduction  
To overcome the limitations of LU decomposition, **LUP decomposition** introduces a permutation matrix \( P \) to reorder rows and improve numerical stability:

\[
PA = LU
\]

where  
- \( P \) is a permutation matrix (rearranges rows),  
- \( L \) is lower triangular with 1's on the diagonal,  
- \( U \) is upper triangular.

#### Why use LUP?  
- It avoids division by zero or very small numbers during elimination.  
- It ensures the decomposition always exists.  
- It is more robust and stable than plain LU decomposition.

#### How does it work?  
- \( P \) permutes rows of \( A \) to bring the largest absolute pivot element to the top-left position.  
- Then LU decomposition is performed on the permuted matrix \( PA \).  
- The system \( Ax = b \) is solved by first applying \( P \) to \( b \), then solving \( LUx = Pb \).

#### Recursive Algorithm  
The LUP decomposition can be computed recursively by:

1. Selecting the row with the largest absolute first element and swapping it to the top.  
2. Factoring the remaining submatrix recursively.  
3. Combining the permutations and factorizations.


### 6. ‚ûñ Tridiagonal Systems and Crout Factorization

#### Introduction  
A **tridiagonal matrix** is a special sparse matrix where nonzero elements appear only on the main diagonal and the diagonals immediately above and below it. These matrices arise in many applications like solving differential equations and modeling physical systems.

#### Crout Factorization for Tridiagonal Matrices  
Because of the sparse structure, the \( L \) and \( U \) factors also have a tridiagonal form, making computations efficient.

The Crout algorithm computes \( L \) and \( U \) such that:

- \( L \) is lower triangular with nonzero diagonal entries,  
- \( U \) is upper triangular with 1's on the diagonal.

The factorization proceeds by solving for the entries of \( L \) and \( U \) using the known entries of \( A \) and previously computed entries of \( L \) and \( U \).


### 7. üü© Cholesky Decomposition

#### Introduction  
Cholesky decomposition is a special factorization for **positive definite symmetric matrices**. It expresses \( A \) as:

\[
A = LL^T
\]

where \( L \) is a lower triangular matrix with positive diagonal entries, and \( L^T \) is its transpose.

#### Why is this useful?  
- It is more efficient than LU decomposition for positive definite matrices.  
- It guarantees numerical stability.  
- It halves the computational effort compared to LU decomposition.

#### How to compute \( L \)?  
The entries of \( L \) can be computed row by row using:

\[
l_{ki} = \frac{1}{l_{ii}} \left( a_{ki} - \sum_{j=1}^{i-1} l_{kj} l_{ij} \right), \quad i < k
\]

\[
l_{kk} = \sqrt{a_{kk} - \sum_{j=1}^{k-1} l_{kj}^2}
\]

#### Summary  
- Cholesky decomposition exists **if and only if** \( A \) is positive definite.  
- It is widely used in optimization, statistics (covariance matrices), and numerical simulations.


### Summary

This lecture covered important matrix classes and decompositions:

- **Diagonally dominant matrices** ensure nonsingularity and stable Gaussian elimination.  
- **Positive definite matrices** have positive eigenvalues and allow Cholesky decomposition.  
- **Vector and matrix norms** provide tools to measure sizes and analyze stability.  
- **LU decomposition** factors matrices into triangular forms for easy solving of linear systems.  
- **LUP decomposition** adds pivoting for numerical stability.  
- **Tridiagonal systems** benefit from specialized Crout factorization.  
- **Cholesky decomposition** efficiently factors positive definite matrices.

Understanding these concepts is fundamental for solving linear systems, analyzing matrix properties, and performing stable numerical computations.



<br>

## Questions

#### 1. Which of the following conditions must hold for a matrix \( A \) to be strictly diagonally dominant?  
A) \( |a_{ii}| > \sum_{j \neq i} |a_{ij}| \) for all \( i \)  
B) \( |a_{ii}| \geq \sum_{j \neq i} |a_{ij}| \) for all \( i \)  
C) \( |a_{ii}| < \sum_{j \neq i} |a_{ij}| \) for some \( i \)  
D) \( |a_{ii}| = \sum_{j \neq i} |a_{ij}| \) for all \( i \)


#### 2. Which statements about strictly diagonally dominant matrices are true?  
A) They are always nonsingular.  
B) Gaussian elimination on them requires row interchanges.  
C) They guarantee a unique solution to \( Ax = b \).  
D) They must be symmetric.


#### 3. A matrix \( A \) is positive definite if and only if:  
A) \( A \) is symmetric and \( x^T A x > 0 \) for all nonzero \( x \).  
B) All eigenvalues of \( A \) are positive.  
C) All leading principal minors of \( A \) have positive determinants.  
D) \( A \) is diagonally dominant.


#### 4. Which of the following are true about the eigenvalues of a positive definite matrix?  
A) They are all real and positive.  
B) They can be complex with positive real parts.  
C) They are all non-negative.  
D) They can be zero.


#### 5. The leading principal submatrix of an \( n \times n \) matrix \( A \) is:  
A) Any \( k \times k \) submatrix of \( A \) for \( k \leq n \).  
B) The submatrix formed by the first \( k \) rows and first \( k \) columns of \( A \).  
C) The submatrix formed by the last \( k \) rows and columns of \( A \).  
D) The diagonal elements of \( A \).


#### 6. Which of the following are properties of a vector norm \( \|\cdot\| \)?  
A) \( \|x\| \geq 0 \) and \( \|x\| = 0 \) if and only if \( x = 0 \).  
B) \( \|x + y\| \leq \|x\| + \|y\| \) (triangle inequality).  
C) \( \|x + y\| = \|x\| + \|y\| \) for all \( x, y \).  
D) \( \|\alpha x\| = |\alpha| \|x\| \) for any scalar \( \alpha \).


#### 7. For the vector \( v = (-5, 1, 3)^T \), which of the following are correct values of its norms?  
A) \( \|v\|_1 = 9 \)  
B) \( \|v\|_2 = \sqrt{35} \)  
C) \( \|v\|_\infty = 3 \)  
D) \( \|v\|_\infty = 5 \)


#### 8. Which of the following statements about matrix norms induced by vector norms are true?  
A) The induced matrix norm is defined as \( \|A\| = \sup_{x \neq 0} \frac{\|Ax\|}{\|x\|} \).  
B) The induced matrix norm depends on the choice of vector norm.  
C) The induced matrix norm is always equal to the spectral radius of \( A \).  
D) The induced matrix norm satisfies the submultiplicative property \( \|AB\| \leq \|A\| \|B\| \).


#### 9. The spectral radius \( \rho(A) \) of a matrix \( A \) is:  
A) The sum of the absolute values of the eigenvalues of \( A \).  
B) The maximum absolute value among the eigenvalues of \( A \).  
C) Always equal to the \( l_2 \) induced norm of \( A \).  
D) The minimum absolute value among the eigenvalues of \( A \).


#### 10. Which of the following are true about LU decomposition?  
A) It factors \( A \) into a product of a lower and an upper triangular matrix.  
B) It always exists for any square matrix \( A \) without pivoting.  
C) It can be used to solve \( Ax = b \) efficiently by forward and backward substitution.  
D) The diagonal entries of \( L \) are always zero.


#### 11. In the Doolittle LU factorization, which of the following is true?  
A) The diagonal entries of \( L \) are all 1.  
B) The diagonal entries of \( U \) are all 1.  
C) \( L \) is upper triangular.  
D) \( U \) is lower triangular.


#### 12. Why is pivoting used in LU decomposition?  
A) To reduce the number of operations.  
B) To avoid division by zero or very small pivot elements.  
C) To ensure the matrix is symmetric.  
D) To improve numerical stability.


#### 13. The LUP decomposition of a matrix \( A \) involves:  
A) A permutation matrix \( P \) that permutes rows of \( A \).  
B) A lower triangular matrix \( L \) with all diagonal entries equal to 1.  
C) An upper triangular matrix \( U \).  
D) A diagonal matrix \( D \).


#### 14. Which of the following statements about LUP decomposition are correct?  
A) It always exists for any square matrix \( A \).  
B) It is unique for every matrix \( A \).  
C) It provides a more robust method than LU decomposition without pivoting.  
D) It requires more computational cost than LU decomposition without pivoting.


#### 15. In solving \( Ax = b \) using LUP decomposition, the system is transformed into:  
A) \( LUx = Pb \)  
B) \( PAx = Pb \)  
C) \( Ax = Pb \)  
D) \( LUx = b \)


#### 16. A tridiagonal matrix has nonzero elements only:  
A) On the main diagonal.  
B) On the main diagonal and the first diagonals above and below it.  
C) On the main diagonal and all diagonals above it.  
D) On the main diagonal and the last column.


#### 17. Which of the following are true about Crout factorization applied to tridiagonal matrices?  
A) Both \( L \) and \( U \) have tridiagonal form.  
B) \( L \) has 1's on the diagonal.  
C) It can be applied only if all \( l_{ii} \neq 0 \).  
D) It is less efficient than general LU decomposition for tridiagonal matrices.


#### 18. Cholesky decomposition requires the matrix \( A \) to be:  
A) Symmetric and positive definite.  
B) Diagonally dominant.  
C) Any square matrix.  
D) Positive semidefinite.


#### 19. In Cholesky decomposition \( A = LL^T \), the matrix \( L \) is:  
A) Lower triangular with positive diagonal entries.  
B) Upper triangular with positive diagonal entries.  
C) Lower triangular with zeros on the diagonal.  
D) Diagonal.


#### 20. Which of the following statements about norms are true?  
A) The \( l_1 \), \( l_2 \), and \( l_\infty \) norms on \( \mathbb{R}^n \) are equivalent.  
B) The Cauchy-Schwarz inequality relates the dot product and the \( l_2 \) norm.  
C) Equivalent norms satisfy inequalities involving positive constants bounding one norm by another.  
D) The spectral radius is a vector norm.



<br>

## Answers

#### 1. Which of the following conditions must hold for a matrix \( A \) to be strictly diagonally dominant?  
A) ‚úì Strict inequality is required for strict diagonal dominance.  
B) ‚úó This defines diagonal dominance but not strict.  
C) ‚úó Opposite of strict diagonal dominance.  
D) ‚úó Equality for all rows means not strictly dominant.

**Correct:** A


#### 2. Which statements about strictly diagonally dominant matrices are true?  
A) ‚úì Strict diagonal dominance guarantees nonsingularity.  
B) ‚úó No row interchanges needed for Gaussian elimination here.  
C) ‚úì Unique solution guaranteed by nonsingularity.  
D) ‚úó Symmetry is not required for diagonal dominance.

**Correct:** A,C


#### 3. A matrix \( A \) is positive definite if and only if:  
A) ‚úì Definition of positive definiteness.  
B) ‚úì Positive eigenvalues are a property of positive definite matrices.  
C) ‚úì Positive determinants of leading principal minors characterize positive definiteness.  
D) ‚úó Diagonal dominance is unrelated to positive definiteness.

**Correct:** A,B,C


#### 4. Which of the following are true about the eigenvalues of a positive definite matrix?  
A) ‚úì Eigenvalues are real and positive due to symmetry and definiteness.  
B) ‚úó Eigenvalues are real, not complex, for symmetric matrices.  
C) ‚úì Non-negative includes positive; positive definite means strictly positive eigenvalues.  
D) ‚úó Zero eigenvalues contradict positive definiteness.

**Correct:** A,C


#### 5. The leading principal submatrix of an \( n \times n \) matrix \( A \) is:  
A) ‚úó Any submatrix is not necessarily leading principal.  
B) ‚úì Correct definition of leading principal submatrix.  
C) ‚úó Last rows/columns do not define leading principal submatrix.  
D) ‚úó Diagonal elements alone are not a submatrix.

**Correct:** B


#### 6. Which of the following are properties of a vector norm \( \|\cdot\| \)?  
A) ‚úì Non-negativity and definiteness are norm properties.  
B) ‚úì Triangle inequality is essential for norms.  
C) ‚úó Equality in triangle inequality is not always true.  
D) ‚úì Homogeneity property of norms.

**Correct:** A,B,D


#### 7. For the vector \( v = (-5, 1, 3)^T \), which of the following are correct values of its norms?  
A) ‚úì Sum of absolute values is 9.  
B) ‚úì Euclidean norm is \(\sqrt{35}\).  
C) ‚úó Max absolute value is 5, not 3.  
D) ‚úì Correct max absolute value.

**Correct:** A,B,D


#### 8. Which of the following statements about matrix norms induced by vector norms are true?  
A) ‚úì Definition of induced matrix norm.  
B) ‚úì Induced norm depends on the underlying vector norm.  
C) ‚úó Induced norm is not always equal to spectral radius.  
D) ‚úì Induced norms satisfy submultiplicative property.

**Correct:** A,B,D


#### 9. The spectral radius \( \rho(A) \) of a matrix \( A \) is:  
A) ‚úó It is not the sum but the max absolute eigenvalue.  
B) ‚úì Correct definition of spectral radius.  
C) ‚úó \( l_2 \) norm is related but not always equal to spectral radius.  
D) ‚úó Spectral radius is max, not min eigenvalue.

**Correct:** B


#### 10. Which of the following are true about LU decomposition?  
A) ‚úì LU factors matrix into lower and upper triangular matrices.  
B) ‚úó LU without pivoting may not exist for all matrices.  
C) ‚úì Forward and backward substitution solve systems efficiently.  
D) ‚úó Diagonal entries of \( L \) are not zero; often 1.

**Correct:** A,C


#### 11. In the Doolittle LU factorization, which of the following is true?  
A) ‚úì \( L \) has 1's on the diagonal in Doolittle.  
B) ‚úó \( U \) does not have 1's on the diagonal in Doolittle.  
C) ‚úó \( L \) is lower triangular, not upper.  
D) ‚úó \( U \) is upper triangular, not lower.

**Correct:** A


#### 12. Why is pivoting used in LU decomposition?  
A) ‚úó Pivoting does not reduce operations but improves stability.  
B) ‚úì Avoids division by zero or very small pivots.  
C) ‚úó Pivoting does not enforce symmetry.  
D) ‚úì Improves numerical stability.

**Correct:** B,D


#### 13. The LUP decomposition of a matrix \( A \) involves:  
A) ‚úì \( P \) permutes rows of \( A \).  
B) ‚úì \( L \) is lower triangular with diagonal entries 1.  
C) ‚úì \( U \) is upper triangular.  
D) ‚úó \( D \) is not part of LUP decomposition.

**Correct:** A,B,C


#### 14. Which of the following statements about LUP decomposition are correct?  
A) ‚úì LUP always exists for any square matrix.  
B) ‚úó LUP decomposition is not unique.  
C) ‚úì More robust than LU without pivoting.  
D) ‚úó Computational cost is approximately the same, not more.

**Correct:** A,C


#### 15. In solving \( Ax = b \) using LUP decomposition, the system is transformed into:  
A) ‚úó \( LUx = Pb \) is correct but not the first step.  
B) ‚úì Multiply both sides by \( P \) to get \( PAx = Pb \).  
C) ‚úó \( Ax = Pb \) is not correct.  
D) ‚úó \( LUx = b \) ignores permutation.

**Correct:** B


#### 16. A tridiagonal matrix has nonzero elements only:  
A) ‚úó Only main diagonal is diagonal, not tridiagonal.  
B) ‚úì Main diagonal and diagonals immediately above and below.  
C) ‚úó Includes more than tridiagonal.  
D) ‚úó Last column is unrelated.

**Correct:** B


#### 17. Which of the following are true about Crout factorization applied to tridiagonal matrices?  
A) ‚úì \( L \) and \( U \) retain tridiagonal structure.  
B) ‚úó \( L \) does not have 1's on diagonal in Crout.  
C) ‚úì Requires \( l_{ii} \neq 0 \) for factorization.  
D) ‚úó Crout is more efficient for tridiagonal than general LU.

**Correct:** A,C


#### 18. Cholesky decomposition requires the matrix \( A \) to be:  
A) ‚úì Must be symmetric and positive definite.  
B) ‚úó Diagonal dominance is not sufficient.  
C) ‚úó Not any square matrix.  
D) ‚úó Positive semidefinite is not enough; must be positive definite.

**Correct:** A


#### 19. In Cholesky decomposition \( A = LL^T \), the matrix \( L \) is:  
A) ‚úì Lower triangular with positive diagonal entries.  
B) ‚úó \( L \) is lower, not upper triangular.  
C) ‚úó Diagonal entries are nonzero, not zero.  
D) ‚úó \( L \) is not diagonal.

**Correct:** A


#### 20. Which of the following statements about norms are true?  
A) ‚úì \( l_1 \), \( l_2 \), and \( l_\infty \) norms are equivalent on finite-dimensional spaces.  
B) ‚úì Cauchy-Schwarz inequality relates dot product and \( l_2 \) norm.  
C) ‚úì Equivalent norms satisfy bounding inequalities with positive constants.  
D) ‚úó Spectral radius is not a vector norm; it relates to eigenvalues.

**Correct:** A,B,C