## 2. Numerical Solutions of Nonlinear Systems

## Study Notes

### 1. üî¢ Understanding Nonlinear Systems of Equations

When we talk about **nonlinear systems of equations**, we mean a set of equations where the unknown variables appear in nonlinear ways‚Äîlike inside trigonometric functions, powers, products, or other nonlinear expressions. Formally, a nonlinear system with *n* equations and *n* unknowns looks like this:


$$
\begin{cases}
f_1(x_1, x_2, ..., x_n) = 0 \\
f_2(x_1, x_2, ..., x_n) = 0 \\
\vdots \\
f_n(x_1, x_2, ..., x_n) = 0
\end{cases}
$$


Here, each $f_i$ is a function that takes an *n*-dimensional vector $\mathbf{x} = (x_1, x_2, ..., x_n)^T$ and outputs a real number. We can think of the whole system as a single vector function:


$$
F(\mathbf{x}) = \begin{pmatrix} f_1(\mathbf{x}) \\ f_2(\mathbf{x}) \\ \vdots \\ f_n(\mathbf{x}) \end{pmatrix}
$$


where $F: \mathbb{R}^n \to \mathbb{R}^n$.

**Why use vector notation?** It simplifies the system into one compact expression $F(\mathbf{x}) = \mathbf{0}$, making it easier to analyze and apply numerical methods.


### 2. üìç Fixed Points in Multidimensional Spaces

A **fixed point** of a function $G: D \subseteq \mathbb{R}^n \to \mathbb{R}^n$ is a point $\mathbf{p} \in D$ such that:


$$
G(\mathbf{p}) = \mathbf{p}
$$


This means applying the function $G$ to $\mathbf{p}$ returns $\mathbf{p}$ itself.

#### Fixed-Point Theorem (Multidimensional Version)

If $G$ is continuous on a closed, bounded domain $D$ (like a box in $\mathbb{R}^n$) and maps $D$ into itself (meaning $G(\mathbf{x}) \in D$ for all $\mathbf{x} \in D$), then **there exists at least one fixed point** in $D$.

Moreover, if the partial derivatives of the components of $G$ are continuous and satisfy a certain contraction condition (the derivatives are bounded by a constant $K < 1$), then:

- The fixed point is **unique**.
- The iterative sequence defined by $\mathbf{x}^{(k)} = G(\mathbf{x}^{(k-1)})$ converges to this fixed point for any starting point $\mathbf{x}^{(0)} \in D$.

This is a powerful result because it guarantees both existence and uniqueness of solutions under these conditions, and it provides a method to approximate the solution by iteration.


### 3. üîÑ Transforming Nonlinear Systems into Fixed-Point Problems

To solve a nonlinear system numerically, one common approach is to rewrite it in **fixed-point form**:


$$
\mathbf{x} = G(\mathbf{x})
$$


This means solving each equation for one variable $x_i$ in terms of the others, creating functions $g_i$ such that:


$$
x_i = g_i(x_1, x_2, ..., x_n)
$$


#### Example

Given the nonlinear system:


$$
\begin{cases}
3x_1 - \cos(x_2 x_3) - 1 = 0 \\
81(x_2 + 0.1)^2 + \sin x_3 + 1.06 = 0 \\
\text{(and a third equation)}
\end{cases}
$$


We solve each equation for $x_i$ to get:


$$
x_1 = \frac{\cos(x_2 x_3) + 1}{3}, \quad x_2 = \text{some function of } x_1, x_3, \quad x_3 = \text{some function of } x_1, x_2
$$


This defines $G(\mathbf{x}) = (g_1(\mathbf{x}), g_2(\mathbf{x}), g_3(\mathbf{x}))^T$.


### 4. ‚úÖ Conditions for Convergence and Continuity

To ensure the fixed-point iteration converges to the true solution, the following must hold:

- The function $G$ must be **continuous** on the domain $D$.
- The domain $D$ must be **invariant** under $G$, meaning $G(\mathbf{x}) \in D$ for all $\mathbf{x} \in D$.
- The **partial derivatives** of the components of $G$ must be bounded by a constant $K < 1$ (this is the contraction condition).

If these conditions are met, the iteration:


$$
\mathbf{x}^{(k)} = G(\mathbf{x}^{(k-1)})
$$


will converge to the unique fixed point $\mathbf{p}$.


### 5. ‚ö° Accelerating Convergence: Gauss-Seidel Method

The basic fixed-point iteration uses the previous iteration vector $\mathbf{x}^{(k-1)}$ to compute the next $\mathbf{x}^{(k)}$. The **Gauss-Seidel method** improves on this by using the most recent updated values within the same iteration step.

For example, when computing $x_2^{(k)}$, instead of using $x_1^{(k-1)}$, use the newly computed $x_1^{(k)}$. This often speeds up convergence because it incorporates the latest information immediately.


### 6. üßÆ Newton's Method for Nonlinear Systems

While fixed-point iteration is useful, it often requires explicitly solving each equation for one variable, which is not always possible. **Newton's method** provides a more general and powerful approach.

#### Idea Behind Newton's Method

Newton's method aims to find the root of $F(\mathbf{x}) = \mathbf{0}$ by iteratively improving guesses $\mathbf{x}^{(k)}$ using the Jacobian matrix $J(\mathbf{x})$, which contains all first-order partial derivatives of $F$:


$$
J(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \cdots & \frac{\partial f_n}{\partial x_n}
\end{bmatrix}
$$


At each iteration, Newton's method solves the linear system:


$$
J(\mathbf{x}^{(k-1)}) \mathbf{y}^{(k-1)} = -F(\mathbf{x}^{(k-1)})
$$


and updates:


$$
\mathbf{x}^{(k)} = \mathbf{x}^{(k-1)} + \mathbf{y}^{(k-1)}
$$


This process is repeated until convergence.

#### Why Newton's Method?

- It has **quadratic convergence**, meaning the error decreases very rapidly once close to the solution.
- It does **not require explicit fixed-point forms**.
- It uses the Jacobian matrix to approximate the nonlinear system locally by a linear one.


### 7. üß© The Jacobian Matrix and Its Role

The **Jacobian matrix** $J(\mathbf{x})$ is central to Newton's method. It generalizes the derivative to multiple variables and functions.

- Each entry $J_{ij}(\mathbf{x})$ is the partial derivative of the $i$-th function with respect to the $j$-th variable.
- The Jacobian tells us how the system changes near a point $\mathbf{x}$.
- Newton's method requires $J(\mathbf{x})$ to be **nonsingular** (invertible) near the solution to solve the linear system.


### 8. üìù Example: Applying Newton's Method

Consider the system:


$$
\begin{cases}
f_1(x_1, x_2, x_3) = 3x_1 - \cos(x_2 x_3) - 1 = 0 \\
f_2(x_1, x_2, x_3) = 81(x_2 + 0.1)^2 + \sin x_3 + 1.06 = 0 \\
f_3(x_1, x_2, x_3) = \text{(some nonlinear function)} = 0
\end{cases}
$$


- Start with an initial guess $\mathbf{x}^{(0)} = (0.1, 0.1, -0.1)^T$.
- Compute $F(\mathbf{x}^{(0)})$ and $J(\mathbf{x}^{(0)})$.
- Solve $J(\mathbf{x}^{(0)}) \mathbf{y}^{(0)} = -F(\mathbf{x}^{(0)})$ for $\mathbf{y}^{(0)}$.
- Update $\mathbf{x}^{(1)} = \mathbf{x}^{(0)} + \mathbf{y}^{(0)}$.
- Repeat until $\mathbf{x}^{(k)}$ converges.

This method converges quickly once the initial guess is close enough to the true solution.


### Summary

- Nonlinear systems can be expressed as vector functions $F(\mathbf{x}) = \mathbf{0}$.
- Fixed points $\mathbf{x} = G(\mathbf{x})$ provide a way to solve nonlinear systems iteratively.
- The Fixed-Point Theorem guarantees existence and uniqueness under certain conditions.
- Gauss-Seidel iteration can speed up convergence by using updated values immediately.
- Newton's method uses the Jacobian matrix to achieve quadratic convergence without needing explicit fixed-point forms.
- The Jacobian matrix is essential for understanding and applying Newton's method.
- Practical application involves iteratively solving linear systems derived from the Jacobian and function values.