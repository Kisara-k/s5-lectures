{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba08e8ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-16T04:06:17.523045Z",
     "iopub.status.busy": "2025-07-16T04:06:17.522500Z",
     "iopub.status.idle": "2025-07-16T04:06:17.571225Z",
     "shell.execute_reply": "2025-07-16T04:06:17.570016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 4. Optimization\n",
       "\n",
       "[Study Notes](#study-notes)\n",
       "\n",
       "[Questions](#questions)\n",
       "\n",
       "\n",
       "\n",
       "### Key Points\n",
       "\n",
       "##### 1. üìà Optimization Basics  \n",
       "- Optimization involves minimizing or maximizing a function.  \n",
       "- At turning points, the first derivative $ f'(x) = 0 $.  \n",
       "- The second derivative test: if $ f''(x) < 0 $, the point is a maximum; if $ f''(x) > 0 $, the point is a minimum.  \n",
       "- The function to optimize is called the **merit function** or **objective function**.  \n",
       "- Variables adjusted to optimize are called **design variables**.\n",
       "\n",
       "##### 2. ‚öñÔ∏è Types of Optimization  \n",
       "- **Unconstrained optimization:** no restrictions on design variables.  \n",
       "- **Constrained optimization:** restrictions (constraints) are placed on design variables, which can be equality or inequality constraints.\n",
       "\n",
       "##### 3. üîÑ Local vs Global Optima  \n",
       "- **Local optimum:** best value in a small neighborhood.  \n",
       "- **Global optimum:** absolute best value over the entire domain.  \n",
       "- Functions with multiple optima are **multimodal**; with one optimum, **unimodal**.\n",
       "\n",
       "##### 4. ‚ûï Multidimensional Optimization and Gradient  \n",
       "- The gradient $ \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right) $ points in the direction of steepest increase.  \n",
       "- To maximize a function, move in the direction of the gradient; to minimize, move opposite to the gradient.\n",
       "\n",
       "##### 5. üö∂‚Äç‚ôÇÔ∏è Steepest Ascent and Descent Methods  \n",
       "- **Steepest ascent:** iterative method moving in the gradient direction to find maxima.  \n",
       "- **Steepest descent:** iterative method moving opposite to the gradient to find minima.  \n",
       "- At each iteration, evaluate the gradient, move a small step, and repeat until convergence.\n",
       "\n",
       "##### 6. üõë Constrained Optimization Formulation  \n",
       "- General constrained minimization:  \n",
       "  $$\n",
       "  \\min f(x) \\quad \\text{subject to} \\quad g_i(x) = 0, \\quad h_j(x) \\leq 0\n",
       "  $$  \n",
       "- Constraints can be equality or inequality.\n",
       "\n",
       "##### 7. üìê Linear Programming (LP)  \n",
       "- Objective function is linear:  \n",
       "  $$\n",
       "  z = a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n\n",
       "  $$  \n",
       "- Constraints are linear inequalities or equalities:  \n",
       "  $$\n",
       "  a_{i1} x_1 + a_{i2} x_2 + \\cdots + a_{in} x_n \\leq b_i\n",
       "  $$  \n",
       "- LP problems can be solved graphically when variables are few.\n",
       "\n",
       "##### 8. üè≠ LP Example: Energy Savers, Inc.  \n",
       "- Objective: maximize revenue $ z = 40x_1 + 88x_2 $ for heaters S and L.  \n",
       "- Constraints:  \n",
       "  $$\n",
       "  2x_1 + 8x_2 \\leq 60 \\quad \\text{(Machine M1 time)}  \n",
       "  $$  \n",
       "  $$\n",
       "  5x_1 + 2x_2 \\leq 60 \\quad \\text{(Machine M2 time)}  \n",
       "  $$  \n",
       "- Optimal solution at intersection $ (10, 5) $ with maximum revenue $ z_{\\max} = 840 $.\n",
       "\n",
       "\n",
       "\n",
       "<br>\n",
       "\n",
       "## Study Notes\n",
       "\n",
       "### 1. üîç What is Optimization?\n",
       "\n",
       "Optimization is a fundamental concept in mathematics and engineering that involves finding the best possible value of a function. This \"best\" value could be either the maximum or minimum of the function, depending on the problem.\n",
       "\n",
       "- **In simple terms:** Optimization means adjusting certain variables to make a function as large or as small as possible.\n",
       "- **Geometrically:** The maximum or minimum of a function occurs at points where the graph \"turns\" ‚Äî these are called turning points ‚Äî or at the boundaries (endpoints) of the domain.\n",
       "- **Mathematically:** At these turning points, the slope of the function is zero. This means the first derivative of the function, denoted as $ f'(x) $, equals zero.\n",
       "- To determine whether this turning point is a maximum or minimum, we look at the second derivative $ f''(x) $:\n",
       "  - If $ f''(x) < 0 $, the point is a **maximum** (the curve is concave down).\n",
       "  - If $ f''(x) > 0 $, the point is a **minimum** (the curve is concave up).\n",
       "\n",
       "##### Key Terms:\n",
       "- **Merit function (or objective function):** The function $ f(x) $ that we want to optimize.\n",
       "- **Design variables:** The variables $ x $ that we can change to optimize the merit function.\n",
       "\n",
       "\n",
       "### 2. üéØ Types of Optimization Problems\n",
       "\n",
       "Optimization problems can be broadly classified into two categories:\n",
       "\n",
       "##### Unconstrained Optimization\n",
       "- There are **no restrictions** on the design variables.\n",
       "- The goal is simply to find the values of $ x $ that maximize or minimize $ f(x) $.\n",
       "\n",
       "##### Constrained Optimization\n",
       "- There are **restrictions or constraints** on the design variables.\n",
       "- These constraints can be equalities (e.g., $ g_i(x) = 0 $) or inequalities (e.g., $ h_j(x) \\leq 0 $).\n",
       "- The solution must satisfy these constraints while optimizing the objective function.\n",
       "\n",
       "\n",
       "### 3. üèûÔ∏è Local vs Global Optima\n",
       "\n",
       "- **Local optimum:** A point where the function is better than all nearby points but not necessarily the best overall.\n",
       "- **Global optimum:** The absolute best value of the function over the entire domain.\n",
       "- Some functions have multiple optima (called **multimodal**), meaning they have several local maxima or minima.\n",
       "- Functions with only one optimum are called **unimodal**.\n",
       "- In practice, we usually want the **global optimum**, so we must be careful not to mistake a local optimum for the global one.\n",
       "\n",
       "\n",
       "### 4. ‚¨ÜÔ∏è Multidimensional Unconstrained Optimization and the Gradient\n",
       "\n",
       "When dealing with functions of multiple variables, such as $ f(x, y) $, optimization becomes more complex.\n",
       "\n",
       "- The **gradient** of a function, denoted $ \\nabla f(x, y) $, is a vector of partial derivatives:\n",
       "  $$\n",
       "  \\nabla f(x, y) = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\n",
       "  $$\n",
       "- The gradient points in the direction of the **steepest increase** of the function.\n",
       "- To maximize $ f(x, y) $, you move in the direction of the gradient.\n",
       "- To minimize $ f(x, y) $, you move in the opposite direction of the gradient.\n",
       "\n",
       "##### Example:\n",
       "For the function $ f(x, y) = 1 - x^2 - y^2 $, the maximum is at $ (0, 0) $ because the gradient is zero there, and the function decreases as you move away from this point.\n",
       "\n",
       "\n",
       "### 5. üö∂‚Äç‚ôÇÔ∏è Steepest Ascent and Steepest Descent Methods\n",
       "\n",
       "These are iterative methods used to find maxima or minima of functions when an explicit solution is difficult.\n",
       "\n",
       "##### Steepest Ascent (for maximization):\n",
       "- Start at an initial guess $ (x_0, y_0) $.\n",
       "- Calculate the gradient $ \\nabla f(x_0, y_0) $.\n",
       "- Move a small step in the direction of the gradient.\n",
       "- Recalculate the gradient at the new point.\n",
       "- Repeat until you reach a point where the gradient is zero (a maximum).\n",
       "\n",
       "##### Steepest Descent (for minimization):\n",
       "- Same as above, but move in the **opposite** direction of the gradient.\n",
       "\n",
       "##### Why this works:\n",
       "- The gradient gives the direction of the fastest increase.\n",
       "- By taking small steps and recalculating, you \"climb\" or \"descend\" the function like walking uphill or downhill.\n",
       "\n",
       "\n",
       "### 6. üìä Example of Steepest Ascent\n",
       "\n",
       "Suppose we want to maximize a function starting from $ (x_0, y_0) = (-1, 1) $.\n",
       "\n",
       "- Calculate the gradient at the starting point.\n",
       "- Find the step size $ h_0 $ that maximizes the function along the gradient direction.\n",
       "- Update the point to $ (x_1, y_1) $.\n",
       "- Repeat the process for several iterations until convergence.\n",
       "\n",
       "This iterative approach gradually leads to the maximum point.\n",
       "\n",
       "\n",
       "### 7. üõë Constrained Optimization\n",
       "\n",
       "In many real-world problems, we cannot freely choose any values for the design variables because of constraints.\n",
       "\n",
       "- Constraints can be **equalities** (e.g., $ g_i(x) = 0 $) or **inequalities** (e.g., $ h_j(x) \\leq 0 $).\n",
       "- The goal is to optimize the objective function while satisfying all these constraints.\n",
       "- This is more complex than unconstrained optimization and often requires specialized methods.\n",
       "\n",
       "\n",
       "### 8. üìê Linear Programming (LP)\n",
       "\n",
       "Linear programming is a special type of constrained optimization where:\n",
       "\n",
       "- The objective function is **linear**:\n",
       "  $$\n",
       "  z = a_1 x_1 + a_2 x_2 + \\cdots + a_n x_n\n",
       "  $$\n",
       "- The constraints are **linear inequalities** or equalities:\n",
       "  $$\n",
       "  a_{i1} x_1 + a_{i2} x_2 + \\cdots + a_{in} x_n \\leq b_i\n",
       "  $$\n",
       "  or\n",
       "  $$\n",
       "  a_{j1} x_1 + a_{j2} x_2 + \\cdots + a_{jn} x_n \\geq b_j\n",
       "  $$\n",
       "\n",
       "##### Example: Energy Savers, Inc.\n",
       "\n",
       "- The company produces two types of heaters: S and L.\n",
       "- Prices: $40 for S, $88 for L.\n",
       "- Two machines M1 and M2 have time constraints:\n",
       "  - M1: 2 min per S heater, 8 min per L heater, max 60 min available.\n",
       "  - M2: 5 min per S heater, 2 min per L heater, max 60 min available.\n",
       "- Objective: Maximize revenue $ z = 40x_1 + 88x_2 $, where $ x_1 $ and $ x_2 $ are the number of S and L heaters produced per hour.\n",
       "- Constraints:\n",
       "  $$\n",
       "  2x_1 + 8x_2 \\leq 60 \\quad \\text{(M1 time)}\n",
       "  $$\n",
       "  $$\n",
       "  5x_1 + 2x_2 \\leq 60 \\quad \\text{(M2 time)}\n",
       "  $$\n",
       "\n",
       "##### Solution:\n",
       "- Graph the constraints and find the feasible region.\n",
       "- Move the line of constant revenue upwards until it just touches the feasible region.\n",
       "- The optimal solution is at the intersection of the constraints, here at $ (10, 5) $.\n",
       "- Maximum revenue:\n",
       "  $$\n",
       "  z_{\\max} = 40 \\times 10 + 88 \\times 5 = 840\n",
       "  $$\n",
       "\n",
       "\n",
       "### Summary\n",
       "\n",
       "Optimization is about finding the best values of variables to maximize or minimize a function. It can be unconstrained or constrained, and can involve one or multiple variables. The derivative and gradient are key tools to find turning points. Iterative methods like steepest ascent/descent help find optima when explicit solutions are hard. Linear programming is a powerful technique for optimization problems with linear objectives and constraints, often solved graphically or with algorithms.\n",
       "\n",
       "\n",
       "\n",
       "<br>\n",
       "\n",
       "## Questions\n",
       "\n",
       "##### 1. Which of the following statements about turning points in optimization are true?  \n",
       "A) The first derivative of the function at a turning point is zero.  \n",
       "B) The second derivative at a turning point determines if it is a maximum or minimum.  \n",
       "C) A turning point always corresponds to a global optimum.  \n",
       "D) The function value at a turning point is always greater than at the endpoints.\n",
       "\n",
       "\n",
       "##### 2. In unconstrained optimization, the design variables are:  \n",
       "A) Variables that are fixed and cannot be changed.  \n",
       "B) Variables that can be adjusted freely to optimize the objective function.  \n",
       "C) Variables that must satisfy equality constraints.  \n",
       "D) Variables that are restricted by inequality constraints.\n",
       "\n",
       "\n",
       "##### 3. Which of the following correctly describe the difference between local and global optima?  \n",
       "A) A local optimum is the best value in the entire domain.  \n",
       "B) A global optimum is the best value over the entire domain.  \n",
       "C) A function with multiple optima is called multimodal.  \n",
       "D) A unimodal function can have multiple local maxima.\n",
       "\n",
       "\n",
       "##### 4. For the function $ f(x) $, if $ f'(x) = 0 $ and $ f''(x) < 0 $, then:  \n",
       "A) $ x $ is a local minimum.  \n",
       "B) $ x $ is a local maximum.  \n",
       "C) $ x $ could be a saddle point.  \n",
       "D) The function is concave down at $ x $.\n",
       "\n",
       "\n",
       "##### 5. The gradient vector $ \\nabla f(x, y) $ points in the direction of:  \n",
       "A) The steepest decrease of the function.  \n",
       "B) The steepest increase of the function.  \n",
       "C) The direction where the function value remains constant.  \n",
       "D) The direction perpendicular to the level curves of the function.\n",
       "\n",
       "\n",
       "##### 6. In the steepest ascent method, which of the following steps are necessary?  \n",
       "A) Calculate the gradient at the current point.  \n",
       "B) Move in the opposite direction of the gradient.  \n",
       "C) Find the step size that maximizes the function along the gradient direction.  \n",
       "D) Repeat the process until the gradient is zero.\n",
       "\n",
       "\n",
       "##### 7. Which of the following statements about constrained optimization are true?  \n",
       "A) Constraints can be equality or inequality conditions on design variables.  \n",
       "B) The objective function must be linear.  \n",
       "C) The solution must satisfy all constraints to be feasible.  \n",
       "D) Constraints restrict the domain of the design variables.\n",
       "\n",
       "\n",
       "##### 8. In linear programming, the objective function and constraints are:  \n",
       "A) Always nonlinear.  \n",
       "B) Always linear.  \n",
       "C) Sometimes nonlinear, depending on the problem.  \n",
       "D) Linear inequalities or equalities.\n",
       "\n",
       "\n",
       "##### 9. Consider the function $ f(x, y) = 1 - x^2 - y^2 $. Which of the following are true?  \n",
       "A) The function has a maximum at $ (0, 0) $.  \n",
       "B) The gradient at $ (0, 0) $ is zero.  \n",
       "C) The function is unimodal.  \n",
       "D) The function has multiple local maxima.\n",
       "\n",
       "\n",
       "##### 10. When optimizing a function with multiple local maxima, which of the following is true?  \n",
       "A) The global maximum is always the first local maximum found.  \n",
       "B) Care must be taken to distinguish between local and global maxima.  \n",
       "C) The function is called multimodal.  \n",
       "D) The steepest ascent method guarantees finding the global maximum.\n",
       "\n",
       "\n",
       "##### 11. Which of the following correctly describe the role of the second derivative in optimization?  \n",
       "A) It confirms whether a stationary point is a maximum or minimum.  \n",
       "B) If the second derivative is zero, the point is always a saddle point.  \n",
       "C) A positive second derivative indicates a local minimum.  \n",
       "D) A negative second derivative indicates a local maximum.\n",
       "\n",
       "\n",
       "##### 12. In the context of the Energy Savers Inc. example, which of the following statements are correct?  \n",
       "A) The constraints represent machine time limitations.  \n",
       "B) The objective function is nonlinear.  \n",
       "C) The optimal solution lies at the intersection of constraints.  \n",
       "D) The feasible region is unbounded.\n",
       "\n",
       "\n",
       "##### 13. Which of the following are true about the steepest descent method?  \n",
       "A) It is used to find the maximum of a function.  \n",
       "B) It moves in the direction opposite to the gradient.  \n",
       "C) It is the counterpart to the steepest ascent method.  \n",
       "D) It requires the second derivative of the function.\n",
       "\n",
       "\n",
       "##### 14. Which of the following statements about the gradient are true?  \n",
       "A) The gradient is a scalar quantity.  \n",
       "B) The gradient is a vector of partial derivatives.  \n",
       "C) The magnitude of the gradient indicates the rate of change of the function.  \n",
       "D) The gradient is zero at local maxima, minima, and saddle points.\n",
       "\n",
       "\n",
       "##### 15. In unconstrained optimization, the condition $ f'(x) = 0 $ is:  \n",
       "A) Sufficient to guarantee a global optimum.  \n",
       "B) Necessary to find turning points.  \n",
       "C) Always guarantees a minimum.  \n",
       "D) Always guarantees a maximum.\n",
       "\n",
       "\n",
       "##### 16. Which of the following are characteristics of multimodal functions?  \n",
       "A) They have multiple local maxima or minima.  \n",
       "B) They have exactly one global optimum.  \n",
       "C) Optimization algorithms may get stuck in local optima.  \n",
       "D) They are always linear functions.\n",
       "\n",
       "\n",
       "##### 17. In linear programming, the graphical method is most suitable when:  \n",
       "A) There are many design variables (more than 3).  \n",
       "B) There are only two design variables.  \n",
       "C) The objective function is nonlinear.  \n",
       "D) The constraints are linear inequalities.\n",
       "\n",
       "\n",
       "##### 18. Which of the following statements about the feasibility region in constrained optimization are true?  \n",
       "A) It is the set of all points that satisfy the constraints.  \n",
       "B) The optimal solution must lie inside or on the boundary of the feasibility region.  \n",
       "C) The feasibility region can be empty if constraints are contradictory.  \n",
       "D) The feasibility region always includes the origin.\n",
       "\n",
       "\n",
       "##### 19. Which of the following are true about the process of moving along the gradient direction in optimization?  \n",
       "A) The slope of the function remains constant along the gradient direction.  \n",
       "B) The gradient direction changes from point to point.  \n",
       "C) Taking very large steps along the gradient always improves convergence.  \n",
       "D) Re-evaluating the gradient after each step is necessary for accuracy.\n",
       "\n",
       "\n",
       "##### 20. Which of the following statements about the objective function in optimization are true?  \n",
       "A) It is also called the merit function.  \n",
       "B) It represents the quantity to be optimized.  \n",
       "C) It must always be differentiable.  \n",
       "D) It can be either maximized or minimized depending on the problem.\n",
       "\n",
       "\n",
       "\n",
       "<br>\n",
       "\n",
       "## Answers\n",
       "\n",
       "##### 1. Which of the following statements about turning points in optimization are true?  \n",
       "A) ‚úì The first derivative is zero at turning points.  \n",
       "B) ‚úì The second derivative indicates max or min.  \n",
       "C) ‚úó Turning points can be local, not always global optima.  \n",
       "D) ‚úó Function value at turning point may be less than at endpoints.\n",
       "\n",
       "**Correct:** A, B\n",
       "\n",
       "\n",
       "##### 2. In unconstrained optimization, the design variables are:  \n",
       "A) ‚úó They are not fixed; they can be changed.  \n",
       "B) ‚úì They are free to adjust to optimize the function.  \n",
       "C) ‚úó No equality constraints in unconstrained optimization.  \n",
       "D) ‚úó No restrictions in unconstrained optimization.\n",
       "\n",
       "**Correct:** B\n",
       "\n",
       "\n",
       "##### 3. Which of the following correctly describe the difference between local and global optima?  \n",
       "A) ‚úó Local optimum is not necessarily best overall.  \n",
       "B) ‚úì Global optimum is best over entire domain.  \n",
       "C) ‚úì Multiple optima mean function is multimodal.  \n",
       "D) ‚úó Unimodal means only one optimum, so no multiple local maxima.\n",
       "\n",
       "**Correct:** B, C\n",
       "\n",
       "\n",
       "##### 4. For the function $ f(x) $, if $ f'(x) = 0 $ and $ f''(x) < 0 $, then:  \n",
       "A) ‚úó Negative second derivative means maximum, not minimum.  \n",
       "B) ‚úì Negative second derivative means local maximum.  \n",
       "C) ‚úó Saddle points have zero second derivative or mixed signs.  \n",
       "D) ‚úì Concave down means negative second derivative.\n",
       "\n",
       "**Correct:** B, D\n",
       "\n",
       "\n",
       "##### 5. The gradient vector $ \\nabla f(x, y) $ points in the direction of:  \n",
       "A) ‚úó Gradient points to increase, not decrease.  \n",
       "B) ‚úì Gradient points to steepest increase.  \n",
       "C) ‚úó Direction of constant function is perpendicular to gradient.  \n",
       "D) ‚úì Gradient is perpendicular to level curves.\n",
       "\n",
       "**Correct:** B, D\n",
       "\n",
       "\n",
       "##### 6. In the steepest ascent method, which of the following steps are necessary?  \n",
       "A) ‚úì Calculate gradient at current point.  \n",
       "B) ‚úó Move in gradient direction, not opposite.  \n",
       "C) ‚úì Find step size that maximizes function along gradient.  \n",
       "D) ‚úì Repeat until gradient is zero.\n",
       "\n",
       "**Correct:** A, C, D\n",
       "\n",
       "\n",
       "##### 7. Which of the following statements about constrained optimization are true?  \n",
       "A) ‚úì Constraints can be equalities or inequalities.  \n",
       "B) ‚úó Objective function need not be linear.  \n",
       "C) ‚úì Solution must satisfy all constraints to be feasible.  \n",
       "D) ‚úì Constraints restrict design variable domain.\n",
       "\n",
       "**Correct:** A, C, D\n",
       "\n",
       "\n",
       "##### 8. In linear programming, the objective function and constraints are:  \n",
       "A) ‚úó They are linear, not nonlinear.  \n",
       "B) ‚úì Both objective and constraints are linear.  \n",
       "C) ‚úó Not sometimes nonlinear; LP requires linearity.  \n",
       "D) ‚úì Constraints are linear inequalities or equalities.\n",
       "\n",
       "**Correct:** B, D\n",
       "\n",
       "\n",
       "##### 9. Consider the function $ f(x, y) = 1 - x^2 - y^2 $. Which of the following are true?  \n",
       "A) ‚úì Maximum at (0,0) since function decreases away.  \n",
       "B) ‚úì Gradient zero at (0,0).  \n",
       "C) ‚úì Function is unimodal with single maximum.  \n",
       "D) ‚úó No multiple local maxima.\n",
       "\n",
       "**Correct:** A, B, C\n",
       "\n",
       "\n",
       "##### 10. When optimizing a function with multiple local maxima, which of the following is true?  \n",
       "A) ‚úó Global max is not always first local max found.  \n",
       "B) ‚úì Must distinguish local vs global maxima.  \n",
       "C) ‚úì Function with multiple maxima is multimodal.  \n",
       "D) ‚úó Steepest ascent may get stuck in local max, no guarantee.\n",
       "\n",
       "**Correct:** B, C\n",
       "\n",
       "\n",
       "##### 11. Which of the following correctly describe the role of the second derivative in optimization?  \n",
       "A) ‚úì Confirms max or min at stationary points.  \n",
       "B) ‚úó Zero second derivative does not always mean saddle point.  \n",
       "C) ‚úì Positive second derivative indicates local minimum.  \n",
       "D) ‚úì Negative second derivative indicates local maximum.\n",
       "\n",
       "**Correct:** A, C, D\n",
       "\n",
       "\n",
       "##### 12. In the context of the Energy Savers Inc. example, which of the following statements are correct?  \n",
       "A) ‚úì Constraints represent machine time limits.  \n",
       "B) ‚úó Objective function is linear, not nonlinear.  \n",
       "C) ‚úì Optimal solution lies at intersection of constraints.  \n",
       "D) ‚úó Feasible region is bounded by constraints.\n",
       "\n",
       "**Correct:** A, C\n",
       "\n",
       "\n",
       "##### 13. Which of the following are true about the steepest descent method?  \n",
       "A) ‚úó Used to find minimum, not maximum.  \n",
       "B) ‚úì Moves opposite to gradient.  \n",
       "C) ‚úì Counterpart to steepest ascent.  \n",
       "D) ‚úó Does not require second derivative.\n",
       "\n",
       "**Correct:** B, C\n",
       "\n",
       "\n",
       "##### 14. Which of the following statements about the gradient are true?  \n",
       "A) ‚úó Gradient is a vector, not scalar.  \n",
       "B) ‚úì Gradient is vector of partial derivatives.  \n",
       "C) ‚úì Magnitude indicates rate of change.  \n",
       "D) ‚úì Gradient zero at maxima, minima, saddle points.\n",
       "\n",
       "**Correct:** B, C, D\n",
       "\n",
       "\n",
       "##### 15. In unconstrained optimization, the condition $ f'(x) = 0 $ is:  \n",
       "A) ‚úó Not sufficient for global optimum.  \n",
       "B) ‚úì Necessary to find turning points.  \n",
       "C) ‚úó Does not always guarantee minimum.  \n",
       "D) ‚úó Does not always guarantee maximum.\n",
       "\n",
       "**Correct:** B\n",
       "\n",
       "\n",
       "##### 16. Which of the following are characteristics of multimodal functions?  \n",
       "A) ‚úì Multiple local maxima or minima.  \n",
       "B) ‚úó May have multiple global optima or one.  \n",
       "C) ‚úì Algorithms may get stuck in local optima.  \n",
       "D) ‚úó Multimodal functions are not always linear.\n",
       "\n",
       "**Correct:** A, C\n",
       "\n",
       "\n",
       "##### 17. In linear programming, the graphical method is most suitable when:  \n",
       "A) ‚úó Not suitable for many variables.  \n",
       "B) ‚úì Suitable for two variables.  \n",
       "C) ‚úó Objective function must be linear.  \n",
       "D) ‚úì Constraints are linear inequalities.\n",
       "\n",
       "**Correct:** B, D\n",
       "\n",
       "\n",
       "##### 18. Which of the following statements about the feasibility region in constrained optimization are true?  \n",
       "A) ‚úì Feasibility region satisfies all constraints.  \n",
       "B) ‚úì Optimal solution lies inside or on boundary.  \n",
       "C) ‚úì Feasibility region can be empty if constraints conflict.  \n",
       "D) ‚úó Feasibility region does not always include origin.\n",
       "\n",
       "**Correct:** A, B, C\n",
       "\n",
       "\n",
       "##### 19. Which of the following are true about the process of moving along the gradient direction in optimization?  \n",
       "A) ‚úó Slope changes along gradient direction.  \n",
       "B) ‚úì Gradient direction changes point to point.  \n",
       "C) ‚úó Large steps can overshoot optimum, harm convergence.  \n",
       "D) ‚úì Re-evaluating gradient after each step is necessary.\n",
       "\n",
       "**Correct:** B, D\n",
       "\n",
       "\n",
       "##### 20. Which of the following statements about the objective function in optimization are true?  \n",
       "A) ‚úì Also called merit function.  \n",
       "B) ‚úì Represents quantity to optimize.  \n",
       "C) ‚úó Need not always be differentiable.  \n",
       "D) ‚úì Can be maximized or minimized.\n",
       "\n",
       "**Correct:** A, B, D"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from source.render import render\n",
    "render(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
