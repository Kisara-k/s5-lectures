## 1.2 Matrix Decomposition

## Study Notes

### 1. üü¶ Diagonally Dominant Matrices

#### Introduction  
Diagonally dominant matrices are a special class of square matrices where the diagonal elements are "dominant" compared to the other elements in their respective rows. This property is important because it guarantees certain desirable behaviors, such as nonsingularity (invertibility) and numerical stability when solving linear systems.

#### What is Diagonal Dominance?  
Consider an $n \times n$ matrix $A = (a_{ij})$. The matrix $A$ is **diagonally dominant** if, for every row $i$, the absolute value of the diagonal element $|a_{ii}|$ is at least as large as the sum of the absolute values of the other elements in that row:


$$
|a_{ii}| \geq \sum_{j \neq i} |a_{ij}|
$$


If the inequality is strict for every row, i.e.,


$$
|a_{ii}| > \sum_{j \neq i} |a_{ij}|
$$


then $A$ is called **strictly diagonally dominant**.

#### Why is this important?  
Strict diagonal dominance ensures that the matrix is **nonsingular** (invertible). This means the system $Ax = b$ has a unique solution. Moreover, Gaussian elimination can be performed without needing to swap rows or columns, which simplifies computations and improves numerical stability.

#### Example  
- Matrix $A = \begin{bmatrix} 7 & 2 & 0 \\ 3 & 5 & 1 \\ 0 & 5 & 6 \end{bmatrix}$ is strictly diagonally dominant because, for each row, the diagonal element is greater than the sum of the other elements in that row.
- Matrix $B = \begin{bmatrix} 6 & 4 & 3 \\ 4 & 6 & 5 \\ 3 & 5 & 6 \end{bmatrix}$ is not strictly diagonally dominant because, for example, in the first row, $|6| < |4| + |3| = 7$.

#### Theorem  
- **Strictly diagonally dominant matrices are nonsingular.**  
- Gaussian elimination on such matrices is stable and does not require pivoting (row or column interchanges).


### 2. üü© Positive Definite Matrices

#### Introduction  
Positive definite matrices are another important class of matrices, especially in optimization, statistics, and numerical analysis. They have properties that guarantee unique solutions and stability in computations.

#### Definition  
A matrix $A$ is **positive definite** if:

1. $A$ is **symmetric** (i.e., $A = A^T$), and  
2. For every nonzero vector $x \in \mathbb{R}^n$, the quadratic form $x^T A x > 0$.

This means that when you multiply $A$ by any nonzero vector $x$, the result is always a positive scalar.

#### Key Properties  
- All eigenvalues of a positive definite matrix are **positive** (or at least non-negative).
- A symmetric matrix $A$ is positive definite **if and only if** all its **leading principal submatrices** have positive determinants.  
  - A **leading principal submatrix** is a submatrix formed by taking the first $k$ rows and columns of $A$, for $k = 1, 2, ..., n$.

#### Why does this matter?  
Positive definiteness ensures that certain matrix factorizations (like Cholesky decomposition) exist and are numerically stable. It also guarantees that the system $Ax = b$ has a unique solution and that Gaussian elimination can be performed without row interchanges, with all pivot elements positive.

#### Example  
Given a symmetric matrix $A$, you can check positive definiteness by verifying the positivity of determinants of its leading principal submatrices.


### 3. üìè Vector and Matrix Norms

#### Introduction  
Norms provide a way to measure the "size" or "length" of vectors and matrices. They are essential in numerical analysis to understand errors, convergence, and stability.

#### Vector Norms  
A **vector norm** $\| \cdot \|$ is a function that assigns a non-negative real number to a vector, satisfying:

1. $\|x\| \geq 0$ and $\|x\| = 0$ if and only if $x = 0$.  
2. $\|\alpha x\| = |\alpha| \|x\|$ for any scalar $\alpha$.  
3. Triangle inequality: $\|x + y\| \leq \|x\| + \|y\|$.

Common vector norms include:

- **$l_1$ norm**: sum of absolute values of components  

$$
  \|x\|_1 = \sum_{i=1}^n |x_i|
$$

- **$l_2$ norm (Euclidean norm)**: square root of sum of squares  

$$
  \|x\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$

- **$l_\infty$ norm (max norm)**: maximum absolute value of components  

$$
  \|x\|_\infty = \max_i |x_i|
$$


#### Example  
For vector $v = (-5, 1, 3)^T$:

- $\|v\|_1 = |{-5}| + |1| + |3| = 9$  
- $\|v\|_2 = \sqrt{(-5)^2 + 1^2 + 3^2} = \sqrt{25 + 1 + 9} = \sqrt{35} \approx 5.92$  
- $\|v\|_\infty = \max(|-5|, |1|, |3|) = 5$

#### Matrix Norms  
A **matrix norm** $\|A\|$ measures the size of a matrix and must satisfy similar properties as vector norms. One important type is the **induced matrix norm**, defined based on a vector norm $\|\cdot\|_v$:


$$
\|A\| = \sup_{x \neq 0} \frac{\|Ax\|_v}{\|x\|_v}
$$


This means the matrix norm measures the maximum stretching effect of $A$ on any vector.

#### Common Induced Matrix Norms  
- **$\|A\|_1$**: maximum absolute column sum  
- **$\|A\|_\infty$**: maximum absolute row sum  
- **$\|A\|_2$**: related to the largest singular value or spectral radius (largest absolute eigenvalue)

#### Spectral Radius  
The **spectral radius** $\rho(A)$ of a matrix $A$ is the maximum absolute value of its eigenvalues. It plays a key role in understanding matrix behavior, especially in iterative methods.


### 4. üî∫ LU Decomposition

#### Introduction  
LU decomposition is a method to factor a square matrix $A$ into the product of a **lower triangular matrix** $L$ and an **upper triangular matrix** $U$:


$$
A = LU
$$


This factorization simplifies solving linear systems, computing determinants, and finding inverses.

#### What are $L$ and $U$?  
- $L$ is a lower triangular matrix: all entries above the main diagonal are zero.  
- $U$ is an upper triangular matrix: all entries below the main diagonal are zero.

#### How to use LU decomposition to solve $Ax = b$?  
1. Write $A = LU$, so $LUx = b$.  
2. Let $Ux = y$. First solve $Ly = b$ by **forward substitution** (easy because $L$ is lower triangular).  
3. Then solve $Ux = y$ by **backward substitution** (easy because $U$ is upper triangular).  
4. The solution $x$ is the solution to the original system.

#### Finding $L$ and $U$  
- If Gaussian elimination can be done without row swaps, $A$ can be factored as $LU$.  
- The diagonal entries of $L$ are often set to 1 (Doolittle factorization). Alternatively, $U$ can have 1's on the diagonal (Crout factorization).

#### Limitations  
LU decomposition can fail if the pivot element (top-left entry) is zero or very small, leading to numerical instability.


### 5. üîÑ LUP Decomposition (LU with Partial Pivoting)

#### Introduction  
To overcome the limitations of LU decomposition, **LUP decomposition** introduces a permutation matrix $P$ to reorder rows and improve numerical stability:


$$
PA = LU
$$


where  
- $P$ is a permutation matrix (rearranges rows),  
- $L$ is lower triangular with 1's on the diagonal,  
- $U$ is upper triangular.

#### Why use LUP?  
- It avoids division by zero or very small numbers during elimination.  
- It ensures the decomposition always exists.  
- It is more robust and stable than plain LU decomposition.

#### How does it work?  
- $P$ permutes rows of $A$ to bring the largest absolute pivot element to the top-left position.  
- Then LU decomposition is performed on the permuted matrix $PA$.  
- The system $Ax = b$ is solved by first applying $P$ to $b$, then solving $LUx = Pb$.

#### Recursive Algorithm  
The LUP decomposition can be computed recursively by:

1. Selecting the row with the largest absolute first element and swapping it to the top.  
2. Factoring the remaining submatrix recursively.  
3. Combining the permutations and factorizations.


### 6. ‚ûñ Tridiagonal Systems and Crout Factorization

#### Introduction  
A **tridiagonal matrix** is a special sparse matrix where nonzero elements appear only on the main diagonal and the diagonals immediately above and below it. These matrices arise in many applications like solving differential equations and modeling physical systems.

#### Crout Factorization for Tridiagonal Matrices  
Because of the sparse structure, the $L$ and $U$ factors also have a tridiagonal form, making computations efficient.

The Crout algorithm computes $L$ and $U$ such that:

- $L$ is lower triangular with nonzero diagonal entries,  
- $U$ is upper triangular with 1's on the diagonal.

The factorization proceeds by solving for the entries of $L$ and $U$ using the known entries of $A$ and previously computed entries of $L$ and $U$.


### 7. üü© Cholesky Decomposition

#### Introduction  
Cholesky decomposition is a special factorization for **positive definite symmetric matrices**. It expresses $A$ as:


$$
A = LL^T
$$


where $L$ is a lower triangular matrix with positive diagonal entries, and $L^T$ is its transpose.

#### Why is this useful?  
- It is more efficient than LU decomposition for positive definite matrices.  
- It guarantees numerical stability.  
- It halves the computational effort compared to LU decomposition.

#### How to compute $L$?  
The entries of $L$ can be computed row by row using:


$$
l_{ki} = \frac{1}{l_{ii}} \left( a_{ki} - \sum_{j=1}^{i-1} l_{kj} l_{ij} \right), \quad i < k
$$



$$
l_{kk} = \sqrt{a_{kk} - \sum_{j=1}^{k-1} l_{kj}^2}
$$


#### Summary  
- Cholesky decomposition exists **if and only if** $A$ is positive definite.  
- It is widely used in optimization, statistics (covariance matrices), and numerical simulations.


### Summary

This lecture covered important matrix classes and decompositions:

- **Diagonally dominant matrices** ensure nonsingularity and stable Gaussian elimination.  
- **Positive definite matrices** have positive eigenvalues and allow Cholesky decomposition.  
- **Vector and matrix norms** provide tools to measure sizes and analyze stability.  
- **LU decomposition** factors matrices into triangular forms for easy solving of linear systems.  
- **LUP decomposition** adds pivoting for numerical stability.  
- **Tridiagonal systems** benefit from specialized Crout factorization.  
- **Cholesky decomposition** efficiently factors positive definite matrices.

Understanding these concepts is fundamental for solving linear systems, analyzing matrix properties, and performing stable numerical computations.