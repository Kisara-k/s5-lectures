[
  {
    "index": 0,
    "title": "Introduction",
    "content": "CS3621: Data Mining Introduction. Slides based on Data Mining: Concepts and Techniques, 3rd ed. Jiawei Han, Micheline Kamber, and Jian Pei University of Illinois at Urbana-Champaign & Simon Fraser University ©2011 Han, Kamber & Pei. All rights reserved. Course Outline Module Code Module Title Data Mining Credits Hours/ Week Lectures Pre - requisites Introduction to Data Science GPA/NGPA GPA. Lab/Assignm ents •ILO1: recall the basic fundamental concepts involved in the process of discovering useful, possibly unexpected, patterns in large data sets •ILO2: select appropriate techniques for data mining, considering the given problem •ILO3: analyse the computational challenges in implementing modern data mining systems. •ILO4: evaluate and implement appropriate solution for a given data mining problem from a wide array of application domains and communicate. Outline Syllabus T1. Introduction to Data Mining and Applications [ILO1] T2. Pattern Mining [ILO 1,2,3] T3. Time Series Mining and Forecasting [ILO 1,2,3] T4. Clustering and Cluster Evaluation [ILO 1,2,3] T5. Outlier Analysis [ILO 1,2,3] T6. Introduction to NoSQL Databases [ILO 1,3] T7. Data Warehouses and Data Lakes [ILO 3,4] T8. Datacubes and Online Analytical Processing [ILO 3,4] T9. Introduction to Information Retrieval [ILO 1,4] T10. Data Mining and Society [ILO 1,4] Final Evaluation Category % on Final Grade Quizzes Assignments Mid Semester Quiz Final Exam Lecture Panel 1. Dr. Thanuja Ambegoda 2. Dr. Sapumal Ahangama 3. Dr. Buddhika Karunaratne Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Why Data Mining? The Explosive Growth of Data: from terabytes to petabytes ◼Data collection and data availability ◼Automated data collection tools, database systems, Web, computerized society ◼Major sources of abundant data ◼Business: Web, e-commerce, transactions, stocks, ... ◼Science: Remote sensing, bioinformatics, scientific simulation, ... ◼Society and everyone: news, digital cameras, YouTube We are drowning in data, but starving for knowledge! “Necessity is the mother of invention”-Data mining-Automated analysis of massive data sets Evolution of Sciences Before 1600, empirical science 1600-1950s, theoretical science Each discipline has grown a theoretical component. Theoretical models often motivate experiments and generalize our understanding. 1950s-1990s, computational science Over the last 50 years, most disciplines have grown a third, computational branch (e.g. empirical, theoretical, and computational ecology, or physics, or linguistics.) Computational Science traditionally meant simulation. It grew out of our inability to find closed-form solutions for complex mathematical models. 1990-now, data science The flood of data from new scientific instruments and simulations The ability to economically store and manage petabytes of data online The Internet and computing Grid that makes all these archives universally accessible Scientific info. management, acquisition, organization, query, and visualization tasks scale almost linearly with data volumes. Data mining is a major new challenge! Jim Gray and Alex Szalay, The World Wide Telescope: An Archetype for Online Science, Comm. ACM, 45(11): 50-54, Nov. 2002 Evolution of Database Technology Data collection, database creation, IMS and network DBMS Relational data model, relational DBMS implementation RDBMS,. advanced data models (extended-relational, OO, deductive, etc.) Application-oriented DBMS (spatial, scientific, engineering, etc.) Data mining, data warehousing, multimedia databases, and Web databases Stream data management and mining Data mining and its applications Web technology (XML, data integration) and global information systems Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary What Is Data Mining? ◼Data mining (knowledge discovery from data) ◼Extraction of interesting (non-trivial, implicit, previously unknown and potentially useful) patterns or knowledge from huge amount of data ◼Data mining: a misnomer? ◼Alternative names ◼Knowledge discovery (mining) in databases (KDD), knowledge extraction, data/pattern analysis, data archeology, data dredging, information harvesting, business intelligence, etc. ◼Watch out: Is everything “data mining”? ◼Simple search and query processing ◼(Deductive) expert systems Knowledge Discovery (KDD) Process This is a view from typical database systems and data warehousing communities Data mining plays an essential role in the knowledge discovery process Data Cleaning Data Integration Databases Data Warehouse Task-relevant Data Selection Data Mining Pattern Evaluation Example: A Web Mining Framework ◼Web mining usually involves ◼Data cleaning ◼Data integration from multiple sources ◼Warehousing the data ◼Data cube construction ◼Data selection for data mining ◼Data mining ◼Presentation of the mining results ◼Patterns and knowledge to be used or stored into knowledge-base Data Mining in Business Intelligence Increasing potential to support business decisions End User Business Analyst Data Analyst DBA Decision Making Data Presentation Visualization Techniques Data Mining Information Discovery Data Exploration Statistical Summary, Querying, and Reporting Data Preprocessing/Integration, Data Warehouses Data Sources Paper, Files, Web documents, Scientific experiments, Database Systems Example: Mining vs. Data Exploration ◼Business intelligence view ◼Warehouse, data cube, reporting but not much mining ◼Business objects vs. data mining tools ◼Supply chain example: tools ◼Data presentation ◼Exploration KDD Process: A Typical View from ML and Statistics Input Data Data Mining Data Pre- Processing Post- Processing This is a view from typical machine learning and statistics communities Data integration Normalization Feature selection Dimension reduction Pattern discovery Association & correlation Classification Clustering Outlier analysis Pattern evaluation Pattern selection Pattern interpretation Pattern visualization Example: Medical Data Mining ◼Health care & medical data mining - often adopted such a view in statistics and machine learning ◼Preprocessing of the data (including feature extraction and dimension reduction) ◼Classification or/and clustering processes ◼Post-processing for presentation Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Multi-Dimensional View of Data Mining Data to be mined ◼Database data (extended-relational, object-oriented, heterogeneous, legacy), data warehouse, transactional data, stream, spatiotemporal, time-series, sequence, text and web, multi-media, graphs & social and information networks Knowledge to be mined (or: Data mining functions) ◼Characterization, discrimination, association, classification, clustering, trend/deviation, outlier analysis, etc. ◼Descriptive vs. predictive data mining ◼Multiple/integrated functions and mining at multiple levels Techniques utilized ◼Data-intensive, data warehouse (OLAP), machine learning, statistics, pattern recognition, visualization, high-performance, etc. Applications adapted ◼Retail, telecommunication, banking, fraud analysis, bio-data mining, stock market analysis, text mining, Web mining, etc. Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Data Mining: On What Kinds of Data? Database-oriented data sets and applications Relational database, data warehouse, transactional database Advanced data sets and advanced applications Data streams and sensor data Time-series data, temporal data, sequence data (incl. bio-sequences) Structure data, graphs, social networks and multi-linked data Object-relational databases Heterogeneous databases and legacy databases Spatial data and spatiotemporal data Multimedia database Text databases The World-Wide Web Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Data Mining Function: (1) Generalization ◼Information integration and data warehouse construction ◼Data cleaning, transformation, integration, and multidimensional data model ◼Data cube technology ◼Scalable methods for computing (i.e., materializing) multidimensional aggregates ◼OLAP (online analytical processing) ◼Multidimensional concept description: Characterization and discrimination ◼Generalize, summarize, and contrast data characteristics, e.g., dry vs. wet region Data Mining Function: (2) Association and Correlation Analysis ◼Frequent patterns (or frequent itemsets) ◼What items are frequently purchased together in your Walmart? ◼Association, correlation vs. causality ◼A typical association rule ◼Diaper → Beer [0.5%, 75%] (support, confidence) ◼Are strongly associated items also strongly correlated? ◼How to mine such patterns and rules efficiently in large datasets? ◼How to use such patterns for classification, clustering, and other applications? Data Mining Function: (3) Classification Classification and label prediction ◼Construct models (functions) based on some training examples ◼Describe and distinguish classes or concepts for future prediction ◼E.g., classify countries based on (climate), or classify cars based on (gas mileage) ◼Predict some unknown class labels Typical methods ◼Decision trees, naïve Bayesian classification, support vector machines, neural networks, rule-based classification, pattern- based classification, logistic regression, ... Typical applications: ◼Credit card fraud detection, direct marketing, classifying stars, diseases, web-pages, ... blue Yellow Data Mining Function: (4) Cluster Analysis ◼Unsupervised learning (i.e., Class label is unknown) ◼Group data to form new categories (i.e., clusters), e.g., cluster houses to find distribution patterns ◼Principle: Maximizing intra-class similarity & minimizing interclass similarity ◼Many methods and applications Data Mining Function: (5) Outlier Analysis Outlier analysis ◼Outlier: A data object that does not comply with the general behavior of the data ◼Noise or exception? ― One person's garbage could be another person's treasure ◼Methods: by product of clustering or regression analysis, ... ◼Useful in fraud detection, rare events analysis Time and Ordering: Sequential Pattern, Trend and Evolution Analysis ◼Sequence, trend and evolution analysis ◼Trend, time-series, and deviation analysis: e.g., regression and value prediction ◼Sequential pattern mining ◼e.g., first buy digital camera, then buy large SD memory cards ◼Periodicity analysis ◼Motifs and biological sequence analysis ◼Approximate and consecutive motifs ◼Similarity-based analysis ◼Mining data streams ◼Ordered, time-varying, potentially infinite, data streams Structure and Network Analysis Graph mining ◼Finding frequent subgraphs (e.g., chemical compounds), trees (XML), substructures (web fragments) Information network analysis ◼Social networks: actors (objects, nodes) and relationships (edges) ◼e.g., author networks in CS, terrorist networks ◼Multiple heterogeneous networks ◼A person could be multiple information networks: friends, family, classmates, ... ◼Links carry a lot of semantic information: Link mining Web mining ◼Web is a big information network: from PageRank to Google ◼Analysis of Web information networks ◼Web community discovery, opinion mining, usage mining, ... Evaluation of Knowledge ◼Are all mined knowledge interesting? ◼One can mine tremendous amount of “patterns” and knowledge ◼Some may fit only certain dimension space (time, location, ...) ◼Some may not be representative, may be transient, ... ◼Evaluation of mined knowledge → directly mine only interesting knowledge? ◼Descriptive vs. predictive ◼Coverage ◼Typicality vs. novelty ◼Accuracy ◼Timeliness Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Data Mining: Confluence of Multiple Disciplines Data Mining Machine Learning Statistics Applications Algorithm Pattern Recognition High-Performance Computing Visualization Database Technology Why Confluence of Multiple Disciplines? ◼Tremendous amount of data ◼Algorithms must be highly scalable to handle such as tera-bytes of data ◼High-dimensionality of data ◼Micro-array may have tens of thousands of dimensions ◼High complexity of data ◼Data streams and sensor data ◼Time-series data, temporal data, sequence data ◼Structure data, graphs, social networks and multi-linked data ◼Heterogeneous databases and legacy databases ◼Spatial, spatiotemporal, multimedia, text and Web data ◼Software programs, scientific simulations ◼New and sophisticated applications Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Applications of Data Mining Web page analysis: from web page classification, clustering to PageRank & HITS algorithms Collaborative analysis & recommender systems Basket data analysis to targeted marketing Biological and medical data analysis: classification, cluster analysis (microarray data analysis), biological sequence analysis, biological network analysis Data mining and software engineering (e.g., IEEE Computer, Aug. 2009 issue) From major dedicated data mining systems/tools (e.g., SAS, MS SQL-. Server Analysis Manager, Oracle Data Mining Tools) to invisible data mining Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Major Issues in Data Mining (1) Mining Methodology ◼Mining various and new kinds of knowledge ◼Mining knowledge in multi-dimensional space ◼Data mining: An interdisciplinary effort ◼Boosting the power of discovery in a networked environment ◼Handling noise, uncertainty, and incompleteness of data ◼Pattern evaluation and pattern- or constraint-guided mining User Interaction ◼Interactive mining ◼Incorporation of background knowledge ◼Presentation and visualization of data mining results Major Issues in Data Mining (2) Efficiency and Scalability ◼Efficiency and scalability of data mining algorithms ◼Parallel, distributed, stream, and incremental mining methods Diversity of data types ◼Handling complex types of data ◼Mining dynamic, networked, and global data repositories Data mining and society ◼Social impacts of data mining ◼Privacy-preserving data mining ◼Invisible data mining Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary A Brief History of Data Mining Society 1989 IJCAI. Workshop on Knowledge Discovery in Databases Knowledge Discovery in Databases (G. Piatetsky-Shapiro and W. Frawley, 1991-1994 Workshops on Knowledge Discovery in Databases Advances in Knowledge Discovery and Data Mining (U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, 1996) 1995-1998 International Conferences on Knowledge Discovery in Databases and Data Mining (KDD'95-98) Journal of Data Mining and Knowledge Discovery (1997) ACM SIGKDD. conferences since 1998 and SIGKDD. Explorations More conferences on data mining PAKDD. ( 1997), PKDD (1997), SIAM-Data Mining (2001), (IEEE) ICDM. ( 2001), etc. ACM Transactions on KDD starting in 2007 Conferences and Journals on Data Mining KDD Conferences ACM SIGKDD. Int. Conf. on Knowledge Discovery in Databases and Data Mining (KDD) SIAM. Data Mining Conf. (SDM) (IEEE). Int. Conf. on Data Mining (ICDM) European Conf. on Machine Learning and Principles and practices of Knowledge Discovery and Data Mining (ECML-PKDD). Pacific-Asia Conf. on Knowledge Discovery and Data Mining (PAKDD). Int. Conf. on Web Search and Data Mining (WSDM) Other related conferences DB conferences: ACM SIGMOD, VLDB, ICDE, EDBT, ICDT. , ... Web and IR conferences: WWW, SIGIR, WSDM ML. conferences: ICML, NIPS PR. conferences: CVPR, Journals Data Mining and Knowledge Discovery (DAMI or DMKD) IEEE. Trans. On Knowledge and Data Eng. (TKDE) KDD. Explorations ACM Trans. on KDD Where to Find References? DBLP, CiteSeer, Google Data mining and KDD (SIGKDD: CDROM). Conferences: ACM-SIGKDD, IEEE-ICDM, SIAM-DM, PKDD, PAKDD,. etc. Journal: Data Mining and Knowledge Discovery, KDD Explorations, ACM TKDD. Database systems (SIGMOD: ACM SIGMOD. Anthology-CD ROM). Conferences: ACM-SIGMOD, ACM-PODS, VLDB, IEEE-ICDE, EDBT, ICDT, DASFAA. Journals: IEEE-TKDE, ACM-TODS/TOIS, JIIS,. J. ACM, VLDB. J., Info. Sys., etc. AI & Machine Learning Conferences: Machine learning (ML), AAAI, IJCAI, COLT. ( Learning Theory), CVPR, NIPS,. etc. Journals: Machine Learning, Artificial Intelligence, Knowledge and Information Systems, IEEE-PAMI,. etc. Web and IR Conferences: SIGIR, WWW, CIKM,. etc. Journals: WWW: Internet and Web Information Systems, Statistics Conferences: Joint Stat. Meeting, etc. Journals: Annals of statistics, etc. Visualization Conference proceedings: CHI, ACM-. SIGGraph, etc. Journals: IEEE Trans. visualization and computer graphics, etc. Chapter 1. Introduction Why Data Mining? What Is Data Mining? A Multi-Dimensional View of Data Mining What Kind of Data Can Be Mined? What Kinds of Patterns Can Be Mined? What Technology Are Used? What Kind of Applications Are Targeted? Major Issues in Data Mining A Brief History of Data Mining and Data Mining Society Summary Summary Data mining: Discovering interesting patterns and knowledge from massive amount of data A natural evolution of database technology, in great demand, with wide applications A KDD process includes data cleaning, data integration, data selection, transformation, data mining, pattern evaluation, and knowledge presentation Mining can be performed in a variety of data Data mining functionalities: characterization, discrimination, association, classification, clustering, outlier and trend analysis, etc. Data mining technologies and applications Major issues in data mining"
  },
  {
    "index": 1,
    "title": "1. Concepts and Techniques",
    "content": "Data Mining: Concepts and Techniques. - Chapter 2 - Jiawei Han, Micheline Kamber, and Jian Pei University of Illinois at Urbana-Champaign Simon Fraser University ©2011 Han, Kamber, and Pei. All rights reserved. Chapter 2: Getting to Know Your Data ◼Data Objects and Attribute Types ◼Basic Statistical Descriptions of Data ◼Data Visualization ◼Measuring Data Similarity and Dissimilarity ◼Summary Types of Data Sets Record Relational records Data matrix, e.g., numerical matrix, crosstabs Document data: text documents: term- frequency vector Transaction data Graph and network World Wide Web Social or information networks Molecular Structures Ordered Video data: sequence of images Temporal data: time-series Sequential Data: transaction sequences Genetic sequence data Spatial, image and multimedia: Spatial data: maps Image data: Video data: Document 1 season timeout lost game score ball pla coach team Document 2 Document 3 TID Items Bread, Coke, Milk Beer, Bread Beer, Coke, Diaper, Milk Beer, Bread, Diaper, Milk Coke, Diaper, Milk Important Characteristics of Structured Data ◼Dimensionality ◼Curse of dimensionality ◼Sparsity ◼Only presence counts ◼Resolution ◼Patterns depend on the scale ◼Distribution ◼Centrality and dispersion Data Objects ◼Data sets are made up of data objects. ◼A data object represents an entity. ◼Examples: ◼sales database: customers, store items, sales ◼medical database: patients, treatments ◼university database: students, professors, courses ◼Also called samples , examples, instances, data points, objects, tuples. ◼Data objects are described by attributes. ◼Database rows -> data objects; columns ->attributes. Attributes ◼Attribute (or dimensions, features, variables): a data field, representing a characteristic or feature of a data object. ◼E.g., customer _ID, name, address ◼Types: ◼Nominal ◼Binary ◼Numeric: quantitative ◼Interval-scaled ◼Ratio-scaled Attribute Types ◼Nominal: categories, states, or “names of things” Hair_color = {auburn, black, blond, brown, grey, red, white} marital status, occupation, ID numbers, zip codes ◼Binary Nominal attribute with only 2 states (0 and 1) Symmetric binary: both outcomes equally important e.g., gender Asymmetric binary: outcomes not equally important. e.g., medical test (positive vs. negative) Convention: assign 1 to most important outcome (e.g., HIV positive) ◼Ordinal Values have a meaningful order (ranking) but magnitude between successive values is not known. Size = {small, medium, large}, grades, army rankings Numeric Attribute Types ◼Quantity (integer or real-valued) ◼Interval Measured on a scale of equal-sized units Values have order E.g., temperature in C˚or F˚, calendar dates No true zero-point ◼Ratio Inherent zero-point We can speak of values as being an order of magnitude larger than the unit of measurement (10 K˚ is twice as high as 5 K˚). e.g., temperature in Kelvin, length, counts, monetary quantities Discrete vs. Continuous Attributes ◼Discrete Attribute ◼Has only a finite or countably infinite set of values ◼E.g., zip codes, profession, or the set of words in a collection of documents ◼Sometimes, represented as integer variables ◼Note: Binary attributes are a special case of discrete attributes ◼Continuous Attribute ◼Has real numbers as attribute values ◼E.g., temperature, height, or weight ◼Practically, real values can only be measured and represented using a finite number of digits ◼Continuous attributes are typically represented as floating-point variables Chapter 2: Getting to Know Your Data ◼Data Objects and Attribute Types ◼Basic Statistical Descriptions of Data ◼Data Visualization ◼Measuring Data Similarity and Dissimilarity ◼Summary Basic Statistical Descriptions of Data ◼Motivation ◼To better understand the data: central tendency, variation and spread ◼Data dispersion characteristics ◼median, max, min, quantiles, outliers, variance, etc. ◼Numerical dimensions correspond to sorted intervals ◼Data dispersion: analyzed with multiple granularities of precision ◼Boxplot or quantile analysis on sorted intervals ◼Dispersion analysis on computed measures ◼Folding measures into numerical dimensions ◼Boxplot or quantile analysis on the transformed cube Measuring the Central Tendency Mean (algebraic measure) (sample vs. population): Note: n is sample size and N is population size. ◼Weighted arithmetic mean: ◼Trimmed mean: chopping extreme values Median: ◼Middle value if odd number of values, or average of the middle two values otherwise ◼Estimated by interpolation (for grouped data): Mode ◼Value that occurs most frequently in the data ◼Unimodal, bimodal, trimodal ◼Empirical formula: width freq freq median median median mean mode mean Symmetric vs. Skewed Data ◼Median, mean and mode of symmetric, positively and negatively skewed data positively skewed negatively skewed symmetric Measuring the Dispersion of Data Quartiles, outliers and boxplots ◼Quartiles: Q1 (25th percentile), Q3 (75th percentile) ◼Inter-quartile range: IQR = Q3 - Q1 ◼Five number summary: min, Q1, median, Q3, max ◼Boxplot: ends of the box are the quartiles; median is marked; add whiskers, and plot outliers individually ◼Outlier: usually, a value higher/lower than 1.5 x IQR Variance and standard deviation (sample: s, population: σ) ◼Variance: (algebraic, scalable computation) ◼Standard deviation s (or σ) is the square root of variance s2 (or σ2) Boxplot Analysis Five-number summary of a distribution ◼Minimum, Q1, Median, Q3, Maximum Boxplot ◼Data is represented with a box ◼The ends of the box are at the first and third quartiles, i.e., the height of the box is IQR ◼The median is marked by a line within the box ◼Whiskers: two lines outside the box extended to Minimum and Maximum ◼Outliers: points beyond a specified outlier threshold, plotted individually Visualization of Data Dispersion: 3-D Boxplots Properties of Normal Distribution Curve ◼The normal (distribution) curve ◼From μ-σ to μ+σ: contains about 68% of the measurements (μ: mean, σ: standard deviation) ◼ From μ-2σ to μ+2σ: contains about 95% of it ◼From μ-3σ to μ+3σ: contains about 99.7% of it Graphic Displays of Basic Statistical Descriptions ◼Boxplot: graphic display of five-number summary ◼Histogram: x-axis are values, y-axis repres. frequencies ◼Quantile plot: each value xi is paired with fi indicating that approximately 100 fi % of data are  xi ◼Quantile-quantile (q-q) plot: graphs the quantiles of one univariant distribution against the corresponding quantiles of another ◼Scatter plot: each pair of values is a pair of coordinates and plotted as points in the plane Histogram Analysis Histogram: Graph display of tabulated frequencies, shown as bars It shows what proportion of cases fall into each of several categories Differs from a bar chart in that it is the area of the bar that denotes the value, not the height as in bar charts, a crucial distinction when the categories are not of uniform width The categories are usually specified as non-overlapping intervals of some variable. The categories (bars) must be adjacent Histograms Often Tell More than Boxplots ◼The two histograms shown in the left may have the same boxplot representation ◼The same values for: min, Q1, median, Q3, max ◼But they have rather different data distributions Quantile Plot ◼Displays all of the data (allowing the user to assess both the overall behavior and unusual occurrences) ◼Plots quantile information ◼For a data xi data sorted in increasing order, fi indicates that approximately 100 fi% of the data are below or equal to the value xi Quantile-Quantile (Q-Q) Plot Graphs the quantiles of one univariate distribution against the corresponding quantiles of another View: Is there is a shift in going from one distribution to another? Example shows unit price of items sold at Branch 1 vs. Branch 2 for each quantile. Unit prices of items sold at Branch 1 tend to be lower than those at Branch 2. Scatter plot ◼Provides a first look at bivariate data to see clusters of points, outliers, etc ◼Each pair of values is treated as a pair of coordinates and plotted as points in the plane Positively and Negatively Correlated Data The left half fragment is positively correlated The right half is negative correlated Uncorrelated Data Chapter 2: Getting to Know Your Data ◼Data Objects and Attribute Types ◼Basic Statistical Descriptions of Data ◼Data Visualization ◼Measuring Data Similarity and Dissimilarity ◼Summary Data Visualization Why data visualization? Gain insight into an information space by mapping data onto graphical primitives Provide qualitative overview of large data sets Search for patterns, trends, structure, irregularities, relationships among data Help find interesting regions and suitable parameters for further quantitative analysis Provide a visual proof of computer representations derived Categorization of visualization methods: Pixel-oriented visualization techniques Geometric projection visualization techniques Icon-based visualization techniques Hierarchical visualization techniques Visualizing complex data and relations Pixel-Oriented Visualization Techniques For a data set of m dimensions, create m windows on the screen, one for each dimension The m dimension values of a record are mapped to m pixels at the corresponding positions in the windows The colors of the pixels reflect the corresponding values (a) Income (b) Credit Limit (c) transaction volume (d) age Laying Out Pixels in Circle Segments To save space and show the connections among multiple dimensions, space filling is often done in a circle segment (a) Representing a data record in circle segment (b) Laying out pixels in circle segment Geometric Projection Visualization Techniques ◼Visualization of geometric transformations and projections of the data ◼Methods ◼Direct visualization ◼Scatterplot and scatterplot matrices ◼Landscapes ◼Projection pursuit technique: Help users find meaningful projections of multidimensional data ◼Prosection views ◼Hyperslice ◼Parallel coordinates Direct Data Visualization Ribbons with Twists Based on Vorticity Scatterplot Matrices Matrix of scatterplots (x-y-diagrams) of the k-dim. data [total of (k2/2-k) scatterplots] Used by ermission of M. Ward, Worcester Polytechnic Institute news articles visualized as a landscape Used by permission of B. Wright, Visible Decisions Inc. Landscapes Visualization of the data as perspective landscape The data needs to be transformed into a (possibly artificial) 2D spatial representation which preserves the characteristics of the data Attr. 1 Attr. 2 Attr. k Attr. 3 Parallel Coordinates n equidistant axes which are parallel to one of the screen axes and correspond to the attributes The axes are scaled to the [minimum, maximum]: range of the corresponding attribute Every data item corresponds to a polygonal line which intersects each of the axes at the point which corresponds to the value for the attribute Parallel Coordinates of a Data Set Icon-Based Visualization Techniques ◼Visualization of the data values as features of icons ◼Typical visualization methods ◼Chernoff Faces ◼Stick Figures ◼General techniques ◼Shape coding: Use shape to represent certain information encoding ◼Color icons: Use color icons to encode more information ◼Tile bars: Use small icons to represent the relevant feature vectors in document retrieval Chernoff Faces A way to display variables on a two-dimensional surface, e.g., let x be eyebrow slant, y be eye size, z be nose length, etc. The figure shows faces produced using 10 characteristics--head eccentricity, eye size, eye spacing, eye eccentricity, pupil size, eyebrow slant, nose size, mouth shape, mouth size, and mouth opening): Each assigned one of 10 possible values, generated using Mathematica (S. Dickson) REFERENCE:. Gonick, L. and Smith, W. The Cartoon Guide to Statistics. New York: Harper Perennial, p. 212, 1993 Weisstein, Eric W. \"Chernoff Face.\" From MathWorld--A Wolfram Web Resource. mathworld.wolfram.com/ChernoffFace.html A census data figure showing age, income, gender, education, etc. Stick Figure A 5-piece stick figure (1 body and 4 limbs w. different angle/length) Hierarchical Visualization Techniques ◼Visualization of the data using a hierarchical partitioning into subspaces ◼Methods ◼Dimensional Stacking ◼Worlds-within-Worlds ◼Tree-Map ◼Cone Trees ◼InfoCube Dimensional Stacking attribute 1 attribute 2 attribute 3 attribute 4 ◼Partitioning of the n-dimensional attribute space in 2-D subspaces, which are 'stacked' into each other ◼Partitioning of the attribute value ranges into classes. The important attributes should be used on the outer levels. ◼Adequate for data with ordinal attributes of low cardinality ◼But, difficult to display more than nine dimensions ◼Important to map dimensions appropriately Used by permission of M. Ward, Worcester Polytechnic Institute Visualization of oil mining data with longitude and latitude mapped to the outer x-, y-axes and ore grade and depth mapped to the inner x-, y-axes Dimensional Stacking Worlds-within-Worlds Assign the function and two most important parameters to innermost world Fix all other parameters at constant values - draw other (1 or 2 or 3 dimensional worlds choosing these as the axes) Software that uses this paradigm N-vision: Dynamic interaction through data glove and stereo displays, including rotation, scaling (inner) and translation (inner/outer) Auto Visual: Static interaction by means of queries Tree-Map ◼Screen-filling method which uses a hierarchical partitioning of the screen into regions depending on the attribute values ◼The x- and y-dimension of the screen are partitioned alternately according to the attribute values (classes) MSR Netscan Image Tree-Map of a File System (Schneiderman) InfoCube ◼A 3-D visualization technique where hierarchical information is displayed as nested semi-transparent cubes ◼The outermost cubes correspond to the top level data, while the subnodes or the lower level data are represented as smaller cubes inside the outermost cubes, and so on Three-D Cone Trees 3D cone tree visualization technique works well for up to a thousand nodes or so First build a 2D circle tree that arranges its nodes in concentric circles centered on the root node Cannot avoid overlaps when projected to G. Robertson, J. Mackinlay, S. Card. “Cone Trees: Animated 3D Visualizations of Hierarchical Information”, ACM SIGCHI'. 91 Graph from Nadeau Software Consulting website: Visualize a social network data set that models the way an infection spreads from one person to the next Ack.: http://nadeausoftware.com/articles/visualization Visualizing Complex Data and Relations ◼Visualizing non-numerical data: text and social networks ◼Tag cloud: visualizing user-generated tags ◼The importance of tag is represented by font size/color ◼Besides text data, there are also methods to visualize relationships, such as visualizing social networks Newsmap: Google News Stories in 2005 Chapter 2: Getting to Know Your Data ◼Data Objects and Attribute Types ◼Basic Statistical Descriptions of Data ◼Data Visualization ◼Measuring Data Similarity and Dissimilarity ◼Summary Similarity and Dissimilarity ◼Similarity ◼Numerical measure of how alike two data objects are ◼Value is higher when objects are more alike ◼Often falls in the range [0,1] ◼Dissimilarity (e.g., distance) ◼Numerical measure of how different two data objects are ◼Lower when objects are more alike ◼Minimum dissimilarity is often 0 ◼Upper limit varies ◼Proximity refers to a similarity or dissimilarity Data Matrix and Dissimilarity Matrix ◼Data matrix ◼n data points with p dimensions ◼Two modes ◼Dissimilarity matrix ◼n data points, but registers only the distance ◼A triangular matrix ◼Single mode Proximity Measure for Nominal Attributes ◼Can take 2 or more states, e.g., red, yellow, blue, green (generalization of a binary attribute) ◼Method 1: Simple matching ◼m: # of matches, p: total # of variables ◼Method 2: Use a large number of binary attributes ◼creating a new binary attribute for each of the M nominal states Proximity Measure for Binary Attributes A contingency table for binary data Distance measure for symmetric binary variables: Distance measure for asymmetric binary variables: Jaccard coefficient (similarity measure for asymmetric binary variables): Note: Jaccard coefficient is the same as “coherence”: Object i Object j Dissimilarity between Binary Variables ◼Example ◼Gender is a symmetric attribute ◼The remaining attributes are asymmetric binary ◼Let the values Y and P be 1, and the value N 0 Name Gender Fever Cough Test-1 Test-2 Test-3 Test-4 Jack Mary Jim mary jim jim jack mary jack Standardizing Numeric Data Z-score: ◼X: raw score to be standardized, μ: mean of the population, σ: standard deviation ◼the distance between the raw score and the population mean in units of the standard deviation ◼negative when the raw score is below the mean, “+” when above An alternative way: Calculate the mean absolute deviation where ◼standardized measure (z-score): Using mean absolute deviation is more robust than using standard deviation Example: Data Matrix and Dissimilarity Matrix point attribute1 attribute2 Dissimilarity Matrix (with Euclidean Distance) Data Matrix Distance on Numeric Data: Minkowski Distance Minkowski distance: A popular distance measure where i = (xi1, xi2, ..., xip) and j = (xj1, xj2, ..., xjp) are two p-dimensional data objects, and h is the order (the distance so defined is also called L-h norm) Properties d(i, j) > 0 if i ≠ j, and d(i, i) = 0 (Positive definiteness) d(i, j) = d(j, i) (Symmetry) d(i, j)  d(i, k) + d(k, j) (Triangle Inequality) A distance that satisfies these properties is a metric Special Cases of Minkowski Distance h = 1: Manhattan (city block, L1 norm) distance ◼E.g., the Hamming distance: the number of bits that are different between two binary vectors h = 2: (L2 norm) Euclidean distance h → . “supremum” (Lmax norm, L norm) distance. ◼This is the maximum difference between any component (attribute) of the vectors Example: Minkowski Distance Dissimilarity Matrices point attribute 1 attribute 2 Manhattan (L1) Euclidean (L2) Supremum Ordinal Variables ◼An ordinal variable can be discrete or continuous ◼Order is important, e.g., rank ◼Can be treated like interval-scaled ◼replace xif by their rank ◼map the range of each variable onto [0, 1] by replacing i-th object in the f-th variable by ◼compute the dissimilarity using methods for interval- scaled variables Attributes of Mixed Type ◼A database may contain all attribute types ◼Nominal, symmetric binary, asymmetric binary, numeric, ordinal ◼One may use a weighted formula to combine their effects ◼f is binary or nominal: dij (f) = 0 if xif = xjf , or dij (f) = 1 otherwise ◼f is numeric: use the normalized distance ◼f is ordinal ◼Compute ranks rif and ◼Treat zif as interval-scaled zif Cosine Similarity A document can be represented by thousands of attributes, each recording the frequency of a particular word (such as keywords) or phrase in the document. Other vector objects: gene features in micro-arrays, ... Applications: information retrieval, biologic taxonomy, gene feature mapping, ... Cosine measure: If d1 and d2 are two vectors (e.g., term-frequency vectors), then cos(d1, d2) = (d1 • d2) /||d1|| ||d2|| , where • indicates vector dot product, ||d||: the length of vector d Example: Cosine Similarity cos(d1, d2) = (d1 • d2) /||d1|| ||d2|| , where • indicates vector dot product, ||d|: the length of vector d Ex: Find the similarity between documents 1 and 2. cos(d1, d2 ) = 0.94 Chapter 2: Getting to Know Your Data ◼Data Objects and Attribute Types ◼Basic Statistical Descriptions of Data ◼Data Visualization ◼Measuring Data Similarity and Dissimilarity ◼Summary Summary Data attribute types: nominal, binary, ordinal, interval-scaled, ratio- scaled Many types of data sets, e.g., numerical, text, graph, Web, image. Gain insight into the data by: ◼Basic statistical data description: central tendency, dispersion, graphical displays ◼Data visualization: map data onto graphical primitives ◼Measure data similarity Above steps are the beginning of data preprocessing. Many methods have been developed but still an active area of research."
  },
  {
    "index": 2,
    "title": "2. Rule Mining (ARM)",
    "content": "Data Mining - Rule Mining (ARM.) [Ch 5] - from text book PPTs by Prof. Jiawei Han http://www-faculty.cs.uiuc.edu/~hanj/bk2/slidesindex.html Session Outcomes ◼describe fundamental concepts involved in the process of ARM in large data sets ◼explain the various stages involved in ARM ◼explain the computational challenges and appropriate solutions available in implementing ARM ◼evaluate, choose and implement appropriate ARM solution for a given application domains Mining Frequent Patterns, Association and Correlations ◼Basic concepts ◼Efficient and scalable frequent itemset mining methods ◼Mining various kinds of association rules ◼From association mining to correlation analysis ◼Constraint-based association mining ◼Summary What Is Frequent Pattern Analysis? Frequent pattern: a pattern (a set of items, subsequences, substructures, etc.) that occurs frequently in a data set ◼First proposed by Agrawal, Imielinski, and Swami [AIS93] in the context of frequent itemsets and association rule mining Motivation: Finding inherent regularities in data ◼What products were often purchased together?- Beer and diapers?! ◼What are the subsequent purchases after buying a PC? ◼What kinds of DNA are sensitive to this new drug? ◼Can we automatically classify web documents? ◼Applications ◼Basket data analysis, cross-marketing, catalog design, sale campaign analysis, Web log (click stream) analysis, and DNA sequence analysis. Why Is Freq. Pattern Mining Important? ◼Discloses an intrinsic and important property of data sets ◼Forms the foundation for many essential data mining tasks ◼Association, correlation, and causality analysis ◼Sequential, structural (e.g., sub-graph) patterns ◼Pattern analysis in spatiotemporal, multimedia, time- series, and stream data ◼Classification: associative classification ◼Cluster analysis: frequent pattern-based clustering Basic Concepts: Frequent Patterns and Association Rules Itemset X = {x1, ..., xk} Find all the rules X → Y with minimum support and confidence ◼support, s, probability that a transaction contains X  Y ◼confidence, c, conditional probability that a transaction having X also contains Y Sup ( X  Y) / Sup (X) Let supmin = 50%, confmin = 50% Freq. Pat.: {A:3, B:3, D:4, E:3, AD:3} Association rules: Customer buys diaper Customer buys both Customer buys beer Transaction-id Items bought Scalable Methods for Mining Frequent Patterns ◼The downward closure property of frequent patterns ◼Any subset of a frequent itemset must be frequent ◼If {beer, diaper, nuts} is frequent, so is {beer, diaper} ◼i.e., every transaction having {beer, diaper, nuts} also contains {beer, diaper} Scalable Methods for Mining Frequent Patterns ◼Scalable mining methods: Three major approaches Apriori (Agrawal & Srikant@VLDB'94) Freq. pattern growth (FPgrowth-Han, Pei & Yin @SIGMOD'. 00) Vertical data format approach (Charm-Zaki & Hsiao @SDM'02) Apriori: A Candidate Generation-and-Test Approach ◼Apriori pruning principle: If there is any itemset which is infrequent, its superset should not be generated/tested! (Agrawal & Srikant @VLDB'94, Mannila, et al. @ KDD' 94) ◼Method: ◼Initially, scan DB once to get frequent 1-itemset ◼Generate length (k+1) candidate itemsets from length k frequent itemsets ◼Test the candidates against DB ◼Terminate when no frequent or candidate set can be generated The Apriori Algorithm-An Example Database TDB 1st scan 2nd scan 3rd scan Tid Items Itemset sup Itemset sup Itemset Itemset sup Itemset sup Itemset Itemset sup Supmin = 2 Important Details of Apriori How to generate candidates? ◼Step 1: self-joining Lk ◼Step 2: pruning How to count supports of candidates? Example of Candidate-generation ◼L3={abc, abd, acd, ace, bcd} ◼Self-joining: L3*L3 ◼abcd from abc and abd ◼acde from acd and ace ◼Pruning: ◼acde is removed because ade is not in L3 ◼C4={abcd} Challenges of Frequent Pattern Mining ◼Challenges ◼Multiple scans of transaction database ◼Huge number of candidates ◼Tedious workload of support counting for candidates ◼Improving Apriori: general ideas ◼Reduce passes of transaction database scans ◼Shrink number of candidates ◼Facilitate support counting of candidates Partition: Scan Database Only Twice ◼Any itemset that is potentially frequent in DB must be frequent in at least one of the partitions of DB ◼Scan 1: partition database and find local frequent patterns ◼Scan 2: consolidate global frequent patterns ◼A. Savasere, E. Omiecinski, and S. Navathe. An efficient algorithm for mining association in large databases. In VLDB'95 Sampling for Frequent Patterns ◼Select a sample of original database, mine frequent patterns within sample using Apriori ◼Scan database once to verify frequent itemsets found in sample, only borders of closure of frequent patterns are checked ◼Example: check abcd instead of ab, ac, ..., etc. ◼Scan database again to find missed frequent patterns ◼H. Toivonen. Sampling large databases for association rules. In VLDB'96 Find All Freq. Itemsets using Apriori Support = 60 % Conf. = 80 % ARM1 Mining Frequent Patterns Without Candidate Generation ◼Grow long patterns from short ones using local frequent items ◼“abc” is a frequent pattern ◼Get all transactions having “abc”: DB|abc ◼“d” is a local frequent item in DB|abc → abcd is a frequent pattern Construct FP-tree from a Transaction Database Header Table Item frequency head min_support = 3 TID Items bought (ordered) frequent items 1. Scan DB once, find frequent 1-itemset (single item pattern) 2. Sort frequent items in frequency descending order, f-list 3. Scan DB again, construct FP-tree F-list=f-c-a-b-m-p Transaction ID Items bought FP-tree from a Transaction Database Step 1 Find frequent-1 item sets Transaction ID Items bought DB Scan 1 Item Frequency Step 2 Sort the set of frequency items in order of descending frequency, and get the ordered f-list Item Frequency Item Frequency sort F-list = f, c, a, b, m, p Step 3 Create the root of the tree, labelled with “null” null Step 4 Scan each transactions in the DB → generate FP-Tree The items in each transaction are processed in f-list order Item Frequency null TID Items bought Frequent items in F-list order F-list = f, c, a, b, m, p Header Table Item Frequency null TID Items bought Frequent items in F-list order F-list = f, c, a, b, m, p Header Table Item Frequency null TID Items bought Frequent items in F-list order F-list = f, c, a, b, m, p Header Table Item Frequency null F-list = f, c, a, b, m, p Header Table TID Items bought Frequent items in F-list order Benefits of the FP-tree Structure ◼Completeness ◼Preserve complete information for frequent pattern mining ◼Never break a long pattern of any transaction ◼Compactness ◼Reduce irrelevant info-infrequent items are gone ◼Items in frequency descending order: the more frequently occurring, the more likely to be shared ◼Never be larger than the original database ◼Compression ratio could be over 100 Find Patterns Having P From P-conditional Database Starting at the frequent item header table in the FP-tree Traverse the FP-tree by following the link of each frequent item p Accumulate all of transformed prefix paths of item p to form p's conditional pattern base Conditional pattern bases item cond. pattern base fca:1, f:1, c:1 fca:2, fcab:1 fcam:2, cb:1 Header Table Item frequency head FP-Growth vs. Apriori: Scalability With the Support Threshold Support threshold(%) Run time(sec.) D1 FP-grow th runtime D1 Apriori runtime Data set T25I20D10K Why Is FP-Growth the Winner? ◼Divide-and-conquer: ◼decompose both the mining task and DB according to the frequent patterns obtained so far ◼leads to focused search of smaller databases ◼Other factors ◼no candidate generation, no candidate test ◼compressed database: FP-tree structure ◼no repeated scan of entire database ◼basic ops-counting local freq items and building sub FP-tree, no pattern search and matching Mining by Exploring Vertical Data Format ◼Vertical format: t(AB) = {T11, T25, ...} ◼tid-list: list of trans.-ids containing an itemset ◼Deriving closed patterns based on vertical intersections ◼t(X) = t(Y): X and Y always happen together ◼t(X)  t(Y): transaction having X always has Y Tid Items Itemset Transaction Id List Tid Mining by Exploring Vertical Data Format ◼Using diffset to accelerate mining ◼Only keep track of differences of tids ◼Diffset (XY, X) = {T2} ◼Eclat/MaxEclat (Zaki et al. @KDD'97), VIPER(P. Shenoy et al.@SIGMOD'. 00), CHARM. ( Zaki & Hsiao@SDM'02) Tid Items Visualization of Association Rules: Plane Graph Visualization of Association Rules: Rule Graph Visualization of Association Rules (SGI/MineSet 3.0) Mining Various Kinds of Association Rules Mining multilevel association Miming multidimensional association Mining quantitative association Mining interesting correlation patterns ARM2 Mining Multiple-Level Association Rules ◼Items often form hierarchies ◼Flexible support settings ◼Items at the lower level are expected to have lower support ◼Exploration of shared multi-level mining (Agrawal & Srikant@VLB'95, Han & Fu@VLDB'95) uniform support Milk [support = 10%] 2% Milk [support = 6%] Skim Milk [support = 4%] Level 1 min_sup = 5% Level 2 min_sup = 5% Level 1 min_sup = 5% Level 2 min_sup = 3% reduced support Multi-level Association: Redundancy Filtering ◼Some rules may be redundant due to “ancestor” relationships between items. ◼Example ◼milk  wheat bread [support = 8%, confidence = 70%] ◼2% milk  wheat bread [support = 2%, confidence = 72%] ◼We say the first rule is an ancestor of the second rule. ◼A rule is redundant if its support is close to the “expected” value, based on the rule's ancestor. Mining Multi-Dimensional Association ◼Single-dimensional rules: buys(X, “milk”)  buys(X, “bread”) ◼Multi-dimensional rules:  2 dimensions or predicates ◼Inter-dimension assoc. rules (no repeated predicates) age(X,”19-25”)  occupation(X,“student”)  buys(X, “coke”) ◼hybrid-dimension assoc. rules (repeated predicates) age(X,”19-25”)  buys(X, “popcorn”)  buys(X, “coke”) ◼Categorical Attributes: finite number of possible values, no ordering among values ◼Quantitative Attributes: numeric, implicit ordering among values-discretization, clustering, and gradient approaches Mining Quantitative Associations Techniques can be categorized by how numerical attributes, such as age or salary are treated Static discretization based on predefined concept hierarchies Dynamic discretization based on data distribution (quantitative rules, e.g., Agrawal & Srikant@SIGMOD96) Clustering: Distance-based association (e.g., Yang & Miller@SIGMOD97) one dimensional clustering then association Quantitative Association Rules age(X,”34-35”)  income(X,”30-50K”)  buys(X,”high resolution TV”) ◼Proposed by Lent, Swami and Widom ICDE'97 ◼Numeric attributes are dynamically discretized ◼Such that the confidence or compactness of the rules mined is maximized ◼2-D quantitative association rules: Aquan1  Aquan2  Acat ◼Cluster adjacent association rules to form general rules using a 2-D grid ◼Example 2-D grid of customers who buy HDTV Interestingness Measure: Correlations (Lift) play basketball  eat cereal [40%, 66.7%] is misleading ◼The overall % of students eating cereal is 75% > 66.7%. play basketball  not eat cereal [20%, 33.3%] is more accurate, although with lower support and confidence Measure of dependent/correlated events: lift lift Basketball Not basketball Sum (row) Cereal Not cereal Sum(col.) lift lift Interestingness Measures Constraint-based (Query-Directed) Mining ◼Finding all the patterns in a database autonomously? - unrealistic! ◼The patterns could be too many but not focused! ◼Data mining should be an interactive process ◼User directs what to be mined using a data mining query language (or a graphical user interface) ◼Constraint-based mining ◼User flexibility: provides constraints on what to be mined ◼System optimization: explores such constraints for efficient mining-constraint-based mining Constraints in Data Mining ◼Knowledge type constraint: ◼classification, association, etc. ◼Data constraint - using SQL-like queries ◼find product pairs sold together in stores in Chicago in Dec.'02 ◼Dimension/level constraint ◼in relevance to region, price, brand, customer category ◼Rule (or pattern) constraint ◼small sales (price < $10) triggers big sales (sum > ◼Interestingness constraint ◼strong rules: min_support  3%, min_confidence  The Apriori Algorithm - Example TID Items Database D itemset sup. itemset sup. Scan D itemset itemset sup itemset sup Scan D itemset Scan D itemset sup Naïve Algorithm: Apriori + Constraint TID Items Database D itemset sup. itemset sup. Scan D itemset itemset sup itemset sup Scan D itemset Scan D itemset sup Constraint: Sum{S.price} < 5 Frequent-Pattern Mining: Summary Frequent pattern mining-an important task in data mining Scalable frequent pattern mining methods Apriori (Candidate generation & test) Projection-based (FPgrowth, ...) Vertical format approach (VIPER. , ...) Mining a variety of rules and interesting patterns Constraint-based mining ARM3"
  },
  {
    "index": 3,
    "title": "3. Time Series Part 1",
    "content": "Time Series Mining & Forecasting Dr. Thanuja Ambegoda Lecture outline Introduction to TS Basic visualization techniques Trend analysis and seasonality detection Cyclic pattern analysis Anomaly detection in TS Stationarity in TS Traditional TS forecasting methods Advanced TS forecasting methods Part 1. Introduction to time series analysis and mining Time series everywhere! - Sunspots Month Monthly Sunspot Numbers, 1749 - 1983 :IIOOO. Half-hourly electricity demand in England and Wales - Dally minimum temperan.es Dllte Daily temperature in Melbourn between 1981 and 1990 What is a time series? A time series is an ordered sequence of values of a variable usually at equally spaced time intervals Typically, the mean and the standard deviation can vary in a time series Understanding time series data helps predict future events based on historical patterns Trend Volatility TS analysis, forecasting & mining Time series analysis Techniques for identifying and analysing temporal dependencies in sequential data Time series mining What has happened? Time series forecasting What's going to happen next? Applications Finance Sales forecasting Inventory analysis Stock market analysis Price estimation Weather Temperature Climate change Seasonal shifts Rain, wind, snow,... Industry Usage Anomaly Maintenance Healthcare Patient occupancy in hospitals Disease outbreaks Patient monitoring Insurance Sales forecasting Insurance beneﬁts awarded Diﬀerence between a time series and a random variable with known mean and standard deviation We might be able to build a distribution that explains all the data points in the time series. But does that mean the distribution can be sampled to produce the time series? Dependence on the past Memory of a TS The extent to which past values inﬂuence future values A few points having a similar trend But there might be no overall trend Actual values tend to depend on the value at the previous time point Autocorrelation Memory of a Time Series Short memory Short-term dependencies. Only recent past values inﬂuence the next value E.g. ﬁnancial data Long memory Long-term dependencies. Distant past values can impact the next value. E.g. climate data Stationary TS often have short memory Non-stationary TS often have long memory but require more complex models Time Series Components Time series components Level: The average value in the series. Trend: The increasing or decreasing value in the series. Cycle: (Long-term) phenomena that happen across seasonal periods, without a ﬁxed period (eg. economic recession) Seasonality: The repeating short-term cycle in the series. Noise: The random variation in the series. (unsystematic) Reference Trend Refers to the increasing or decreasing value in the series. Can be linear or nonlinear. Methods to detect: Moving Averages Polynomial Fitting Seasonal component Periodic ups and downs. Period can be a year, several years, months, weeks or days. Retail Sales of Used Car Dealers in the United States 12 month seasonal period Month How to measure/detect seasonality Autocorrelation plots FFT (Fast Fourier Transform) Seasonal decomposition Autocorrelation Plots Autocorrelation, often denoted as R(k), measures the correlation between a time series and its lagged version. A signiﬁcant spike at a particular lag indicates seasonality at that lag. If there's seasonality present at a particular lag k, the autocorrelation plot will show a significant spike at that lag. For example, if there's a yearly seasonality in monthly data, we would expect a significant autocorrelation at lag 12. Decomposing TS into components Often this is done to help improve understanding of the time series, but it can also be used to improve forecast accuracy. Additive models (linear) y(t) = Level + Trend + Seasonality + Noise Multiplicative models (nonlinear) y(t) = Level * Trend * Seasonality * Noise Real-world problems are messy and noisy. There may be additive and multiplicative components. There may be an increasing trend followed by a decreasing trend. There may be non-repeating cycles mixed in with the repeating seasonality components. Decomposing TS into components Electrical equipment manufacturing (Euro area) Year series Data - Trend-cycle Original Trend-cycle component Seasonal component Residual component Seasonal and Trend Decomposition using LOESS (STL) LOESS:. Locally Weighted Scatterplot Smoothing Alternatively LOWESS,. which means Locally Weighted Regression Non-parametric smoothing technique. Fits smooth curves using locally weighted polynomial regression. Each point estimated based on subset of its neighbors. Close neighbors have more inﬂuence via weighted least squares. Ensures gradual transitions, yielding a smooth curve. Reference Steps in STL Trend extraction Applies LOESS. smoother to time series. Captures the central path or growth trajectory. Not inﬂuenced by seasonality. Detrending & Seasonal Extraction Remove trend from original series. Apply LOESS. to detrended series to capture seasonality. Seasonal pattern can be of any type: monthly, quarterly, etc. Extract Residual Component Residuals = Original Series - Trend - Seasonality. Captures randomness and potential anomalies. Helpful for model diagnostics. Seasonally adjusted data If the variation due to seasonality is not of primary interest, the seasonally adjusted series can be useful. For example, monthly unemployment data are usually seasonally adjusted in order to highlight variation due to the underlying state of the economy rather than the seasonal variation. TS decomposition using statsmodel package Naive, or classical, decomposition method. Requires you to know if the model is additive or multiplicative Statsmodels python package \"iii 0.95 Month Cycles Oscillations that aren't ﬁxed in a seasonal pattern. Typically longer duration than seasonality. Can be caused by economic conditions, large-scale events, etc. Why detect cycles? Improve forecasting accuracy.Uncover underlying long-term patterns. Make informed strategic decisions (e.g., economic, business planning). Separate cyclical eﬀects from irregular noise. Detecting cycles Spectral analysis Transform time series into frequency domain using Fourier Transform. Identify dominant frequencies (apart from seasonal frequencies). The peaks in spectral density indicate cyclical components. Seasonality shows up as distinct peaks at regular intervals in the spectral density plot. Cyclical components will manifest as broader peaks or bands at lower frequencies, representing longer-term ﬂuctuations. Detecting cycles 2. Wavelet transform Decompose time series into diﬀerent frequency bands. Detect cycles of various lengths. Suitable for non-stationary time series Seasonality: Seasonal components will appear as consistent patterns at speciﬁc scales (frequencies) across the time domain. Cycles: Cyclical components will appear as patterns at broader scales that may vary over time. TS and regression Focused on identifying underlying trends and patterns Mathematically modeling / describing those patterns Predict/forecast future values TS and regression Regression can model the trend over time Linear regression for mean demand. Function of economic growth? Within year cycles - seasonal variability? TS allows you to model the process without knowing the underlying causes Level Noise Non-constant Variance Trend Seasonality Outlier Signal and Noise Signal and noise C'ansignal Noise Ho isy slgna I Signal and noise: some deﬁnitions Statistical moments Mean and standard deviation Stationary vs non-stationary Trends in mean and/or standard deviation Stationary - doesn't depend on the time of observation Seasonality Periodic patterns Autocorrelation Degree to which the time series values in period t are related to time series values in period t-1, t-2, ... Stationarity of a Time Series Stationary TS TS whose properties do not depend on the time of observation Constant variance and level No seasonality No trend Stationarity check Visual inspection Seasonal-Trend decomposition Summary statistics at random partitions Statistical tests Stationarity check: Visual inspection Mean Variance Seasonality Stationarity check: Seasonal-Trend decomposition Statsmodels python package from pandas import read_csv from statsmodels.tsa.seasonal import seasonal_decompose df = read_csv( a1rhne-p ssengers csv', header=O) df.Month = pd.to_datetime(df.Month) df = df.set_index('Month') result = seasonal_decompose( df, model='multiplicative') result.plotQ d L1tfl--- q Stationarity check: summary statistics Summary statistics at random partitions Stationarity check: statistical tests Augmented Dickey-Fuller test TS is non-stationary! Adjustments Fixing non-constant variance Trend removal Fixing non-constant variance + trend removal Fixing non-constant variance Air Passengers from 1949 to 1961 • Logarithmic - Removes extreme ( exponential) trends • Square root transformation - Removes a quadratic growth trend • Box-Cox transform - Supports both square root and log transform lil50 Time Fixing non-constant variance To reverse the boxcox transform: from scipy.special import boxcox, inv_boxcox from pandas import read_csv from numpy import log from numpy import sqrt from scipy.stats import boxcox import matplotlib.pyplot as pit series = read_csv('airline-passengers.csv', header=O, parse_dates=[O], index_col=O, squeeze=True) series_log = log(series) series_sqr = sqrt(series) series_boxcox, lam= boxcox(series) print('Lambda: %f % lam) pit.plot( series_log) plt.plot(series_sqr) pit.plot( series_boxcox) pit.show() IIO• lll. Original Time Series Square Root Transform Box-Cox Transform lll Trend removal eoo. Differencing !IIO· - First order : the time series value yi is replaced by the difference between it and the previous value. - First-order - Second-order Original Time Series !!ISO lllS2 !!ISi from pandas import datetime from matplotlib import pyplot After Differencing (First-order) series = read_csv('airline-passengers.csv', header=0, parse_dates=[0], index_col=0, squeeze=True) diff = series.diffQ pyplot.plot( series. values) pyplot.plot( diff. values) Fixing non-constant variance + trend removal from pandas import read_csv from pandas import Series import pandas as pd from scipy.stats import boxcox series = read_csv('airline-passengers.csv', header=0, parse_dates=[0], index_col=0, squeeze=True) series_boxcox, lam= boxcox(series) series_boxcox=pd.DataFrame(series_boxcox) series_boxcox_diff = series_boxcox.diff() pit.plot( series) plt.plot(series_boxcox_diff) Original Time Series Box Cox Transform+ Differencing"
  },
  {
    "index": 4,
    "title": "4. Time Series Part 2",
    "content": "Time Series Mining & Forecasting - 2 Dr. Thanuja Ambegoda Time Series Part. 2 - Lecture Outline Time series forecasting (ctd) Autocorrelation revisited Forecasting methods Evaluation of time series forecasting Time series mining Time series mining tasks Time series compression Time series similarity measures Motifs, matrix proﬁle, discord, anomalies Time series representations Correlation Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate) Autocorrelation temperature on day i-1 shift Autocorrelation plot (ACF) • ACF. describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information. • How to calculate ACF? Autocorrelation For lag= I :Maxlag: ACF(lag)= corr (Xt, X1agN) • Example:X=[I 2 3 4 5 6 7 8] Xlag I = [2 3 4 5 6 7] ACF(lag= I )=correlation(X,X1agl) ACF(lag=2)=correlation(X,X1ag2) Partial autocorrelation (PACF) plot • PACF describes direct relationship between an observation and its lag. • How to calculate PACF? For lag= I :Maxlag Fit Linear Regression: Xt = $o + $ -Xt-1 + $+ Xt-2 + ... + <1>1ag Xt-lag PACF(lag)= <1>1ag Partial Autocorrelation -oos Time Series Models Traditional Time Series Models Univariate Models -ARIMA -SARIMAX. - Prophet - Neural Prophet Multivariate Models - Vector Auto regression Machine Learning Models - Neural Network Regressor - Catboost Regressor - Any reg ressor Simple forecasting methods Average method Naive method Seasonal naive method Drift method Homework: Read about simple forecasting methods These methods are covered for the sake of completeness of fundamental theory, and can be used as primary benchmarks. Time series forecasting Estimating future values of a time series Exponential smoothing Autoregressive methods ARMA ARIMA SARIMA. Exponential smoothing Simple Exponential Smoothing (SES) (Brown, 1956) is suitable for forecasting data with no clear trend or seasonal pattern. Prediction is a weighted linear sum of recent past observations Weights decline exponentially, making observations much earlier in the past less relevant Exponential smoothing Double Exponential Smoothing (Holt, 1957) Smoothing for level + trend Triple Exponential Smoothing (Holt-Winter, 1960) Smoothing for trend + seasonality + level Exponential smoothing - Python import pandas as pd import numpy as np import matplotlib.pyplot as pit from statsmodels.tsa.holtwinters import ExponentialSmoothing from statsmodels.tsa.holtwinters import SimpleExpSmoothing df = pd.read_csv('airline-passengers.csv', parse_dates=['Month'],index_col='Month') df.index.freq = 'MS' ts=df.iloc[:, 0] es I = SimpleExpSmoothing(ts).fit(smoothing_level=0.2) ts_es I = es 1.predict(start=ts.index[0], end=ts.index(-1 ]) es2 = ExponentialSmoothing(ts, trend='add').fit(smoothing_level=0.2,smoothing_trend=0.2) ts_ es2 = es2.predict( start=ts.index[0], end=ts.index[-1 ]) es3 = ExponentialSmoothing(ts, trend='add',seasonal='mul', seasonal_periods= 12).fit(smoothing_level=0.2,smoothing_trend=0.2,smoothing_seasonal=0.2) ts_es3 = es3.predict(start=ts.index[0], end=ts.index(-1 ]) Exponential smoothing - Python 600 _ - Raw Data JlO Simple Exponential Smoothing Level only 6ll0 _ - Ra v Data Double Exponential Smoothing Level+ Trend fiO0. - Ra Data - Holt-Wmters Triple Exponential Smoothing Level+ Trend + Seasonality Autoregressive model (AR) In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself. εt is the white noise. This is an autoregressive model of order p - AR(p) Autoregressive model • 98 Equations, 3 Coefficients • Solution: Linear least squares Autoregressive model - Python import statsmodels.api as sm import matplotlib.pyplot as pit from statsmodels.tsa.ar _model import AutoReg train, test = x[ I :len(x)-7], x[len(x)-7:] model = AutoReg(train, lags=2) model_fit = model.fitQ print('Coefficients: %s' % model_fit.params) x_ar = model_fit.predict(start=O, end=len(x), dynamic=False) plt.plot(x[2:len(x)],label='Actual') plt.plot(x_ar, color='red',label='AR(2) Model') plt.legend(loc='best') Moving average model (MA) Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model. εt is the white noise. This is a moving average model of order q - MA(q) The MA component models the relationship between an observation and the white noise errors of previous observations Moving average model It is possible to write any stationary AR(p) model as an MA(oo) model. For example, using repeated substitution, we can demonstrate this for an AR(1) model: = <PiYt-2 + </>1ct-l + ct = ¢f Yt-3 + </>ict-2 + </>1ct-l + ct etc. Provided -1 < ¢1 < 1, the value of¢, will get smaller ask gets larger. So eventually we obtain Yt =ct+ </>1ct-l + </>ict-2 + </>ict-3 + · · ·, an MA(oo) process. Moving average model Parameter estimation: Maximum likelihood estimation Non-linear least squares estimation Moving average model - python from pandas ·mport read_csv from statsmodels.tsa.arima_model import ARMA from sklearn.metrics import mean_squared_error import matplotlib.pyplot as pit df = read_csv('HalmstadTempM.csv', header=O, parse_dates=[O], index_col=O) dftrain, dftest = df[l :len(df)-7], df[len(df)-7:] model = ARMA( dftrain, order=(O, 5)) #MA(S) model_fit = model.fit() pri,nt('Coefficients: %s' % model_fit.params) predictions = model_fit.predict( start=len( dftrain ), end=len( dftrain)+len(dftest)-1, dynamic=False) for i in range(len(predictions) ): print('pr dicted=%f, expected=%f % (predictions[i], dftest.values[i])) rmse = sqrt(mean_squared_error(dftest.values, predictions)) pr\"nt(iest RMSE: %.3 % rmse) pit.plot( dftest,label='Actu I') plt.plot(predictions, color=' red', label='MA( I) Forecast') plt.legend(loc='best') pit.show() Autoregressive moving average model (ARMA) Combines AR and MA models AR accounts for autocorrelations in the TS (deterministic) MA accounts for smoothing ﬂuctuations (stochastic) Model fitting: Maximum likelihood estimation (Box-Jenkins method) Autoregressive integrated moving average (ARIMA) ARIMA = ARMA. + trend removal Seasonal ARIMA (SARIMA) SARIMA = ARMA. + trend removal + seasonal adjustment • SARI MA. ( p, d, q)(P. D, Q)m • Example: SARI MA. ( 2, /,3)( /, 1,2) 12) ■ Autoregressive with 2 lags ■ Autoregressive with one Seasonal la s ( 12) ■ Differencing data in order of one ■ Differencing data in order of one with Seasonal legs ( 12) ■ Moving Average with 3 lags ■ Moving Average with 2 Seasonal lags ( 12) ----, Passengers JJO Months -- Month Months -- SARIMA. - Python from pandas import read_csv from sklearn.metrics import mean_squared_error import matplotlib.pyplot as pit from pmdarima import auto_arima import pandas as pd df = read_csv('airline-passengers.csv', header=O, parse_dates=[O], index_col=O) train, test = df[ I :len( df)-7], df[len( df)-7:] # SARIMA. wtth automatic hyperparameter settJng model = auto_arima(train, start_p= I, start_q= I, max_p=3, max_q=3, m= 12, start_P=O, seasonal=True, d= I, D= I, trace=True, error _action='ignore', suppress_ warnings=True, stepwise=True) model.fit(train) forecast = model.predict(n_periods=len(test)) plt.plot(test.values,label='Actual') plt.plot(forecast, color='red' ,label='SARIMA'+. str( model.order )+str( model.seasonal_ order)) plt.legend(loc='best') Comparison of model assumptions Model Constant Variance ARMA ARIMA SARIMA. ✓ Constant Mean (No Trend) Seasonality­ Free Evaluation Residual plots: ACF, PACF. residual plots, histogram or density plot of residuals Information Criteria AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are used to compare and select models with diﬀerent parameters. They take into account both the likelihood of the model and the number of parameters used. The model with the lowest AIC or BIC is usually preferred. These criteria are often used to choose the order of ARIMA. models (i.e., the values of p, d, and q). Model selection (hyperparameter tuning) Visualization MA - Autocorrelation function (ACF) plot RA - Partial autocorrelation function (PACF) plot Grid search Test for diﬀerent model orders exhaustively A model that is more accurate and less complex is preferred More accurate - ﬁts the data Less complex - smaller number of parameters Based on the error metrics, residual analysis, and overﬁtting checks, select the model that seems most likely to generalize well to new data. This is typically the model with low error on the validation/test set, whose residuals are closest to white noise, and has lower AIC or BIC values. Time Series Mining Time series mining tasks Indexing (query by content) Clustering Classiﬁcation Prediction Summarization Anomaly detection Segmentation Indexing (query by content) Whole matching: a query time series is matched against a database of individual time series to identify the ones similar to the query Subsequence Matching: a short query subsequence time series is matched against longer time series by sliding it along the longer sequence, looking for the best matching location. Brute force technique (linear or sequential matching) Very slow Somewhat more eﬃcient method: Store a compressed version of the TS and use it for an initial comparison (lower bound) using linear scan Even more eﬃcient method: Use an index structure that clusters similar sequences together Indexing Two types Vector-based: Original sequences are compressed using dimensionality reduction Resulting sequences grouped in the new dimensions Hierarchical (e.g. R-tree) or non-hierarchical Performance deteriorates when dim > 5 Metric-based: Superior in performance Works for higher dimensionalities (dim < 30) Require only distances between objects Clusters objects using the distance between objects Summarization Text/graphical descriptions (summaries) Anomaly detection and motif discovery are special cases of summarization Some of popular approaches for visualizing massive time series datasets include TimeSearcher, Calendar-Based Visualization, Spiral and VizTree Read section 3.5 of this reference Time series compression TS compression helps reduce storage costs and allows eﬃcient indexing Delta encoding Delta of delta encoding Simple 8b Run Length Encoding (RLE) Reference Similarity measures Dissimilarity measures Distance measures Time series similarity measures Similarity is the inverse of distance They play an important role in time series mining tasks Euclidean distance and Lp norms Dynamic time warping Longest common subsequence similarity Probabilistic similarity measures General transformations Euclidean distance and Lp norms Dissimilarity between sequences C and Q p=2 reduces to Euclidean distance (p=1 is Manhattan distance) Widely used Only works when the two sequences are of the same length For two sequences of length n, consider each of them as a point in n dimensional Euclidean space Euclidean distance between normalized sequences To normalize C, replace C with c' where Dynamic Time Warping (DTW) Useful when the two sequences have a similar shape but don't line up perfectly (i.e. out of phase) in the x axis Euclidean distance doesn't capture the similarity well DTW allows acceleration/deceleration of the signal (warping) along the time (x) axis Example application: speech processing Dynamic Time Warping Uses bottom-up dynamic programming approach Basic implementation O(mn) With a warping window w, O(nw) DTW cost/distance: number of changes (edits) made to C to make it aligned with Q Reference Basic idea Consider X = x1, x2, ..., xn , and Y = y1, y2, ..., yn We are allowed to extend each sequence by repeating elements Euclidean distance now calculated between the extended sequences X' and Y' Matrix M, where mij = d(xi, yj) Motifs, anomalies, discords, matrix proﬁles Matrix Profile TL;DR Abdullah Mueen, Eamonn Keogh, et al. A novel data structure for time series data mining. Features domain agnostic, exact or approximate, fast, one hyper-parameter, space efficient, para I lel izable, streaming support, missing data, interactive constant in time, LOO Anomaly fllote that there is a tiny signal unnoticable to the naked eye in the thousandths, but the matrix profile picks it up. Indices The matrix profile (red) is composed of two arrays; distances and 1-NN indices. Large distances are anomalous events. Repeated patterns are found in the 1-NN indices. rma\u0004 by 1y1er ►1a Time series representations Refer section 4 of this reference References and additional reading Book chapter on Time Series Mining NeuralProphet"
  },
  {
    "index": 5,
    "title": "5. Clustering I",
    "content": "Cluster Analysis Basic concepts and Methods Dr. Thanuja Ambegoda Senior Lecturer Department of Computer Science and Engineering University of Moratuwa Lecture outline Cluster Analysis: Basic Concepts Partitioning Methods Hierarchical Methods Density-Based Methods Grid-Based Methods Evaluation of Clustering Summary 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods What is cluster analysis Cluster: A collection of data objects that are, similar (or related) to one another within the same group dissimilar (or unrelated) to the objects in other groups Cluster analysis (or clustering, data segmentation, ...) Partitioning a set of data objects into groups (subsets) such that similar ones are grouped together while dissimilar ones are placed in diﬀerent groups. Unsupervised learning: no predefined classes (i.e., learning by observations not by examples) Typical applications As a stand-alone tool to get insight into data distribution As a preprocessing step for other algorithms Clustering for Data Understanding and Applications Biology: taxonomy of living things: kingdom, phylum, class, order, family, genus and species Information retrieval: document clustering Land use: Identification of areas of similar land use in an earth observation database Marketing: Help marketers discover distinct groups in their customer bases, and then use this knowledge to develop targeted marketing programs City-planning: Identifying groups of houses according to their house type, value, and geographical location Earthquake studies: Observed earthquake epicenters should be clustered along continent faults Clustering as a Preprocessing Tool (Utility) Summarization: Preprocessing for regression, PCA, classification, and association analysis Compression: Image processing: vector quantization Finding K-nearest Neighbors Localizing search to one or a small number of clusters Outlier detection Outliers are often viewed as those “far away” from any cluster Quality: What Is Good Clustering? A good clustering method will produce high quality clusters, i.e., high intra-class similarity: cohesive within clusters low inter-class similarity: distinctive between clusters The quality of a clustering method depends on the similarity measure used by the method its implementation, and Its ability to discover some or all of the hidden patterns Measure the Quality of Clustering Dissimilarity/Similarity metric Similarity is expressed in terms of a distance function, typically metric: d(i, j) The definitions of distance functions are usually rather diﬀerent for interval-scaled, boolean, categorical, ordinal, ratio, and vector variables. Weights should be associated with diﬀerent variables based on applications and data semantics Quality of clustering: There is usually a separate “quality” function that measures the “goodness” of a cluster. It is hard to define “similar enough” or “good enough” The answer is typically highly subjective Considerations for Cluster Analysis Partitioning criteria Single level vs. hierarchical partitioning (often, multi-level hierarchical partitioning is desirable) Separation of clusters Exclusive (e.g., one customer belongs to only one region) vs. non-exclusive (e.g., one document may belong to more than one class) Similarity measure Distance-based (e.g., Euclidian, road network, vector) vs. connectivity-based (e.g., density or contiguity) Clustering space Full space (often when low dimensional) vs. subspaces (often in high-dimensional clustering) Requirements and Challenges Scalability Clustering all the data instead of only on samples Ability to deal with diﬀerent types of attributes Numerical, binary, categorical, ordinal, linked, and mixture of these Constraint-based clustering User may give inputs on constraints Use domain knowledge to determine input parameters Interpretability and usability Others Discovery of clusters with arbitrary shape Ability to deal with noisy data Incremental clustering and insensitivity to input order High dimensionality Major Clustering Approaches (1) Partitioning approach: Construct various partitions and then evaluate them by some criterion, e.g., minimizing the sum of squared errors Typical methods: k-means, k-medoids, CLARANS. Hierarchical approach: Create a hierarchical decomposition of the set of data (or objects) using some criterion Typical methods: Diana, Agnes, BIRCH, CHAMELEON. Density-based approach: Based on connectivity and density functions Typical methods: DBSACN, OPTICS,. DenClue Grid-based approach: based on a multiple-level granularity structure Typical methods: STING,. WaveCluster, CLIQUE. Major Clustering Approaches (2) Model-based: A model is hypothesized for each of the clusters and tries to find the best fit of that model to each other Typical methods: EM, SOM, COBWEB. Frequent pattern-based: Based on the analysis of frequent patterns Typical methods: p-Cluster User-guided or constraint-based: Clustering by considering user-specified or application-specific constraints Typical methods: COD (obstacles), constrained clustering Link-based clustering: Objects are often linked together in various ways Massive links can be used to cluster objects: SimRank, LinkClus 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods Partitioning Algorithms: Basic Concept Partitioning method: Partitioning a database D of n objects into a set of k clusters, such that the sum of squared distances is minimized (where ci is the centroid or medoid of cluster Ci) Given k, find a partition of k clusters that optimizes the chosen partitioning criterion Global optimal: exhaustively enumerate all partitions Heuristic methods: k-means and k-medoids algorithms k-means (MacQueenʼ67, Lloydʼ57/ʼ82): Each cluster is represented by the center of the cluster k-medoids or PAM (Partition around medoids) (Kaufman & Rousseeuwʼ87): Each cluster is represented by one of the objects in the cluster The K-Means Clustering Method Given k, the k-means algorithm is implemented in four steps: Partition objects into k non-empty subsets Compute seed points as the centroids of the clusters of the current partitioning (the centroid is the center, i.e., mean point, of the cluster) Assign each object to the cluster with the nearest seed point Go back to Step 2, stop when the assignment does not change An Example of K-Means Clustering Comments on the K-Means Method Strength: Eﬀicient: O(tkn), where n is # objects, k is # clusters, and t is # iterations. Normally, k, t << n. Comparing: PAM: O(k(n-k)2 ), CLARA: O(. ks2 + k(n-k)) Comment: Often terminates at a local optimal. Weakness Applicable only to objects in a continuous n-dimensional space Using the k-modes method for categorical data In comparison, k-medoids can be applied to a wide range of data Need to specify k, the number of clusters, in advance (there are ways to automatically determine the best k (see Hastie et al., 2009) Sensitive to noisy data and outliers Not suitable to discover clusters with non-convex shapes Variations of the K-Means Method Most of the variants of the k-means which diﬀer in Selection of the initial k means Dissimilarity calculations Strategies to calculate cluster means Handling categorical data: k-modes Replacing means of clusters with modes Using new dissimilarity measures to deal with categorical objects Using a frequency-based method to update modes of clusters A mixture of categorical and numerical data: k-prototype method What Is the Problem of the K-Means Method? The k-means algorithm is sensitive to outliers ! Since an object with an extremely large value may substantially distort the distribution of the data K-Medoids: Instead of taking the mean value of the object in a cluster as a reference point, medoids can be used, which is the most centrally located object in a cluster PAM: K-Medoids algorithm Total Cost = 20 Arbitrarily choose k object as initial medoids Assign each remaining object to nearest medoids Randomly select a nonmedoid object,Orandom Compute total cost of swapping Total Cost = 26 Swapping O and Oramdom If quality is improved. Loop until no change The K-Medoid Clustering Method K-Medoids Clustering: Find representative objects (medoids) in clusters PAM (Partitioning Around Medoids, Kaufmann & Rousseeuw 1987) Starts from an initial set of medoids and iteratively replaces one of the medoids by one of the non-medoids if it improves the total distance of the resulting clustering PAM works eﬀectively for small data sets, but does not scale well for large data sets (due to the computational complexity) Eﬀiciency improvement on PAM CLARA. ( Kaufmann & Rousseeuw, 1990): PAM on samples CLARANS. ( Ng & Han, 1994): Randomized re-sampling 1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods Hierarchical Clustering Use distance matrix as clustering criteria. This method does not require the number of clusters k as an input, but needs a termination condition AGNES. ( Agglomerative Nesting) Introduced in Kaufmann and Rousseeuw (1990) Implemented in statistical packages, e.g., Splus Use the single-link method and the dissimilarity matrix Merge nodes that have the least dissimilarity Go on in a non-descending fashion Eventually all nodes belong to the same cluster Dendrogram: Shows How Clusters are Merged Decompose data objects into several levels of nested partitioning (tree of clusters), called a dendrogram A clustering of the data objects is obtained by cutting the dendrogram at the desired level, then each connected component forms a cluster DIANA. ( Divisive Analysis) Introduced in Kaufmann and Rousseeuw (1990) Implemented in statistical analysis packages, e.g., Splus Inverse order of AGNES. Eventually each node forms a cluster on its own Distance between Clusters Single link: smallest distance between an element in one cluster and an element in the other, i.e., dist(Ki, Kj) = min(tip, tjq) Complete link: largest distance between an element in one cluster and an element in the other, i.e., dist(Ki, Kj) = max(tip, tjq) Average: avg distance between an element in one cluster and an element in the other, i.e., dist(Ki, Kj) = avg(tip, tjq) Centroid: distance between the centroids of two clusters, i.e., dist(Ki, Kj) = dist(Ci, Medoid: distance between the medoids of two clusters, i.e., dist(Ki, Kj) = dist(Mi, Medoid: a chosen, centrally located object in the cluster Centroid, Radius and Diameter of a Cluster (for numerical data sets) Centroid: the “middle” of a cluster Radius: square root of average distance from any point of the cluster to its centroid Diameter: square root of average mean squared distance between all pairs of points in the cluster Extensions to Hierarchical Clustering Major weakness of agglomerative clustering methods Can never undo what was done previously Do not scale well: time complexity of at least O(n2), where n is the number of total objects Integration of hierarchical & distance-based clustering BIRCH. ( 1996): uses CF-tree and incrementally adjusts the quality of sub-clusters CHAMELEON. ( 1999): hierarchical clustering using dynamic modeling BIRCH. ( Balanced Iterative Reducing and Clustering Using Hierarchies) Zhang, Ramakrishnan & Livny, SIGMODʼ96 Incrementally construct a CF (Clustering Feature) tree, a hierarchical data structure for multiphase clustering Phase 1: scan DB to build an initial in-memory CF tree (a multi-level compression of the data that tries to preserve the inherent clustering structure of the data) Phase 2: use an arbitrary clustering algorithm to cluster the leaf nodes of the CF-tree Scales linearly: finds a good clustering with a single scan and improves the quality with a few additional scans Weakness: handles only numeric data, and sensitive to the order of the data record Clustering Feature Vector in BIRCH. Clustering Feature (CF): CF = (N, LS, SS) N:. Number of data points LS: linear sum of N points: SS: square sum of N points CF-Tree in BIRCH. Clustering feature: Summary of the statistics for a given subcluster: the 0-th, 1st, and 2nd moments of the subcluster from the statistical point of view Registers crucial measurements for computing cluster and utilizes storage eﬀiciently A CF tree is a height-balanced tree that stores the clustering features for a hierarchical clustering A non-leaf node in a tree has descendants or “children” The non-leaf nodes store sums of the CFs of their children A CF tree has two parameters Branching factor: max # of children (B, for non-leaf nodes and L for leaf nodes) Threshold (T): max diameter of sub-clusters stored at the leaf nodes The CF Tree Structure Figure: T. Zhang et al. The Birch Algorithm Cluster Diameter For each point in the input (processed in the order they appear, sequentially) Find closest leaf entry Add point to leaf entry and update CF If entry diameter > max_diameter, then split leaf (take the two farthest points in the leaf cluster and seeds for the two new clusters and re-assign points to the closest seed. If splitting the leaf exceeds it branching factor (L), add a new non-leaf entry to the parent. If this exceeds the parents branching factor (B), then split the parent (and recursively split other non-leaf nodes (grand parents) on the path as needed) Algorithm is O(n) each data point is processed only once, sequentially Concerns Sensitive to insertion order of data points Since we fix the size of leaf nodes, so clusters may not be so natural Clusters tend to be spherical given the radius and diameter measures CHAMELEON:. Hierarchical Clustering Using Dynamic Modeling (1999) CHAMELEON:. G. Karypis, E. H. Han, and V. Kumar, 1999 Measures the similarity based on a dynamic model Two clusters are merged only if the interconnectivity and closeness (proximity) between two clusters are high relative to the internal interconnectivity of the clusters and closeness of items within the clusters Graph-based, and a two-phase algorithm Use a graph-partitioning algorithm: cluster objects into a large number of relatively small sub-clusters Use an agglomerative hierarchical clustering algorithm: find the genuine clusters by repeatedly combining these sub-clusters Overall Framework of CHAMELEON CHAMELEON. ( Clustering Complex Objects) Probabilistic Hierarchical Clustering Algorithmic hierarchical clustering Nontrivial to choose a good distance measure Hard to handle missing attribute values Optimization goal not clear: heuristic, local search Probabilistic hierarchical clustering Use probabilistic models to measure distances between clusters Generative model: Considers the set of data objects to be clustered as a sample of some underlying data generation mechanism. Easy to understand, same eﬀiciency as algorithmic agglomerative clustering method, can handle partially observed data. In practice, assume the generative models adopt common distributions functions, e.g., Gaussian distribution or Bernoulli distribution, governed by parameters Generative Model Given a set of 1-D points X = {x1, ..., xn} for clustering analysis & assuming they are generated by a Gaussian distribution: The probability that a point xi ∈ X is generated by the model The likelihood that X is generated by the model: The task of learning the generative model is to find the parameters µ and σ2 that maximizes the likelihood. A Probabilistic Hierarchical Clustering Algorithm For a set of objects partitioned into m clusters C1, . . ,Cm, the quality can be measured by, where P() is the maximum likelihood Distance between clusters Ci and Cj:"
  },
  {
    "index": 6,
    "title": "6. Clustering II",
    "content": "1. Cluster analysis: Basic concepts 2. Partitioning methods 3. Hierarchical methods 4. Density-based methods 5. Grid-based methods 6. Evaluation of clustering 7. Summary and outlook Density-based methods DBSCAN OPTICS DENCLUE. Density-Based Clustering Methods Clustering based on density (local cluster criterion), such as density-connected points Major features: Discover clusters of arbitrary shape Handle noise One scan Need density parameters Several interesting studies: DBSCAN:. Ester, et al. (KDDʼ96) OPTICS:. Ankerst, et al (SIGMODʼ99). DENCLUE:. Hinneburg & D. Keim (KDDʼ98) CLIQUE:. Agrawal, et al. (SIGMODʼ98) (more grid-based) Density-Based Clustering: Basic Concepts Two parameters: Eps: Maximum radius of the neighbourhood of some datapoint p. MinPts: Minimum number of points in the Eps-neighbourhood (NEps(p) ) of that data point p. NEps(p): {q belongs to D | dist(p,q) ≤ Eps} If |NEps(p)| ≥ MinPts, then p is called a core point. Directly density-reachable: A point p is directly density-reachable from a point q w.r.t. Eps, MinPts if p belongs to NEps(q) core point condition: |NEps (q)| ≥ MinPts Density-Reachable and Density-Connected Density-reachable: A point p is density-reachable from a point q w.r.t. Eps, MinPts if there is a chain of points p1, ..., pn, p1 = q, pn = p such that pi+1 is directly density-reachable from pi Density-connected A point p is density-connected to a point q w.r.t. Eps, MinPts if there is a point o such that both, p and q are density-reachable from o w.r.t. Eps and MinPts DBSCAN DBSCAN:. Density-Based Spatial Clustering of Applications with Noise Relies on a density-based notion of cluster: A cluster is defined as a maximal set of density-connected points Discovers clusters of arbitrary shape in spatial databases with noise DBSCAN:. The Algorithm Arbitrary select a point p If p is a core point, Retrieve all points density-reachable from p w.r.t. Eps and MinPts Form a cluster If p is a border point, no points are density-reachable from p and DBSCAN. visits the next point of the database Continue the process until all of the points have been processed DBSCAN:. Sensitive to Parameters OPTICS OPTICS:. A Cluster-Ordering Method (1999) OPTICS:. Ordering Points To Identify the Clustering Structure Ankerst, Breunig, Kriegel, and Sander (SIGMODʼ99) Produces a special order of the database w.r.t. its density-based clustering structure. This cluster-ordering contains information equivalent to the density-based clusterings corresponding to a broad range of parameter settings. Good for both automatic and interactive cluster analysis, including finding intrinsic clustering structure. Can be represented graphically or using visualization techniques. OPTICS:. Some Extension from DBSCAN. Core Distance: Minimum radius s.t. point is core (undefined if point is not core w.r.t. Eps and MinPts) Reachability Distance from point o to p. = Max (core-distance (o), d (o, p)) OPTICS:. Some Extension from DBSCAN. Core Distance: Minimum radius s.t. point is core (undefined if point is not core w.r.t. Eps and MinPts) Reachability Distance from point o to p. = Max (core-distance (o), d (o, p)) To retrieve the Eps-neighbourhood of a point o, a region query with o as the center and Eps as the radius is used. Uses a special index such as R*-tree, X-tree or M-tree to do this eﬀiciently. Complexity: O(NlogN) Otherwise it will be O(N2) OPTICS:. Main Steps Maintains a list (priority queue) called ordered_seeds, where objects are sorted in the ascending order of their reachability distance to their respective closest core point. Picks an arbitrary datapoint p from the Dataset. Retrieves the Eps_neighbourhood(p) Computes the core_distance(p) and sets reachability_distance(p) = “undefined”. Write p to output (cluster order of data objects). If p is not a core object: move to next object in ordered_seeds (or from the dataset if ordered_seeds is empty). If p is a core object: For each object q in Eps_neighbourhood(p): If q is not in ordered_seeds: Set reachability_distance(q)= reachability_distance(p,q) and add q to the appropriate location in ordered_seeds If q is in ordered_seeds: Set reachability_distance(q)= min{reachability_distance(p,q), reachability_distance(q)} move q to its correct place in ordered_seeds if reachability_distance(q) has decreased Output of OPTICS. Density-Based Clustering: OPTICS. & Its Applications DENCLUE DENCLUE:. Using Statistical Density Functions DENsity-based CLUstEring by Hinneburg & Keim (KDDʼ98) Uses statistical density functions: DENCLUE:. Using Statistical Density Functions DENsity-based CLUstEring by Hinneburg & Keim (KDDʼ98) Using statistical density functions: Major features Solid mathematical foundation Good for data sets with large amounts of noise Allows a compact mathematical description of arbitrarily shaped clusters in high-dimensional data sets Significant faster than existing algorithm (e.g., DBSCAN). But needs a large number of parameters Density Attractor Density attractors - local density maxima Density attractor of point x the local density maximum reached by a gradient ascend starting from x. Density Attractor Density attractor of point x is the local density maximum reached by a gradient accent starting from x. DENCLUE. - Clustering Center based clusters The set of density attractors X, forms the set of cluster centers. Each datapoint is assigned to the cluster represented by its density attractor Arbitrary shaped clusters Density attractors are connected to merge clusters. Two density attractors are connected if there's a high density path between them. Center-Deﬁned and Arbitrary Denclue: Technical Essence Influence function: describes the impact of a data point within its neighborhood Overall density of the data space can be calculated as the sum of the influence function of all data points Clusters can be determined mathematically by identifying density attractors Density attractors are local maximum of the overall density function Center defined clusters: assign to each density attractor the points density attracted to it Arbitrary shaped cluster: merge density attractors that are connected through paths of high density (> threshold) Uses grid cells but only keeps information about grid cells that do actually contain data points and manages these cells in a tree-based access structure Grid-based Clustering Dr. Thanuja Ambegoda Senior Lecturer Department of Computer Science and Engineering University of Moratuwa Based on the textbook and slides by Jiawei Han, Micheline Kamber, and Jian Pei University of Illinois at Urbana-Champaign & Simon Fraser University Grid-Based Clustering Method Using multi-resolution grid data structure Several interesting methods STING. ( a STatistical INformation Grid approach) by Wang, Yang and Muntz (1997) WaveCluster by Sheikholeslami, Chatterjee, and Zhang (VLDBʼ98) A multi-resolution clustering approach using wavelet method CLIQUE:. Agrawal, et al. (SIGMODʼ98) Both grid-based and subspace clustering STING:. A Statistical Information Grid Approach Wang, Yang and Muntz (VLDBʼ97) The spatial area is divided into rectangular cells There are several levels of cells corresponding to diﬀerent levels of resolution The STING. Clustering Method Each cell at a high level is partitioned into a number of smaller cells in the next lower level Statistical info of each cell is calculated and stored beforehand and is used to answer queries count, mean, stdv, min, max type of distribution-normal, uniform, etc. Statistics of higher-level cells can be easily calculated from those of lower-level cell Use a top-down approach to answer spatial data queries STING. Algorithm and Its Analysis Start from a pre-selected layer-typically with a small number of cells. For each cell in the current level, compute how relevant it is to the query. E.g., use a confidence interval. Remove the irrelevant cells from further consideration. Proceed to the next level using only the relevant cells. Repeat this process until the bottom layer is reached. If the query specifications are met, return the regions of the relevant cells in the bottom layer. If not, the data points in the relevant cells are retrieved for further processing. STING. Algorithm and Its Analysis Advantages: Query-independent, easy to parallelize, incremental update O(K), where K is the number of grid cells at the lowest level Disadvantages: All the cluster boundaries are either horizontal or vertical, and no diagonal (or arbitrary) boundaries are detected CLIQUE. ( Clustering In QUEst) Agrawal, Gehrke, Gunopulos, Raghavan (SIGMODʼ98) Automatically identifying subspaces of a high dimensional data space that allow better clustering than original space. CLIQUE. can be considered as both density-based and grid-based It partitions each dimension into the same number of equal length intervals It partitions a d-dimensional data space into non-overlapping rectangular cells A cell(also called a unit) is dense if the fraction of total data points contained in the cell exceeds the some threshold value A cluster is a maximal set of connected dense cells within a subspace CLIQUE:. The Major Steps Partition the d-dimensional data space into non-overlapping rectangular cells. Identify dense cells in all subspaces. I.e., cells with at least l number of data points in them. Uses the apriori principle: A k-dimensional cell is dense only if all of its (k-1)-dimensional projections are dense. Find all 1-dimensional dense cells in all dimensions Iteratively join k-dimensional dense cells to form candidate (k+1)-dimensional dense cells. If c1 and c2 are two dense cells in k-dimensions [d1, d2,...dk-1, a] and [d1, d2,...dk-1, b], and share the same intervals in their common dimensions, then c1 and c2 are joined to form a candidate for a dense cell in the (k+1) dimensional space [d1, d2,...dk-1, a, b]. CLIQUE:. The Major Steps Once the dense cells in each subspace of interest are found, form clusters by connecting them. A cluster is a maximal set of connected dense cells in k-dimensions. Two k-dimensional cells c1,c2 are connected if they have a common face or if there exists another k-dimensional cell c3 such that c1 is connected to c3 and c2 is connected to c3. This is known to be an NP-hard problem. CLIQUE. uses a greedy approach to tackle it. Strength and Weakness of CLIQUE. Strengths automatically finds subspaces of the highest dimensionality such that high density clusters exist in those subspaces insensitive to the order of records in input and does not presume some canonical data distribution scales linearly with the size of input and has good scalability as the number of dimensions in the data increases Weakness The accuracy of the clustering result depends heavily on the choice of the number and widths of the grid cells and the density threshold. Chapter 10. Cluster Analysis: Basic Concepts and Methods Cluster Analysis: Basic Concepts Partitioning Methods Hierarchical Methods Density-Based Methods Grid-Based Methods Evaluation of Clustering Summary Assessing Clustering Tendency Assess if non-random structure exists in the data by measuring the probability that the data is generated by a uniform data distribution Test spatial randomness by statistic test: Hopkins Static Given a dataset D regarded as a sample of a random variable o, determine how far away o is from being uniformly distributed in the data space Sample n points, p1, ..., pn, uniformly from D. For each pi, find its nearest neighbor in D: xi = min{dist (pi, v)} where v in D Sample n points, q1, ..., qn, uniformly from D. For each qi, find its nearest neighbor in D - {qi}: yi = min{dist (qi, v)} where v in D and Calculate the Hopkins Statistic: If D is uniformly distributed, ∑ xi and ∑ yi will be close to each other and H is close to 0.5. If D is highly skewed, H is close to 0 Hopkins Statistic Determine the Number of Clusters Empirical method # of clusters ≈√n/2 for a dataset of n points Elbow method Use the turning point in the curve of sum of within cluster variance w.r.t the # of clusters Determine the Number of Clusters (2) Cross validation method Divide a given data set into m parts Use m - 1 parts to obtain a clustering model Use the remaining part to test the quality of the clustering ■E.g., For each point in the test set, find the closest centroid, and use the sum of squared distance between all points in the test set and the closest centroids to measure how well the model fits the test set For any k > 0, repeat it m times, compare the overall quality measure w.r.t. different k's, and find # of clusters that fits the data the best Cross validation method Measuring Clustering Quality Two methods: extrinsic vs. intrinsic Extrinsic: supervised, i.e., the ground truth is available ■Compare a clustering against the ground truth using certain clustering quality measure ■Ex. BCubed precision and recall metrics Intrinsic: unsupervised, i.e., the ground truth is unavailable ■Evaluate the goodness of a clustering by considering how well the clusters are separated, and how compact the clusters are ■Ex. Silhouette coefficient Silhouette coefficient Measuring Clustering Quality: Extrinsic Methods Clustering quality measure: Q(C, Cg), for a clustering C given the ground truth Cg. Q is good if it satisfies the following 4 essential criteria ■Cluster homogeneity: the purer, the better ■Cluster completeness: should assign objects belong to the same category in the ground truth to the same cluster ■Rag bag: putting a heterogeneous object into a pure cluster should be penalized more than putting it into a rag bag (i.e., “miscellaneous” or “other” category) ■Small cluster preservation: splitting a small category into pieces is more harmful than splitting a large category into pieces Chapter 10. Cluster Analysis: Basic Concepts and Methods Cluster Analysis: Basic Concepts Partitioning Methods Hierarchical Methods Density-Based Methods Grid-Based Methods Evaluation of Clustering Summary"
  },
  {
    "index": 7,
    "title": "7. Outlier Detection",
    "content": "Outlier Detection. Jian Pei Simon Fraser University Outlier Detection • “In 2017, an astronomical event occurred that was unlike any other: for the first time, we observed an object that we are certain originated from beyond our Solar System.” Jian Pei: Data Mining -- Outlier Detection 'Oumuamua Fraud detection • Credit card transaction fraud detection • Unusual amounts • Unusual locations/time • Fraud detection in stock markets • Example: unusual chain transactions by a small group of connected users • Fraud detection in healthcare insurance • A child and the parents frequently see the same medical doctor at the same time • A patient sees a medical doctor for flu every week Jian Pei: Data Mining -- Outlier Detection Outlier Analysis • “One person's noise is another person's signal” • Outliers: the objects considerably dissimilar from the remainder of the data Jian Pei: Data Mining -- Outlier Detection Outliers and Noise • Different from noise • Noise is random error or variance in a measured variable • Outliers are interesting: an outlier violates the mechanism that generates the normal data • Outlier detection vs. novelty detection • Early stage may be regarded as outliers • But later merged into the model Jian Pei: Data Mining -- Outlier Detection Types of Outliers • Three kinds: global, contextual and collective outliers • A data set may have multiple types of outlier • One object may belong to more than one type of outlier • Global outlier (or point anomaly) • An outlier object significantly deviates from the rest of the data set • challenge: find an appropriate measurement of deviation Jian Pei: Data Mining -- Outlier Detection Contextual Outliers • An outlier object deviates significantly based on a selected context • Ex. Is 10C in Vancouver an outlier? (depending on summer or winter?) • Attributes of data objects should be divided into two groups • Contextual attributes: defines the context, e.g., time & location • Behavioral attributes: characteristics of the object, used in outlier evaluation, e.g., temperature • A generalization of local outliers-whose density significantly deviates from its local area • Challenge: how to define or formulate meaningful context? Jian Pei: Data Mining -- Outlier Detection Collective Outliers • A subset of data objects collectively deviate significantly from the whole data set, even if the individual data objects may not be outliers • Application example: intrusion detection when a number of computers keep sending denial-of-service packages to each other • Detection of collective outliers • Consider not only behavior of individual objects, but also that of groups of objects • Need to have the background knowledge on the relationship among data objects, such as a distance or similarity measure on objects Jian Pei: Data Mining -- Outlier Detection Outlier Detection: Challenges • Modeling normal objects and outliers properly • Hard to enumerate all possible normal behaviors in an application • The border between normal and outlier objects is often a gray area • Application-specific outlier detection • Choice of distance measure among objects and the model of relationship among objects are often application-dependent • Example: in clinic data a small deviation could be an outlier; while in marketing analysis, larger fluctuations Jian Pei: Data Mining -- Outlier Detection Outlier Detection: Challenges • Handling noise in outlier detection • Noise may distort the normal objects and blur the distinction between normal objects and outliers • Noise may help hide outliers and reduce the effectiveness of outlier detection • Interpretability • Understand why these are outliers: Justification of the detection • Specify the degree of an outlier: the unlikelihood of the object being generated by a normal mechanism Jian Pei: Data Mining -- Outlier Detection Outlier Detection Methods • Whether user-labeled examples of outliers can be obtained • Supervised, semi-supervised, and unsupervised methods • Assumptions about normal data and outliers • Statistical, proximity-based, and clustering- based methods Jian Pei: Data Mining -- Outlier Detection Supervised Methods • Modeling outlier detection as a classification problem • Samples examined by domain experts used for training & testing • Methods for Learning a classifier for outlier detection effectively: • Model normal objects & report those not matching the model as outliers, or • Model outliers and treat those not matching the model as normal • Challenges • Imbalanced classes, i.e., outliers are rare: Boost the outlier class and make up some artificial outliers • Catch as many outliers as possible, i.e., recall is more important than accuracy (i.e., not mislabeling normal objects as outliers) Jian Pei: Data Mining -- Outlier Detection Unsupervised Methods • Assume the normal objects are somewhat ``clustered'' into multiple groups, each having some distinct features • An outlier is expected to be far away from any groups of normal objects • Weakness: Cannot detect collective outlier effectively • Normal objects may not share any strong patterns, but the collective outliers may share high similarity in a small area • Many clustering methods can be adapted for unsupervised methods • Find clusters, then outliers: not belonging to any cluster Jian Pei: Data Mining -- Outlier Detection Unsupervised Methods: Challenges Semi-Supervised Methods • In many applications, the number of labeled data is often small • Labels could be on outliers only, normal objects only, or both • If some labeled normal objects are available • Use the labeled examples and the proximate unlabeled objects to train a model for normal objects • Those not fitting the model of normal objects are detected as outliers • If only some labeled outliers are available, a small number of labeled outliers many not cover the possible outliers well • To improve the quality of outlier detection, one can get help from models for normal objects learned from unsupervised methods Jian Pei: Data Mining -- Outlier Detection Statistical/Model-based Methods • Make assumptions of data normality: normal data objects are generated by a statistical (stochastic) model, and that data not following the model are outliers • Effectiveness of statistical methods highly depends on whether the assumption of statistical model holds in the real data • There are many kinds of statistical models • Parametric vs. non-parametric Jian Pei: Data Mining -- Outlier Detection Proximity-based Methods • An object is an outlier if the nearest neighbors of the object are far away, i.e., the proximity of the object significantly deviates from the proximity of most of the other objects in the same data set • The effectiveness of proximity-based methods highly relies on the proximity measure • In some applications, proximity or distance measures cannot be obtained easily • Often have a difficulty in identifying a group of outliers that stay close to each other • Two major types of proximity-based outlier detection methods • Distance-based vs. density-based Jian Pei: Data Mining -- Outlier Detection Reconstruction-based Methods • Idea: sine the normal data samples often share certain similarities, they can often be represented in a more succinct way, compared with their original representation • Samples which cannot be well reconstructed by such alternative, succinct representation are regarded outliers • Two types of reconstruction-based outlier detection methods • Matrix-factorization based methods for numeric data • Pattern-based compression methods for categorical data Jian Pei: Data Mining -- Outlier Detection Statistical Approaches • Learn a generative model fitting the given data set, and then identify those objects in low-probability regions of the model as outliers • Two categories • A parametric method assumes that the normal data objects are generated by a parametric distribution with a finite number of parameters Θ • A nonparametric method tries to determine the model from the input data • A nonparametric method is not completely parameter-free Jian Pei: Data Mining -- Outlier Detection Detection of Univariate Outliers Based on Normal Distribution • Univariate outlier detection using maximum likelihood method • Example • A city's average temperature values in July in the last 10 years are, in value- ascending order, 24.0◦C, 28.9 ◦C, 28.9 ◦C, 29.0 ◦C, 29.1 ◦C, 29.1 ◦C, 29.2 ◦C, 29.2 ◦C, 29.3 ◦C, and 29.4 ◦C • Model assumption: the average tem- perature follows a normal distribution • Task: estimate the paramters 𝜇𝜇and 𝜎𝜎, that is, maximize the log-likelihood function Jian Pei: Data Mining -- Outlier Detection Maximum Likelihood Estimates • Results Jian Pei: Data Mining -- Outlier Detection Outlier Detection • The most deviating value, 24.0◦C, is 4.61◦C away from the estimated mean • Model assumption: μ ± 3σ region contains 99.7% data under the assumption of normal distribution • Because $.%$ = 3.04 > 3, the probability that the value 24.0◦C is generated by the normal distribution is less than 0.15% -- it is an outlier Jian Pei: Data Mining -- Outlier Detection Boxplot • A five-number summary: the smallest nonoutlier value (Min), the lower quartile (Q1), the median (Q2), the upper quartile (Q3), and the largest nonoutlier value (Max) • The interquantile range (IQR) is defined as • Any object that is more than 1.5 × IQR smaller than Q1 or 1.5 × IQR larger than Q3 is treated as an outlier because the region between Q1 - 1.5 × IQR and Q3 + 1.5 × IQR contains 99.3% of the objects Jian Pei: Data Mining -- Outlier Detection Grubb's Test (Maximum Normed Residual Test) • z-score 𝑧𝑧= • An object is an outlier if • where 𝑡𝑡⁄ is the value taken by a t-distribution at a significance level of α/(2n), and n is the number of objects in the data set Jian Pei: Data Mining -- Outlier Detection Detection of Multivariate Outliers Using the Mahalanobis Distance • Let ̅𝑜𝑜be the sample mean vector • For an object o, the squared Mahalanobis distance from o to ̅𝑜𝑜is Mdist o, ̅𝑜𝑜= 𝑜𝑜-̅𝑜𝑜+𝑆𝑆($(𝑜𝑜-̅𝑜𝑜), where S is the sample covariance matrix • Detect outliers using the univariate variable Mdist o, ̅𝑜𝑜 Jian Pei: Data Mining -- Outlier Detection Detection of Multivariate Outliers Using the χ2-statistic • For an object o, the χ2-statistic is • 𝑜𝑜) is the value of o on the i-th dimension • 𝐸𝐸) is the mean of the i-th dimension among all objects • n is the dimensionality • The larger the χ2-statistic, the more outlying the object Jian Pei: Data Mining -- Outlier Detection Detection Using a Mixture of Parametric Distributions • Model assumption: the normal data objects are generated by multiple normal distributions • Using EM (expectation-maximization) algorithm to learn the parameters Jian Pei: Data Mining -- Outlier Detection Non-parametric Method • Not assume an a-priori statistical model, instead, determine the model from the input data • Not completely parameter free but consider the number and nature of the parameters are flexible and not fixed in advance • Examples: histogram and kernel density estimation Jian Pei: Data Mining -- Outlier Detection Histogram • A transaction in the amount of $7,500 is an outlier, since only 0.2% transactions have an amount higher than • Hard to choose an appropriate bin size for histogram • Too small bin size → normal objects in empty/rare bins, false positive • Too big bin size → outliers in some frequent bins, false negative Jian Pei: Data Mining -- Outlier Detection Kernel Density Estimation • Treat an observed object as an indicator of high probability density in the surrounding region • The probability density at a point depends on the distances from this point to the observed objects • Use a kernel function to model the influence of a sample point within its neighborhood • A kernel K() is a non-negative real-valued integrable function that satisfies two conditions • 𝐾𝐾-𝑥𝑥= 𝐾𝐾(𝑥𝑥) for any x Jian Pei: Data Mining -- Outlier Detection Kernel Density Estimation • Using a kernel function K, estimate by • A frequently used kernel is a standard Gaussian function with mean 0 and variance 1 Jian Pei: Data Mining -- Outlier Detection Proximity-based Outlier Detection • Objects far away from the others are outliers • The proximity of an outlier deviates significantly from that of most of the others in the data set • Distance-based outlier detection: An object o is an outlier if its neighborhood does not have enough other points • Density-based outlier detection: An object o is an outlier if its density is relatively much lower than that of its neighbors Jian Pei: Data Mining -- Outlier Detection Distance-based Outliers • A DB(p, D)-outlier is an object O in a dataset T such that at least a fraction p of the objects in T lie at a distance greater than distance D from O • The larger D, the more outlying • The larger p, the more outlying Jian Pei: Data Mining -- Outlier Detection Drawback of Distance- based Methods • Both o1 and o2 are outliers • Distance-based methods can detect o1, but not o2 Jian Pei: Data Mining -- Outlier Detection Density-based Methods: Intuition • Outliers comparing to their local neighborhoods, instead of the global data distribution • The density around an outlier object is significantly different from the density around its neighbors • Use the relative density of an object against its neighbors as the indicator of the degree of the object being outliers Jian Pei: Data Mining -- Outlier Detection K-Distance • The k-distance of p is the distance between p and its k-th nearest neighbor • In a set D of points, for any positive integer k, the k-distance of object p, denoted as k-distance(p), is the distance d(p, o) between p and an object o such that • For at least k objects o' Î D \\ {p}, d(p, o') £ d(p, o) • For at most (k-1) objects o' Î D \\ {p}, d(p, o') < d(p, o) Jian Pei: Data Mining -- Outlier Detection K-distance Neighborhood • Given the k-stance of p, the k-distance neighborhood of p contains every object whose distance from p is not greater than the k-distance • Nk-distance(p)(p) = {q Î D\\{p} | d(p, q) £ k-distance(p)} • Nk-distance(p)(p) can be written as Nk(p) Jian Pei: Data Mining -- Outlier Detection Jian Pei: Data Mining -- Outlier Detection Reachability Distance • The reachability distance of object p with respect to object o is reach- distk(p, o) = max{k-distance(o), d(p, o)} If p and o are close to each other, reach-dist(p, o) is the k-distance, otherwise, it is the real distance Local Reachability Density Jian Pei: Data Mining -- Outlier Detection Local outlier factor lrdk(o) = o02Nk(o) reachdistk(o0 ←o) Reconstruction-based Approaches • Find a succinct representation • Use the succinct representation to reconstruct the original data samples • Measure the quality (i.e., goodness) of reconstruction Jian Pei: Data Mining -- Outlier Detection Matrix Factorization Based Methods for Numeric Data Jian Pei: Data Mining -- Outlier Detection Singular Vector Decomposition (SVD) Jian Pei: Data Mining -- Outlier Detection Clustering-based Outlier Detection • An object is an outlier if • It does not belong to any cluster; • There is a large distance between the object and its closest cluster ; or • It belongs to a small or sparse cluster Jian Pei: Data Mining -- Outlier Detection Classification-based Outlier Detection • Train a classification model that can distinguish “normal” data from outliers • A brute-force approach: Consider a training set that contains some samples labeled as “normal” and others labeled as “outlier” • A training set in practice is typically heavily biased: the number of “normal” samples likely far exceeds that of outlier samples • Cannot detect unseen anomaly Jian Pei: Data Mining -- Outlier Detection One-Class Model • A classifier is built to describe only the normal class • Learn the decision boundary of the normal class using classification methods such as SVM • Any samples that do not belong to the normal class (not within the decision boundary) are declared as outliers • Advantage: can detect new outliers that may not appear close to any outlier objects in the training set • Extension: Normal objects may belong to multiple classes Jian Pei: Data Mining -- Outlier Detection Semi-Supervised Learning Methods • Combine classification-based and clustering-based methods • Method • Use a clustering-based approach to find a large cluster, C, and a small cluster, C1 • Since some objects in C carry the label “normal”, treat all objects in C as normal • Use the one-class model of this cluster to identify normal objects in outlier detection • Since some objects in cluster C1 carry the label “outlier”, declare all objects in C1 as outliers • Any object that does not fall into the model for C (such as a) is considered an outlier as well Jian Pei: Data Mining -- Outlier Detection Contextual Outliers • An outlier object deviates significantly based on a selected context • Ex. Is 10C in Vancouver an outlier? (depending on summer or winter?) • Attributes of data objects should be divided into two groups • Contextual attributes: defines the context, e.g., time & location • Behavioral attributes: characteristics of the object, used in outlier evaluation, e.g., temperature • A generalization of local outliers-whose density significantly deviates from its local area • Challenge: how to define or formulate meaningful context? Jian Pei: Data Mining -- Outlier Detection Detection of Contextual Outliers • If the contexts can be clearly identified, transform it to conventional outlier detection • Identify the context of the object using the contextual attributes • Calculate the outlier score for the object in the context using a conventional outlier detection method Jian Pei: Data Mining -- Outlier Detection Example • Detect outlier customers in the context of customer groups • Contextual attributes: age group, postal code • Behavioral attributes: the number of transactions per year, annual total transaction amount • Method • Locate c's context; • Compare c with the other customers in the same group; and • Use a conventional outlier detection method Jian Pei: Data Mining -- Outlier Detection Modeling Normal Behavior • Model the “normal” behavior with respect to contexts • Use a training data set to train a model that predicts the expected behavior attribute values with respect to the contextual attribute values • An object is a contextual outlier if its behavior attribute values significantly deviate from the values predicted by the model • Use a prediction model to link the contexts and behavior • Avoid explicit identification of specific contexts • Some possible methods: regression, Markov Models, and Finite State Automaton ... Jian Pei: Data Mining -- Outlier Detection Collective Outliers • Objects as a group deviate significantly from the entire data • Examine the structure of the data set, i.e, the relationships between multiple data objects • The structures are often not explicitly defined and have to be discovered as part of the outlier detection process. Jian Pei: Data Mining -- Outlier Detection Detecting High Dimensional Outliers • Interpretability of outliers • Which subspaces manifest the outliers or an assessment regarding the “outlying-ness” of the objects • Data sparsity: data in high-D spaces are often sparse • The distance between objects becomes heavily dominated by noise as the dimensionality increases • Data subspaces • Local behavior and patterns of data • Scalability with respect to dimensionality • The number of subspaces increases exponentially Jian Pei: Data Mining -- Outlier Detection HilOut • Find distance-based outliers, but uses the ranks of distance instead of the absolute distance in outlier detection • For each object, o, find the k-nearest neighbors of o, denoted by nn1(o), . . , nnk(o), where k is an application-dependent parameter • The weight of o is • All objects are ranked in weight-descending order • The top-l objects in weight are outliers Jian Pei: Data Mining -- Outlier Detection Angle-based Outliers Jian Pei: Data Mining -- Outlier Detection Summary • Outlier detection and applications • Types of outliers • Statistical methods • Proximity-based methods • Clustering- and classification-based methods • Contextual outlier and collective outliers • Outlier detection for high-dimensional data Jian Pei: Data Mining -- Outlier Detection"
  },
  {
    "index": 8,
    "title": "8. NoSQL",
    "content": "Introduction NoSQL. Databases Slides based on multiple sources Outline • SQL Databases - SQL Standard - SQL Characteristics • NoSQL Databases - NoSQL Defintion - General Characteristics - NoSQL Database Types - NoSQL Database Examples What is in the Name: NoSQL • Is it No to SQL ? • Not Only SQL NOSQL. - Defined by what it is Not • “Any database that is not a Relational Database” • The term was coined at a meetup with the creators behind some • prominent emerging databases • “Non-Relational Databases” might be more correct • - But it's a mouthful! - ... then there was a conference ... - ... and a mailing list ... - ... the name caught on ... - ... then there were more conferences ... - ... and here we are! Why NoSQL: Trend 1 Data Size Why NoSQL: Trend 2 Connectedness Why NoSQL: Trend 3 Semi-structure Individualization of content In the salary lists of the 1970s, all elements had exactly one job In the salary lists of the 2000s, we need 5 job columns! Or 8? Or 15? All encompassing “entire world views” Store more data about each entity Trend accelerated by the decentralization of content generation that is the hallmark of the age of participation (“web 2.0”) Why NoSQL: Trend 4 Architecture Limitations of RDBMS. Evolution of NoSQL DBs Why NoSQL ACID doesn't scale well Web apps have different needs (than the apps that RDBMS. were designed for) Low and predictable response time (latency) Scalability & elasticity (at low cost!) High availability Flexible schemas / semi-structured data Geographic distribution (multiple datacenters) Web apps can (usually) do without Transactions / strong consistency / integrity Complex queries NoSQL use cases 1. Massive data volumes - Massively distributed architecture required to store the data - Google, Amazon, Yahoo, Facebook - 10-100K servers 2. Extreme query workload 3. Impossible to efficiently do joins at that scale with an RDBMS. - Schema evolution - Schema flexibility (migration) is not trivial at large scale - Schema changes can be gradually introduced with NoSQL Dis / Advantages of NoSQL • Advantages - Massive scalability - High availability - Lower cost (than competitive solutions at that scale) - (usually) predictable elasticity - Schema flexibility, sparse & semi-structured data • Disadvantages - Limited query capabilities (so far) - Eventual consistency is not intuitive to program for - Makes client applications more complicated - No standardizatrion - Portability might be an issue - Insufficient access control SQL Characteristics • Data stored in columns and tables • Relationships represented by data • Data Manipulation Language • Data Definition Language • Transactions • Abstraction from physical layer SQL Physical Layer Abstraction • Applications specify what, not how • Query optimization engine • Physical layer can change without modifying applications - Create indexes to support queries - In Memory databases Data Manipulation Language (DML) • Data manipulated with Select, Insert, Update, & Delete statements - Select T1.Column1, T2.Column2 ... From Table1, Table2 ... Where T1.Column1 = T2.Column1 ... • Data Aggregation • Compound statements • Functions and Procedures • Explicit transaction control Data Definition Language • Schema defined at the start • Create Table (Column1 Datatype1, Column2 Datatype 2, ...) • Constraints to define and enforce relationships - Primary Key - Foreign Key - Etc. • Triggers to respond to Insert, Update , & Delete • Stored Modules • Alter ... • Drop ... • Security and Access Control Transactions - ACID Properties • Atomic - All of the work in a transaction completes (commit) or none of it completes • Consistent - A transaction transforms the database from one consistent state to another consistent state. Consistency is defined in terms of constraints. • Isolated - The results of any changes made during a transaction are not visible until the transaction has committed. • Durable - The results of a committed transaction survive failures Brewer's CAP Theorem (E. Brewer, N. Lynch) A distributed system can support only two of the following characteristics: •Consistency •Availability •Partition tolerance Consistency • all nodes see the same data at the same time • client perceives that a set of operations has occurred all at once • More like Atomic in ACID transaction properties Availability - Node failures do not prevent survivors from continuing to operate - Every operation must terminate in an intended response Partition Tolerance - the system continues to operate despite arbitrary message loss • Operations will complete, even if individual components are unavailable Image Source : http://blog.nosqltips.com BASE Transactions • Acronym contrived to be the opposite of ACID - Basically Available, - Soft state, - Eventually Consistent • Characteristics - Weak consistency - stale data OK - Availability first - Best effort - Approximate answers OK - Aggressive (optimistic) - Simpler and faster BASE Model Focuses on Partition tolerance and availability and throws consistency out the window Basic Availability : - This constraint states that the system does guarantee the availability of the data as regards CAP Theorem Soft State : - The state of the system could change over time and called as 'soft' state Eventual Consistency : - The system will eventually become consistent once it stops receiving input. The data will propagate to everywhere it should sooner or later, but the system will continue to receive input and is not checking the consistency of every transaction before it moves onto the next one. NoSQL Database Types Discussing NoSQL databases is complicated because there are a variety of types: • Column Store - Each storage block contains data from only one column • Document Store - stores documents made up of tagged elements • Key-Value Store - Hash table of keys Other Non-SQL Databases • XML Databases • Graph Databases • Codasyl Databases • Object Oriented Databases • Etc... NoSQL Example: Column Store • Each storage block contains data from only one column • Example: Hadoop/Hbase - http://hadoop.apache.org/ - Yahoo, Facebook • Example: Ingres VectorWise - Column Store integrated with an SQL database - http://www.ingres.com/products/vectorwise Column Store Comments • More efficient than row (or document) store if: - Multiple row/record/documents are inserted at the same time so updates of column blocks can be aggregated - Retrievals access only some of the columns in a row/record/document Ex. Column Store Google BigTable • Google, ~2006 • Sparse, distributed, persistent multidimensional sorted map • (row, column, timestamp) dimensions, value is string • Key features - Hybrid row/column store - Single master (stand-by replica) - Versioning Google BigTable (Data Model) • Row - Keys are arbitrary strings - Data is sorted by row key • Tablet - Row range is dynamically partitioned into tablets (sequence of rows) - Range scans are very efficient - Row keys should be chosen to improve locality of data access • Column, Column Family - Column keys are arbitrary strings, unlimited number of columns - Column keys can be grouped into families - Data in a CF is stored and compressed together (Locality Groups) - Access control on the CF level Google BigTable (Data ModelCont.) • Timestamps - Each cell has multiple versions - Can be manually assigned • Versioning - Automated garbage collection - Retain last N versions - Retain versions newer than TS • Architecture - Data stored on GFS - Relies on Chubby (distributed lock service, Paxos) - 1 Master server - thousands of Tablet servers Google BigTable (Architecture) • Master server - Assign tablets to Tablet Servers - Balance TS load - Garbage collection - Schema management - Client data does not move through the MS (directly through TS) - Tablet location not handled by MS • Tablet server (many) - thousands of tablets per TS - Manages Read / Write / Split of its tablets Google BigTable (Performance) NoSQL Examples: Key-Value Store • Hash tables of Keys • Values stored with Keys • Fast access to small data values • Example - Project-Voldemort - http://www.project-voldemort.com/ - Linkedin • Example - MemCacheDB - http://memcachedb.org/ - Backend storage is Berkeley-DB Key-Value Store Store items as alpha-numeric identifiers(keys) and associated values in simple, standalone tables ( Hash table) The values can be simple text strings or more complex list and sets. Query can usually only be performed against keys and it limited to exact matches Primary Use : - Manage user profile or session or retrieving product names - Amazon core services use Dynamo as distributed data store Example Key Value Store: Dynamo • Amazon, ~2007 • P2P key-value store - Object versioning - Consistent hashing - Gossip - membership & failure detection - Quorum reads • Requirements & assumptions - Simple query model (unique keys, blobs, no schema, no multi-access) - Scale out (elasticity) - Eventual consistency (improved availability) - Decentralized & symmetric (P2P), heterogeneous (load distribution) - Low latency / high throughput - Internal service (no security model) NoSQL Example: Document Store • Example: CouchDB - http://couchdb.apache.org/ - BBC • Example: MongoDB - http://www.mongodb.org/ - Foursquare, Shutterfly • JSON - JavaScript Object Notation Document Store Example: CouchDB • Schema-free, document oriented database - Documents stored in JSON format (XML in old versions) - B-tree storage engine - MVCC model, no locking - no joins, no PK/FK (UUIDs are auto assigned) - Implemented in Erlang - 1st version in C++, 2nd in Erlang and 500 times more scalable (source: “Erlang Programming” by Cesarini & Thompson) - Replication (incremental) • Documents - UUID, version - Old versions retained CouchDB JSON Example \"_id\": \"guid goes here\", \"_rev\": \"314159\", \"type\": \"abstract\", \"author\": \"Keith W. Hare\" \"title\": \"SQL Standard and NoSQL Databases\", \"body\": \"NoSQL databases (either no-SQL or Not Only SQL) are currently a hot topic in some parts of computing.\", \"creation_timestamp\": \"2011/05/10 13:30:00 +0004\" CouchDB JSON Tags - GUID - Global Unique Identifier - Passed in or generated by CouchDB • \"_rev\" - Revision number - Versioning mechanism • \"type\", \"author\", \"title\", etc. - Arbitrary tags - Schema-less - Could be validated after the fact by user-written routine MapReduce on CouchDB MapReduce • Technique for indexing and searching large data volumes • Two Phases, Map and Reduce - Map • Extract sets of Key-Value pairs from underlying data • Potentially in Parallel on multiple machines - Reduce • Merge and sort sets of Key-Value pairs • Results may be useful for other searches MapReduce • Map Reduce techniques differ across products • Implemented by application developers, not by underlying software Map Reduce Patent Google granted US Patent 7,650,331, January 2010 System and method for efficient large-scale data processing A large-scale data processing system and method includes one or more application-independent map modules configured to read input data and to apply at least one application-specific map operation to the input data to produce intermediate data values, wherein the map operation is automatically parallelized across multiple processors in the parallel processing environment. A plurality of intermediate data structures are used to store the intermediate data values. One or more application- independent reduce modules are configured to retrieve the intermediate data values and to apply at least one application-specific reduce operation to the intermediate data values to provide output data. NoSQL “Definition” From www.nosql-database.org: Next Generation Databases mostly addressing some of the points: being non-relational, distributed, open-source and horizontal scalable. The original intention has been modern web-scale databases. The movement began early 2009 and is growing rapidly. Often more characteristics apply as: schema-free, easy replication support, simple API, eventually consistent / BASE (not ACID), a huge data amount, and more. NoSQL Distinguishing Characteristics • Large data volumes - Google's “big data” • Scalable replication and distribution - Potentially thousands of machines - Potentially distributed around the world • Queries need to return answers quickly • Mostly query, few updates • Asynchronous Inserts & Updates • Schema-less • ACID transaction properties are not needed - BASE • CAP. Theorem • Mostly Open Source development Storing and Modifying Data • Syntax varies - HTML - Java Script - Etc. • Asynchronous - Inserts and updates do not wait for confirmation • Versioned • Optimistic Concurrency Retrieving Data • Syntax Varies - No set-based query language - Procedural program languages such as Java, C, etc. • Application specifies retrieval path • No query optimizer • Quick answer is important • May not be a single “right” answer Adoption of NoSQL Database • Couchbase survey was conducted in the 2012. NoSQL Summary • NoSQL databases reject: - Overhead of ACID transactions - “Complexity” of SQL - Burden of up-front schema design - Declarative query expression • Programmer responsible for - Step-by-step procedural language - Navigating access path Image Source : http://blog.flux7.com/blogs/nosql/cap-theorem-why-does-it-matter Databases Landscape with respect to Availability, Consistency, Partition Tolerance Database Ranking NOTE: Most of the major Relational DB Vendors have included NoSQL components to their solutions to stay ahead of the competition. Image captured at : http://db-engines.com/en/ranking Books • “CouchDB The Definitive Guide”, J. Chris Anderson, Jan Lehnardt and Noah Slater. O'Reilly Media Inc., Sebastopool, CA, USA. 2010 • “Hadoop The Definitive Guide”, Tom White. O'Reilly Media Inc., Sebastopool, CA, USA. 2011 • “MongoDB The Definitive Guide”, Kristina Chodorow and Michael Dirolf. O'Reilly Media Inc., Sebastopool, CA, USA. 2010"
  },
  {
    "index": 9,
    "title": "9. NoSQL Examples",
    "content": "Class 7 Firestore Elements of Databases Mar 25, 2022 Slides Taken from https://www.cs.utexas.edu/~scohen/cs327e.html The \"NoSQL Movement\" Need for greater scalability Throughput Response time More expressive data models and schema flexibility Object-relational mismatch Preference for open-source software Source: schema.org Firestore Overview + Distributed system + Fully \"serverless\" + Simple APIs for reading and writing + Supports ACID transactions (uses Spanner behind the scenes) + Designed for mobile, web and IoT apps + Implements document model + Change data capture for documents + Inexpensive - Only runs on Google Cloud - Write throughput limits (10K writes/sec) Firestore's Document Model Firestore document == collection of typed <key, value> pairs Primitive types: String, Number, Bool, Timestamp Complex types: Array, Map, Geopoint Documents Concepts: grouped into collections same type documents can have different schemas assigned unique identifiers (id) store hierarchical data in subcollections Writing Single Documents Every document has unique identifier of String type The set method converts a Python dictionary into Firestore document A document write must also update any existing indexes on the collection Writing Nested Documents Subcollections are nested under documents Subcollections can be nested under other subcollections (max depth = 100) Writing Multiple Documents Write batches of documents in increments of 500 or less Batches can contain documents for multiple collections Reading Single Documents The get method fetches a single document The stream method fetches all documents in collection or subcollection stream + where methods filter documents in collection or subcollection order_by method for sorting results limit method for limiting number retrieved All reads require an index, query will fail if an index does not exist Reading Multiple Documents from google.cloud import firestore db= firestore.Client() authors_ref = db.collection('authors') query= authors_ref.where('seniority', results= query.stream() 8T for doc in results: print('Document: ' + str(doc.to_dict())) from google.cloud import firestore db= firestore.Client() authors_ref = db.collection('authors') 'L3').order_by('last_name').limit(S) query= authors_ref.where( 'secondary_specialties', 'array_contains', 'Business') \\ .where('articles_to_date', '>', 100) results= query.stream() 9T for doc in results: print('Document: ' + str(doc.to_dict())) Updates and Deletes from google.cloud import firestore db= firestore.Client() author_ref = db.collection( 1 authors 1).document( 1sarah.asch 1 ) author _ref. update ( { 1 a rticles_to_date 1 : f irestore. Increment ( 1), 1 seniority 1 from google.cloud import firestore db= firestore.Client() author_ref = db.collection( 1 authors 1 ).document( 1 sarah.asch 1 author_ref.update({ 1 articles_to_date 1 : firestore.DELETE_FIELD}) from google.cloud import firestore db= firestore.Client() db.collection( 1authors 1 ).document( 1 sarah.asch 1 ).delete() College Schema Convert this relational model to Firestore. Read access patterns: Get classes by cname Get students and their classes by sid Get instructor and their classes by tid College Schema Access patterns: Get classes by cname Get students and their classes by sid Get instructor and their classes by tid Converted relational model to Firestore. Design Guidelines for Document Databases Identify the expected access patterns against the database. For each access pattern, group entities into a hierarchy: top-level and lower-level types. Convert each top-level entity into a Firestore collection. Convert each lower-level entity into a Firestore subcollection nested in its parent collection. Construct a single unique identifier for each document by using the Primary Key column as is or concatenating multiple Primary Key columns. Exercise: Data Modeling Convert Shopify schema to Firestore. Access patterns: Get apps by category (Category.title) Get apps with highest review_count Get pricing plan details by app (Apps.id) Get key benefits by app (Apps.id) Class 7 MongoDB Elements of Databases Apr 1, 2022 Slides Taken from https://www.cs.utexas.edu/~scohen/cs327e.html The CAP Theorem Theorem: You can have at most two of these properties for any database system. Eric Brewer, PODC keynote, July 2000. MongoDB Overview Distributed database system Open-source software (sponsored by MongoDB Inc.) Designed for storing and processing web data Document-oriented data model \"Schemaless\" (schema-on-reads) Rich query language Secondary indexes Horizontal scaling through replication and sharding Runs on-premise and in cloud (Atlas offering) Primary datastore for many web applications Multi-document transaction support Sharding is not automatic Replication in MongoDB High-availability Redundancy Automatic failovers Load balancing reads Sharding in MongoDB shard key = one or more fields of a document which determine how documents get sliced Documents with the same shard key are assigned to the same chunk Chunks are assigned to a shard Key Range Chunk Shard Sharding + Replication Each shard is deployed as a replica set Scales both reads and writes Widely used in prod environments Data Model MongoDB Document = BSON object Unordered key/value pairs with nesting Documents have unique identifiers (_id) Data types: String, Int, Double, Boolean, Date, Timestamp, Array, Object, ObjectId Documents are nested via Object type Max document size: 16 MB (including nested objects) Documents grouped into collections Collections grouped into databases \"_id\" : ObjectId(\"5f807ab092ea454d1100d13a\"), \"name\" : { \"first\" : \"Jim\", \"last\" : \"Gray\" \"nationality\" : \"American\", \"born\" : Date(\"1944-01-12\"), \"employers\" : [ \"Microsoft\", \"DEC\", \"Tandem\", \"IBM\" \"contributions\" : [ \"database transactions\", \"OLAP cube\" Inserts db.coll.insertOne(document) db.coll.insert([document1, document2, documentn]) db.coll.insertMany([document1, document2, documentn]) Legend: Document, Output Inserts Legend: Document, Output > doc2 = {\"company name\": \"Google Inc.\", \"exchange\": \"NASDAQ\",. \" symbol\": \"GOOG\", \"summary\": {\"date\": 20211022, \"open\": 2807.02, \"high\": 2831.17, \"low\": 2743.41}} \"company name\" : \"Google Inc.\", \"exchange\" : \"NASDAQ\",. \" symbol\" : \"GOOG\", \"summary\" : { \"date\" \"open\" \"high\" \"low\" : 2743.41 > doc3 = {\"company name\": \"Google Inc.\", \"symbol\": \"GOOG\", \"exchange\": \"NASDAQ\",. \" summary\": [{\"date\": 20201007, \"open\" 1464.29, \"high\": 1468.96, \"low\": 1461.47}, {\"date\": 20201006, \"open\": 1476.89, \"high\": 1480.93, \"low\": 1453.44}]} \"company name\" : \"Google Inc.\", \"symbol\" : \"GOOG\", \"exchange\" : \"NASDAQ\",. \" summary\" : [ \"date\" ''open'' \"high\" \"low\" : 1461.47 > db.rnarket.insertMany([doc2, doc3)) \"acknowledged\" : true, \"date\" ''open'' \"high\" \"insertedids\" Objectid(\"624744a17dbfdcf5f676721c\"), Objectid(\"624744a17dbfdcf5f676721d\") \"low\" : 1453.44 Reads db.coll.findOne(selection, projection) db.coll.find(selection, projection) Legend: Selection, Projection, Output Nested Queries Legend: Selection, Projection, Output >selection= {\"summary.date\": 20211022} { \"summary.date\" : 20211022 } > projection = {\"summary. date\" : 1, \"summary. open\" : 1, \"summary. high\" : { \"summary.date\" : 1, \"summary.open\" : 1, \"summary.high\" : 1, \" id\" > db.market.find(selection, proJection) { \"summary\" : { \"date\" : 20211022, \"open\" : 2807.02, \"high\" : 2831.17 > selection = {\"summary.date\": 20211022, \"symbol\": \"GOOG\"} { \"summary.date\" : 20211022, \"symbol\" : \"GOOG\" } > proJec ion summary. a e : summary.open : summary. { \"summary.date\" : 1, \"summary.open\" : 1, \"summary.high\" : 1, \" id\" ection, proJection).pretty() \"summary\" : \"date\" \"open\" \"high\" Or Queries Boolean Operators: $and Legend: Selection, Projection, Output Range Queries Range operators: $lte $gte Legend: Selection, Projection, Output Updates db.coll.update(selection, update) db.coll.updateMany(selection, update) Legend: Selection, Update Deletes db.coll.deleteOne(selection) db.coll.deleteMany(selection) Legend: Selection, Output Class 7 Neo4j Elements of Databases Apr 8, 2022 Slides Taken from https://www.cs.utexas.edu/~scohen/cs327e.html Neo4j Overview Labeled property graph database Highly connected data Declarative, SQL-inspired query language (Cypher) Open-source, sponsored by Neo4j Inc. Rich plugin and extension language (similar to Postgres) ACID-compliant transactions Distributed architecture for scaling reads Visualization tools (Neo4j Browser, Bloom) Optimized for graph traversals Available as a cloud offering (Aura) Limited scalability for writes (no sharding) “Hello World” in Cypher 1 CREATE ();. 2 CREATE (:. Person); 3 CREATE (:. Place); 5 MATCH(. n) RETURN. n; 7 CREATE (:. Person {name: 11Ethan11})-[:LIVES_IN]->(:Place {city: 11Austin11 8 CREATE (:. Person {name: 11Sasha11})-[:LIVES_IN]->(:Place {city: 11New York11 10 MATCH. ( p)-[r]->(c) 11 RETURN. p, type(r), c; 13 MATCH ()-[. r]->() 14 RETURN. type(r), COUNT(. r); 16 MATCH. ( p)-[r:LIVES_IN]->(c) 17 WHERE. p.name = 11Ethan11 18 AND c. city = 11Austin1 19 RETURN. p, r, c; IAM model: a labeled property graph example name: Ethan HAS_ROLE name: Project Owner HAS_PERMISSION name: storage.list name: storage.create name: storage.delete IN_GROUP name: Data Engineer name: DB Editor HAS_PERMISSION name: jobs.get name: jobs.create Creating the IAM Nodes Creating the Relationships 1 MATCH. ( p:Person {name: 1 1Ethan11 2 MATCH. ( r:Role {name: 11Owner11 3 CREATE. ( p)-[:HAS_ROLE]->(r); 5 MATCH. ( p:Person {name: 1 1Ethan11 6 MATCH. ( g:Group {name: 11Data Engineer11 7 CREATE. ( p}-[:IN_GROUP]->(g); 9 MATCH. ( g: Group {name: 11 Data Eng inee r11 10 MATCH. ( r:Role {name: 11DB Editor11 11 CREATE. ( g)-[:HAS_ROLE]->(r); 13 MATCH. ( p)-[h]->(r) RETURN. p, h, r; name:Owner V name: Data Engineer name: storage.create I (:Person {name: \"Ethan\", email: \"ethan@utexas.edu\"}) I [:IN_GROUPJ I (:Group {owner: \"Alex\", name: \"Data Engineer\"}) I (:Person {name: \"Ethan\", email: \"ethan@utexas.edu\"}) I [:HAS_ROLEJ I (:Role {name: \"Owner\", resource: \"Project\"}) I (:Group {owner: \"Alex\", name: \"Data Engineer\"}) I [:HAS_ROLEJ I (:Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) Creating the Relationships (cont.) 16 MATCH. ( r:Role {resource: 11Project11 17 MATCH. ( p:Permission {name: 11storage. list11 18 CREATE. ( r)-[:HAS_PERMISSION]->(p); 20 MATCH. ( r:Role {resource: 11Project11 21 MATCH. ( p:Permission {name: 11storage.create11 22 CREATE. ( r)-[:HAS_PERMISSION]->(p); 24 MATCH. ( r: Ro le {name: 110wne r11 25 MATCH. ( p:Permission {name: 1 1storage.delete11 26 CREATE. ( r)-[:HAS_PERMISSION]->(p); 28 MATCH. ( r:Role)-[h]->(p:Permission) 29 WHERE. r. resource = 11 Project II OR r. name 30 RETURN. r, h, p; 110wner11 name: Owner Role name: Data Engineer HAS_PE lSSION name: storage.list I ( :Role {name: \"Owner\", resource: \"Project\"}) I [ :HAS_PERMISSIONJ I ( :Permission {name: \"storage.delete\"}) I I ( :Role {name: \"Owner\", resource: \"Project\"}) I [ :HAS_PERMISSIONJ I ( :Permission {name: \"storage.create\"}) I I ( :Role {name: \"Owner\", resource: \"Project\"}) I ( :HAS_PERMISSIONJ I ( :Permission {name: \"storage.list\"}) Creating the Relationships (cont.) MATCH. { r:Role {name: \"DB Editor\"}) MATCH. ( p:Permission {name: \"jobs.list\"}) CREATE. { r)-[:HAS_PERMISSION]->{p); MATCH. ( r:Role {name: \"DB Editor\"}) MATCH. ( p:Permission {name: \"jobs.get\"}) CREATE. ( r)-[:HAS_PERMISSION]->{p); MATCH. ( r:Role {name: \"DB Editor\"}) MATCH. { p:Permission {name: \"jobs.create\"}) CREATE. { r)-[:HAS_PERMISSION]->{p); HAS_ OLE IN_GRO Group name: Owner Role name: Data Engineer HAS_PERMISSION name: storage.create name: storage.delete 45 MATCH. { r:Role)-[h:HAS_PERMISSION]->{p:Permission) 46 WHERE. r.name = \"DB Editor\" 47 RETURN. r, h, p; name: DB Editor HAS_PERMISSION I (:Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) I [:HAS_PERMISSION] I (:Permission {name: \"jobs.create\"}) I (:Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) I [:HAS_PERMISSION] I (:Permission {name: \"jobs.get\"}) I ( :Role {name: \"DB Editor\", resource: \"Cloud SQL\"}) I [ :HAS_PERMISSION] I ( :Permission {name: \"jobs.list\"}) Visualizing the Graph CD localhost:7474/browser/ neo4j$ MATCH. ( n) RETURN. n LIMIT. 25 Parmission(6) HAS PERMISSION(. 6) Table Code Permission <id>: 6 name: jobs.create jobs.ere .. Counting Nodes and Relationships Querying the Graph 1 MATCH. ( p:Person)-[r*]->(m:Permission) 2 WHERE. p. name = 11Ethan11 3 RETURN. r, m.name 4 ORDER BY. m; I m.name [ [ :IN_GROUP], [ :HAS_ROLE], [ :HAS_PERMISSION] l \"jobs.list\" [ [ :IN_GROUP], [ :HAS_ROLE], [ :HAS_PERMISSION] l \"jobs.get\" [ [:IN_GROUPJ , [:HAS_ROLEJ , [:HAS_PERMISSION]] \"jobs.create\" [ [ :HAS_ROLE], [ :HAS_PERMISSION]] \"storage.list\" [ [ :HAS_ROLE], [ :HAS_PERMISSION] l \"storage.create\" [ [ :HAS_ROLE], [ :HAS_PERMISSION]] \"storage.delete\" 6 MATCH. ( p:Person)-[r*]->(m:Permission) 7 WHERE. p.name = \"Ethan\" 8 WITH distinct m.name as distinct_perms 9 RETURN. distinct_perms 10 ORDER BY. distinct_perms; name: Data Engineer HAS_PERMISSION I distinct_perms \"jobs.create\" \"jobs.get\" \"jobs.list\" \"storage.create\" \"storage.delete\" \"storage.list\" Querying the Graph 12 MATCH. ( p:Person)-[r*l]->(m:Permission) 13 WHERE. p. name = 11 Ethan 11 14 RETURN. r, m.name 15 ORDER BY. m; I r Im.name I MATCH WHERE. ( p:Person)-[r*l .. 2]->(m:Permission) p.name = \"Ethan\" RETURN. r, m.name ORDER BY. m; I m.name I [ [:HAS_ROLEJ, [:HAS_PERMISSION] ] I \"storage.list\" I [ [ :HAS_ROLE] , [ :HAS_PERMISSIONJ) I \"storage.create\" I I [ [ :HAS_ROLEJ, [ :HAS_PERMISSIONJ J I \"storage.delete\" I name: Data Engineer HAS_PERMISSION Updating Nodes Adding node properties: Adding node labels: Updating Relationships Adding and updating relationship properties: \"Renaming\" a relationship type: Deleting Relationships and Nodes Drop the relationships connected to nodes labeled Person: Drop nodes labeled Person: Drop all the nodes and relationships in the current database:"
  },
  {
    "index": 10,
    "title": "10. Data Warehouse Modeling",
    "content": "Data Warehouse Modeling. Data Mining slides from text book PPTs by Prof. Jiawei Han & other online sources Decision Makers Wants to Know........ Which are our lowest/highest margin customers ? Who are my customers and what products are they buying? Which customers are most likely to to the competition ? What impact will new products/services have on revenue and margins? What product prom- -otions have the biggest impact on revenue? What is the most effective distribution channel? Balance Scorecards and Dashboards Data Warehouse Modeling : Learning Outcome ➢What is a data warehouse? ➢A multi-dimensional data model ➢Data warehouse architecture ➢From data warehousing to data mining What is a Data Warehouse? Defined in many different ways. A decision support database that is maintained separately from the organization's operational database Support information processing by providing a solid platform of consolidated, historical data for analysis. “A data warehouse is a subject-oriented, integrated, time-variant, and nonvolatile collection of data in support of management's decision-making process.”-W. H. Inmon Data warehousing: The process of constructing and using data warehouses Definition Data Warehouse Subject Oriented Integrated Time Variant Non Volatile For Management Decision Making Data Warehouse : Subject-Oriented Organized around major subjects, such as customer, product, sales Focusing on the modeling and analysis of data for decision makers, not on daily operations or transaction processing Provide a simple and concise view around particular subject issues by excluding data that are not useful in the decision support process Subject Oriented Data Warehouse : Integrated Constructed by integrating multiple, heterogeneous data sources ■relational databases, flat files, on-line transaction records Data cleaning and data integration techniques are applied. ■Ensure consistency in naming conventions, encoding structures, attribute measures, etc. among different data sources ■E.g., Hotel price: currency, tax, breakfast covered, etc. ■When data is moved to the warehouse, it is converted. Data Warehouse : Integrated Data Warehouse : Time Variant ■The time horizon for the data warehouse is significantly longer than that of operational systems ■Operational database: current value data ■Data warehouse data: provide information from a historical perspective (e.g., past 5-10 years) Data Warehouse : Time Variant Data Warehouse : Nonvolatile A physically separate store of data transformed from the operational environment Operational update of data does not occur in the data warehouse environment ■Does not require transaction processing, recovery, and concurrency control mechanisms ■Requires only two operations in data accessing: ■initial loading of data and access of data Data Warehouse : Nonvolatile Data Warehouse vs. Operational DBMS OLTP. ( on-line transaction processing) Major task of traditional relational DBMS Day-to-day operations: purchasing, inventory, banking, manufacturing, payroll, registration, accounting, etc. OLAP (on-line analytical processing) Major task of data warehouse system Data analysis and decision making Distinct features (OLTP vs. OLAP): User and system orientation: customer vs. market Data contents: current, detailed vs. historical, consolidated Database design: ER + application vs. star + subject View: current, local vs. evolutionary, integrated Access patterns: update vs. read-only but complex queries DBMS vs. Data Warehouse Application-Orientation vs. Subject-Orientation Application-Orientation Operational Database Loans Credit Card Trust Savings Subject-Orientation Data Warehouse Customer Vendor Product Activity Why Separate Data Warehouse? High performance for both systems DBMS- tuned for OLTP: access methods, indexing, concurrency control, recovery Warehouse-tuned for OLAP: complex OLAP queries, multidimensional view, consolidation Different functions and different data: missing data: Decision support requires historical data which operational DBs do not typically maintain data consolidation: DS requires consolidation (aggregation, summarization) of data from heterogeneous sources data quality: different sources typically use inconsistent data representations, codes and formats which have to be reconciled Note: There are more and more systems which perform OLAP analysis directly on relational databases ❖OLTP on DBMSs are used to “run” a business ❖OLAP on Data Warehouses help to “optimize” the business Data Warehouse Modeling : An Overview ➢What is a data warehouse? ➢A multi-dimensional data model ➢Data warehouse architecture ➢From data warehousing to data mining From Tables and Spreadsheets to Data Cubes A data warehouse is based on a multidimensional data model which views data in the form of a data cube A data cube, such as sales, allows data to be modeled and viewed in multiple dimensions Dimension tables, such as item (item_name, brand, type), or time(day, week, month, quarter, year) Fact table contains measures (such as dollars_sold) and keys to each of the related dimension tables In data warehousing literature, an n-D base cube is called a base cuboid. The top most 0-D cuboid, which holds the highest-level of summarization, is called the apex cuboid. The lattice of cuboids forms a data cube. Cube: A Lattice of Cuboids time,item time,item,location time, item, location, supplier Conceptual Modeling of Data Warehouses Modeling data warehouses: dimensions & measures ■Star schema: A fact table in the middle connected to a set of dimension tables ■Snowflake schema: A refinement of star schema where some dimensional hierarchy is normalized into a set of smaller dimension tables, forming a shape similar to snowflake ■Fact constellations: Multiple fact tables share dimension tables, viewed as a collection of stars, therefore called galaxy schema or fact constellation Example of Star Schema time_key day day_of_the_week month quarter year time location_key street city state_or_province country location Sales Fact Table time_key item_key branch_key location_key units_sold dollars_sold avg_sales Measures item_key item_name brand type supplier_type item branch_key branch_name branch_type branch Example of Snowflake Schema time_key day day_of_the_week month quarter year time location_key street city_key location Sales Fact Table time_key item_key branch_key location_key units_sold dollars_sold avg_sales Measures item_key item_name brand type supplier_key item branch_key branch_name branch_type branch supplier_key supplier_type supplier city_key city state_or_province country city Example of Fact Constellation time_key day day_of_the_week month quarter year time location_key street city province_or_state country location Sales Fact Table time_key item_key branch_key location_key units_sold dollars_sold avg_sales Measures item_key item_name brand type supplier_type item branch_key branch_name branch_type branch Shipping Fact Table time_key item_key shipper_key from_location to_location dollars_cost units_shipped shipper_key shipper_name location_key shipper A Concept Hierarchy: Dimension (location) all Europe North_America Mexico Canada Spain Germany Vancouver M. Wind L. Chan all region office country Toronto Frankfurt city Multidimensional Data ■Sales volume as a function of product, month, and region Product Region Month Dimensions: Product, Location, Time Hierarchical summarization paths Industry Region Year Category Country Quarter Product City Month Week Office Day Multidimensional Data with Concept Hierarchies A Sample Data Cube Total annual sales Date Product Country sum sum VCR 1Qtr 2Qtr 3Qtr 4Qtr Canada Mexico sum A Sample Data Cube A Sample Data Cube Browsing a Data Cube Visualization OLAP capabilities Interactive manipulation Tracking Changes in Data warehouse Dimensions :Slowly Changing Dimensions • Type 1: Overwrite the value • Type 2: Add a Dimension row • Type 3: Add a Dimension column Typical OLAP Operations Roll up (drill-up): summarize data ■by climbing up hierarchy or by dimension reduction Drill down (roll down): reverse of roll-up ■from higher level summary to lower level summary or detailed data, or introducing new dimensions ■Slice and dice: project and select Pivot (rotate): ■reorient the cube, visualization, 3D to series of 2D planes Typical OLAP Operations A Star-Net Query Model Shipping Method AIR-EXPRESS TRUCK ORDER. Customer Orders CONTRACTS. Customer Product PRODUCT GROUP PRODUCT LINE PRODUCT ITEM SALES PERSON DISTRICT DIVISION. Organization Promotion CITY COUNTRY REGION. Location DAILY QTRLY ANNUALY. Time Each circle is called a footprint Data Warehouse Modeling : An Overview ➢What is a data warehouse? ➢A multi-dimensional data model ➢Data warehouse architecture ➢From data warehousing to data mining Design of Data Warehouse: A Business Analysis Framework Four views regarding the design of a data warehouse ■Top-down view ■allows selection of the relevant information necessary for the data warehouse ■Data source view ■exposes the information being captured, stored, and managed by operational systems ■Data warehouse view ■consists of fact tables and dimension tables ■Business query view ■sees the perspectives of data in the warehouse from the view of end-user Data Warehouse Design Process Top-down, bottom-up approaches or a combination of both Top-down: Starts with overall design and planning (mature) Bottom-up: Starts with experiments and prototypes (rapid) From software engineering point of view Waterfall: structured and systematic analysis at each step before proceeding to the next Spiral: rapid generation of increasingly functional systems, short turn around time, quick turn around Typical data warehouse design process Choose a business process to model, e.g., orders, invoices, etc. Choose the grain (atomic level of data) of the business process Choose the dimensions that will apply to each fact table record Choose the measure that will populate each fact table record Data Warehouse: A Multi-Tiered Architecture Data Warehouse Extract Transform Load Refresh OLAP Engine Analysis Query Reports Data mining Monitor Integrato Metadata Data Sources Front-End Serve Data Marts Operational DBs Other sources Data OLAP Server Three Data Warehouse Models Enterprise warehouse ■collects all of the information about subjects spanning the entire organization Data Mart ■a subset of corporate-wide data that is of value to a specific groups of users. Its scope is confined to specific, selected groups, such as marketing data mart ■Independent vs. dependent (directly from warehouse) data mart Virtual warehouse ■A set of views over operational databases ■Only some of the possible summary views may be materialized Data Lake Vs Data Warehouse Data Warehouse Back-End Tools and Utilities Data extraction ■get data from multiple, heterogeneous, and external sources Data cleaning ■detect errors in the data and rectify them when possible Data transformation ■convert data from legacy or host format to warehouse format Load ■sort, summarize, consolidate, compute views, check integrity, and build indicies and partitions Refresh ■propagate the updates from the data sources to the warehouse Metadata Repository Meta data is the data defining warehouse objects. It stores: Description of the structure of the data warehouse schema, view, dimensions, hierarchies, derived data defn, data mart locations and contents Operational meta-data data lineage (history of migrated data and transformation path), currency of data (active, archived, or purged), monitoring information (warehouse usage statistics, error reports, audit trails) The algorithms used for summarization The mapping from operational environment to the data warehouse Data related to system performance warehouse schema, view and derived data definitions Business data business terms and definitions, ownership of data, charging policies Data Warehouse Development: A Recommended Approach Define a high-level corporate data model Data Mart Data Mart Distributed Data Marts Multi-Tier Data Warehouse Enterprise Data Warehouse Model refinement Model refinement Data Mart Centric Data Marts Data Sources Data Warehouse Problems with Data Mart Centric Solution If you end up creating multiple warehouses, integrating them is a problem True Warehouse Data Marts Data Sources Data Warehouse OLAP Server Architectures Relational OLAP (ROLAP). Use relational or extended-relational DBMS to store and manage warehouse data and OLAP middle ware Include optimization of DBMS backend, implementation of aggregation navigation logic, and additional tools and services Greater scalability Multidimensional OLAP (MOLAP). Sparse array-based multidimensional storage engine Fast indexing to pre-computed summarized data Hybrid OLAP (HOLAP). ( e.g., Microsoft SQLServer) Flexibility, e.g., low level: relational, high-level: array Specialized SQL servers (e.g., Redbricks) Specialized support for SQL queries over star/snowflake schemas Data Warehouse Modeling : An Overview ➢What is a data warehouse? ➢A multi-dimensional data model ➢Data warehouse architecture ➢From data warehousing to data mining Data Warehouse Usage Three kinds of data warehouse applications Information processing ■supports querying, basic statistical analysis, and reporting using crosstabs, tables, charts and graphs Analytical processing ■multidimensional analysis of data warehouse data ■supports basic OLAP operations, slice-dice, drilling, pivoting Data mining ■knowledge discovery from hidden patterns ■supports associations, constructing analytical models, performing classification and prediction, and presenting the mining results using visualization tools Dashboards from Data warehousing From On-Line Analytical Processing (OLAP) to On Line Analytical Mining (OLAM) Why online analytical mining? ■High quality of data in data warehouses ■DW contains integrated, consistent, cleaned data ■Available information processing structure surrounding data warehouses ■ODBC, OLEDB,. Web accessing, service facilities, reporting and OLAP tools ■OLAP-based exploratory data analysis ■Mining with drilling, dicing, pivoting, etc. ■On-line selection of data mining functions ■Integration and swapping of multiple mining functions, algorithms, and tasks An OLAM System Architecture Data Warehouse Meta Data MDDB OLAM. Engine OLAP Engine User GUI API. Data Cube API Database API Data cleaning Data integration Layer3 OLAP/OLAM. Layer2 MDDB Layer1 Data Repository Layer4 User Interface Filtering&Integration Filtering Databases Mining query Mining result Summary: Data Warehouse Modeling Why data warehousing? A multi-dimensional model of a data warehouse Star schema, snowflake schema, fact constellations A data cube consists of dimensions & measures OLAP operations: drilling, rolling, slicing, dicing and pivoting Data warehouse architecture OLAP servers: ROLAP, MOLAP, HOLAP. From OLAP to OLAM (on-line analytical mining)"
  },
  {
    "index": 11,
    "title": "11. Data Cube Technology",
    "content": "Slides from text book PPTs by Prof. Jiawei Han & other online sources Data Mining Data Cube Technology. Chapter S: Data Cube Technolog- data cell □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods time □ Processing Advanced Queries with Data Cube Technology □ Multid\"mensional Data Analysis in Cube Space □ Summary time,item time,item,location Data Cube: A Lattice of Cuboids all 0-D(apex) cuboid supplier 1-D cuboids ocation,supplier 2-D cuboids 3-D cuboids · em,location,supplier 4-D(base) cuboid time, item, location, supplierc time time,item,I Data Cube: A Lattice of Cuboids all ,supp tion supplier O-D(apex) cuboid D Base vs. aggregate cells Ancestor vs. descendant cells 1-D cuboids □ Parent vs. child cells location,supplier time, item, location, supplier 2-D cuboids 3-D cuboids 4-D(base) cuboid □ {*, milk, *, *) □ {*, milk, Urbana, *) □ {*, milk, Chicago, *) □ {9/15, milk, Urbana, *) □ {9/15, milk, Urbana, Dairy_land) □ Fu II cube vs. iceberg cu be compute cube sales iceberg as select month, city, customer group, count(*) from sale,slnfo cube by month, city, customer group iceberg having count(*)>= min support □ Compute only the cells whose measure satisfies the iceberg ,con1dition □ Only a small portion of cells may be \"above the water\" in a sparse cube □ Ex.: Show only those cells whose count is no less than 100 Whó Iceberg Cube? □ Advantages of computing iceberg cubes □ No need to save nor show those cells whose value is below the threshold (iceberg condition) □ Efficient methods may even avoid computing the un-needed, intermediate cells □ Avoid explosive growth □ Example: A cube with 100 dimensions □ Suppose it contains only 2 base cells: {(a1, a2, a3, •••• , a100), (a1, a2, b3, ••• , b100)} □ How many aggregate cells if \"having count >= 1\"? □ Answer: 2101 - □ (Why?!) □ What about the iceberg cells, (i,e., with condition: \"having count>= 2\")? □ Answer: 4 (Why?!) s Iceberg Cube Good Enough? Closed Cube & Cube Shell □ Let cube P have only 2 base cells: {(a1, a2, a3 ••• , a100):10, (a1, a2, b3, ••• , b100):10} How many cells will the iceberg cube contain if \"having count(*)E 10\"? □ Answer: 2101 - 4 (still too bigl) □ Cose cube: A cell c is closed if there exists no cell d, such that dis a descendant of c, and d has the same measure value as c Ex. P has only 3 closed cells: ------, : 20 A closed cube is a cube consisting of only closed cells □ Cube She Cube She I: The cuboids involving a small# of dimensions, e.g., 3 Idea: Only compute cube shells, other dimension combinations can be computed on the fly -----v Q: For (A11 A2, ••• A10), how many comb1nat1ons to compute? □ Da a Cube Computa ion: Basic Co I cepts □ Da a Cube Computa, ion Methods time □ Processing Adva · ced Que ies with Data Cu I e Technology □ Mult.dimensional Data Analysis in Cube Space □ Summary Roadm for Efficient Computation □ General computation heuristics {Agarwal et al.'96) □ Computing full/iceberg cubes: 3 methodologies Bottom-Up: Multi-Way array aggregation {Zhao, Deshpande & Naughton, SIGMOD'. 97) □ Top-down: D BUC {Beyer & Ramarkrishnan, SIGMOD'. 99) Integrating Top-Down and Bottom-Up: □ Star-cubing algorithm {Xin, Han, Li & Wah: VLDB'03) □ High-dimensional OLAP: A Shell-Fragment Approach {Li, et al. VLDB'04) D Computing alternative kinds of cubes: Partial cube, closed cube, approximate cube, ..... □ Sorting, hashing, and grouping operations are applied to the dimension attributes in order to reorder and cluster related tuples all □ Aggregates may be computed from previously computed aggregates, rather than from the base fact table □ Sma lest-child: computing a cuboid from the smallest, prod,da ·. 1 try previously computed cuboid country . ate, count\u0019 Cache resu ts: caching results of a cuboid from which other cuboids are computed to reduce disk I/Os prod, date, country □ Amortize-sea s: computing as many as possible cuboids at the same time to amortize disk reads Share-sorts: sharing sorting costs cross multiple cuboids when sort-based meth1od is used □ Share-parff ons: sharing the partitioning cost across multiple cuboids when hash-based algorithms are used S. Agarwal, R. Agrawal, P. M. Deshpande, A. Gupta, J. F. Naughton, R. Ramakrishnan, S. Sarawagi. On the computation of multidimensional aggregates. VLDB'96 □ Array based \"bottom up\" algorithm (from ABC to AB, ... ) □ Using multi-dimensional chunks □ Simultaneous aggregation on multiple dimensions □ Intermediate aggregate values are re-used for computing ancestor cuboids □ Cannot do Apriori pruning: No iceberg optimization □ Comments on the method All □ Efficient for computing the full cube for a small number of dimensions □ If there are a large number of dimensions, \"top-down\" computation and iceberg cube computation methods (e.g., BUC) should be used Cube Computation: Multi-Wa& Ana& Aggregation (MQLAP). □ Partition arrays into chunks {a small subcube which fits in memory). □ Compressed sparse array addressing: {chunk_id, offset) □ Compute aggregates in \"multiway\" by visiting cube cells in the order which minimizes the# of times to visit each cell, and reduces memory access and storage cost What is the best traversing order to do multi-way aggregation? ti-way Ar ay Aggregatican □ How to minimizes the memory requirement and reduc_ I/Os? One chunk of BC plane Entire AB plane □ Keep the smallest plane in main One column of AC plane memory, fetch and compute only one chunk at a time for the largest plane □ The planes should be sorted and computed according to their size in ascending order □ Same methodology for computing 2-D and 1-D planes All Cube Computatio : Computing in Reverse Orde, □ BUC {Beyer & Ramakrishnan, SIGMOD'. 99) BUC: acronym of Bottom-Up {cube) Computation {Note: It is \"top-down\" in our view since we put Apex cuboid on the top!) □ Divides dimensions into partitions and facilitates iceberg pruning □ If a partition does not satisfy min_sup, its descendants can be pruned □ If minsup = 1 D compute full CUBE! □ No simultaneous aggregation 4ABC all ABD ABCD. 1 all 6ABD 8ACD 12BCD 5ABCD □ Usually, entire data set cannot fit in main memory □ Sort distinct values □ partition into blocks that fit □ Continue processing □ Optimizations Partitioning □ External Sorting, Hashing, Counting Sort Ordering dimensions to encourage pruning □ Cardinality, Skew, Correlation Collapsing duplicates □ Cannot do holistic aggregates anymore! Curse f Dimensionali y □ High-D OLAP:. Needed in many applications □ Science and engineering analysis Bio-data analysis: thousands of genes □ Statistical surveys: hundreds of variables □ None of the previous cubing method can handle high dimensionalityl ceberg cube and compressed cubes: only delay the inevitable explosion Full materialization: still significant overhead in accessing results on disk □ A shell-fragment approach: X. Li, J. Han, and H. Gonzalez, High-Dimensional OLAP: A Minimal Cubing Approach, VLDB104 A curse of dimensionality: A database of 600k t1uples. Each dimension has cardinality of 100 and zip/ of 2. Fast □ Observation: OLAP occurs only on a small subset of dimensions at a time □ Semi-On ine Computational odel Partition the set of dimensions into s ell fragments Compute data cubes for each shell fragment while retaining inverted ind·ces or value-list i d·ces □ Given the pre-computed f agment cubes, dynamically compute cube cells of the high-dimensional data cube online Major idea: Tradeoff between the amount of pre-computation and the speed of online computation Reducing computing high-dimensional cube into precomputing a set of lower dimensional cub,es Online re-construction of original high-dimensional space □ Lossless reduction □ Example: Let the cube aggregation function b·e counit tid □ Divide the 5-D table into 2 shell fragments: {A, B, C) and {D, E) □ Build traditional invert index or R D list Attribute TID llist L'ist Value □ Generalize the :1 D inverted indice.s to mu dimensional ones in1 th1e data cube sen.se lti □ Compute all cuboids for data cubes ABC a n1d ID1E wh1ile retaining the inverted indices □ Ex. shell fragment cube ABC contains 7 □ A, B, C; AB, AC, BC; ABC. cub □ This completes the offline computation Shelll-fragment AB Mlea.sure I ab □ l·f measures other than1 ti·d count sum count a re present, store in 2 ID_measure table separate 3 from the she I I fragments Cell Attfbute· TIID List List Value Size Intersection TID List List Size □ Given a database of T tuples, D dimensions, and F shell fragment size, the fragment cubes' space requirement is: For F < 5, the growth is sub-linear □ Shell fragments do not have to be disjoint □ Fragment groupings can be arbitrary to allow for maximum online performance Known common combinations (e.g.,<city, state>) should be grouped together □ Shell fragment sizes can be adjusted for optimal balance between offline and online computation Cell Attribute TIO List List Vallue Size Intersection TID List List Size D'imensi'ons ABC Cube EF Cuboid DE Cuboid Cell Tuple-ID List DEF Cube Instantiated Base Table Online Cube On ine Oue,ô Computation with S e •Fragments A query has the general form: <a1, a2, ••• , an: M> -ach ai has 3 possible values (e.g., <3, ?, ?, *, 1: count> returns a 2-D data cube) ln1stan1tiated value Aggregate* function □ Inquire ? Function Method: Given the fragment cubes, process a query as follows Divide the query into fragment, same as the shell-fragment Fetch the corresponding T D list for each fragment from the fragment cube □ Intersect the TID lists from each fragment to construct· sta tiated ase table Compute the data cube using the base table with any cubing algorithm bperimenl: Size s. Dimensicanalitv (SO an 100 carclinalil.) Ex er·ments on real-world data □ UCI Forest CoverType data set □ 54 dimensions, 581K tuples Shell fragments of size 2 took 33 seconds and 325MB to compute 3-D subquery with 1 instantiate D: 85ms\"'l.4 sec. □ Longitudinal Study of Vocational Rehab. Data: 24 dimensions, 8818 tuples Shell fragments of size 3 took 0.9 seconds and 60MB to compute {50-C): 106 tuples, 0 skew, 50 cardinality, fragment size 3 {100-C): 106 tuples, 2 skew, 100 cardinality, fragment size 2 □ 5-D query with O instantiated D: 227ms\"'2.6 sec. Chapter : Data Cube 1echna oa, □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods □ Processing Advanced Q eries with Data Cube Tech no ogy □ M ltidimensional Data Ana ysis in Cube Space □ Summary time ta Cube □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods echncalcag. time □ Processing Advanced Q. eries with Data Cube Techno ogy □ M tidimensiona Data Analysis ·n Cube Space □ Summary Data Mining in Cube S ace □ Data cube greatly increases the analysis bandwidth □ Four ways to interact OLAP-styled analysis and data mining □ Using cube space to define data space for mining □ Using OLAP queries to generate features and targets for mining, e.g., multi-feature cube □ Using data-mining models as building blocks in a multi-step mining process, e.g., prediction cube □ Using data-cube computation techniques to speed up repeated model construction □ Cube-space data mining may require building a model for each candidate data space □ Sharing computation across model-construction for different candidates may lead to efficient mining lex Aggregation at Multiple Granularities: Mu ti-Feature Cubes □ Multi-feature cubes (Ross, et al. 1998): Compute complex queries involving multiple dependent aggregates at multiple granularities □ Ex. Grouping by all subsets of {item, region, month}, find the maximum price in 2010 for each group, and the total sales among all maximum price tuples select item, region, month, max(price), sum(R.sales) from purchases where year = 2010 cube by item, region, month: R such that R.price = max(price) □ Continuing the last example, among the max price tuples, find the min and max shelf live, and find the fraction of the total sales due to tuple that have min shelf life within the set of all max price tuples õ iscoue,ö•Driuen Exp o,ation of ·÷ ata Cubes □ Discovery-driven exploration of huge cube space {Sarawagi, et al.'98) Effective navigation of large OLA data cubes □ pre-compute measures indicating exceptions, guide user in the data analysis, at all levels of aggregation Exception: significantly different from the value anticipated, based on a statistical model □ Visual cues such as background color are used to reflect the degree of exception of each cell □ Kinds of exceptions □ SelfExp: surprise of cell relative to other cells at same level of aggregation □ lnExp: surprise beneath the cell □ Path Exp: surprise beneath cell for each drill-down path □ Computation of exception indicator can be overlapped with cube construction Exceptions can be stored, indexed and retrieved like precomputed aggregates Exa les: Disca i cem Jan 1Feb MfaC' ApC' Tota] mie n 1'1 Feb \"Mac- ApL p r-1 ntec:- b o rne c o1mp1uter- 1l ap1to1p. ic o mputec mou:s,e rnp 15 Ir lvf .a-- Dec □ Data Cube Computation: Basic Concepts □ Data Cube Computation Methods time □ Processing Advanced Queries with Data Cube Technology □ Mu tidimensional Data Analysis in Cube Space □ Summary -/ ata Cube Techno agy: Summary □ Data Cube Computation: Prel im ·nary Concepts Data Cube Computation Methods M ultiWay Array Aggregation BUC High-Dimensional OLAP with Shel l- ragments Multidimensional Data Analysis in Cube Space M ulti-feature Cubes Discovery-Driven Exploration of Data Cubes"
  },
  {
    "index": 12,
    "title": "12. Intro to Information Retrieval",
    "content": "Introduction to Information Retrieval Slides adapted from Hongning Wang, Department of Computer Science, University of Virginia What is information retrieval? I> bing wl Go gle what is information retrieval Web Videos Images News Shopping More• Search tools About 14,300,000 results (0.43 seconds) in·for·ma·tion re·triev·al NI n orma 10n re neva 1s e ac 1v1 y of obtaining information resources relevant to an information nee rom a collection of information resources. Pl Searches canbe oasecr on iiiel data or on full-text indexing. Automated lD :QflD o.c te eH sI rns are used to reduce what has been-called - 11 \"information overload\". Many universities and public libraries use IR syst Int based on metadata or on full-text (or other content-based) Indexing Category:lnformation retrieval - Relevance - Human-computer inforrnation __ [PDF) nlp.stanford.edu/lR-book/pdf/01bool.pdf • Information retrieval (IR) is finding material (usually documents) Jf an u structure nature (usually text) that satisfies an information need from within large (usually stored on computers). www.iva.dk/ ./inf .. • The Royal School of Library and Information Science • Oct 15, 2006 - Information retrieval (IR). The tem1 IR may be considered a research field but it may also be considered a research tradition (or rather a set of .. Information Retrieval - Merriam-Webster www.merriam-webster.com/ ../infonnation%20retrieva . • Merriam-Webster • the techniques of storing and recovering and often disseminating ,Øcorded data especially through the use of a computerized system. ADVERTISEMENT. ... What is information retrieval? • Apple's vision 3+ decades ago Knowledge Navigator Why information retrieval • Information overload - \"It refers to the difficulty a person can have understanding an issue and making decisions that can be caused by the presence of too much information.\" - wiki Why information retrieval • Information overload 1 uu,ouu, uuu Hobbes' Internet Timeline Copyright ©2014 Robert H Zakon Hobbes' Internet Timeline Copvright ©2014 Robert H Zakon http://www.zakon.org/robert/internet/timeline/ DATE SITES DATE SITES. Figure 1: Growth of Internet Why information retrieval • Handling unstructured data - Structured data: database system is a good choice - Unstructured data is more dominant • Text in Web documents or emails, image, audio, video ... • 1185 percent of all business information exists as Jack ssor unstructurecl data\" - Mernll Lynch • Unknown semantic meaning Why information retrieval • An essential tool to deal with information overload You are here! History of information retrieval • Idea popularized in the pioneer article \"As We May Think'1 by Vannevar Bush, 1945 - \"Wholly new forms of encyclopedias will appear, ready-made with a mesh of associative trails running through them, ready to be dropped into the memex and there amplifi-9cMJWW - \"A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility.\" -> Search engine Major research milestones • Early days (late 1950s to 1960s): foundation of the field Luhn's work on automatic indexing Cleverdon's Cranfield evaluation methodology and index experiments - Salton's early work on SMART. system and experiments • 1970s-1980s: a large number of retrieval models - Vector space model Probabilistic models • 1990s: further development of retrieval models and new tasks - Language models - TREC evaluation - Web search • 2000s-present: more applications, especially Web search and interactions with other fields Learning to rank - Scalability (e.g., MapReduce) - Real-time search History of information retrieval • Catalyst - Academia: Text Retrieval Conference (TREC) in 1992 • \"Its purpose was to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies. • \" . . about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines.\" • Till today, it is still a major test-bed for academic research in History of information retrieval • Catalyst - Industry: web search engines • WWW unleashed explosion of published information and drove the innovation of IR techniques • First web search engine: /(Oscar Nierstrasz at the University of Geneva wrote a series of Perl scripts that periodically mirrored these pages and rewrote them into a standard format.\" Sept 2, 1993 • Lycos (started at CMU) was launched and became a major commercial endeavor in 1994 • Booming of search engine industry: Magellan, Excite, Jnfoseek, lnktomi, Northern Light, Altavista, Yahoo!, Google, and Bing Major players in this game • Global search engine market - desktop http://ma rketsha re.hits I in k.com/search-engine-m ot-ch:lro :lcnv Other: 1.13 % 1 Excite - Global: 0.01 % AOL - Global: 0.05 % Ask - Global: 0.17 % Yahoo - Global: 4.74 % Baidu: 5.82 % Bing: 6.97 % Fall 2017 \\_ Google - Global: 81.12 % Major players in this game • Global search engine market - mobile http ://marketshare. hits I ink.com/search-engi ne-m Other: 1.27 % \\ AOL - Global: 0 % Ask - Global: 0.03 % Baidu: 0.43 % Bing: 0.67 °/o Yahoo - Global: 1.21 % Fall 2017 ( Google - Global: 96.38 % How to perform information retrieval • Information retrieval when we did not have a computer Recap: why information retrieval • Handling unstructured data - Structured data: database system is a good choice - Unstructured data is more dominant • Text in Web documents or emails, image, audio, video ... • 1185 percent of all business information exists as unstructured data\" - Merrill Lynch • Unknown semantic meaning How to perform information retrieval Crawler and indexer Document analyzer Repository How to perform information retrieval PARSING & INDEXING. query Repository User Ranking LEARNING. Evaluation results --- judgments FEEDBACK. Core concepts in IR • Query representation - Lexical gap: say v.s. said - Semantic gap: ranking model v.s. retrieval method • Document representation - Special data structure for efficient access - Lexical gap and semantic gap • Retrieval model - Algorithms that find the most relevant documents for the given information need A glance of modern search engine • In old times Yet Another Hierarchical Officious/Obstreperous/ Odiferous/Organized Oracle -Qr\u0007 b.SoOt Q \b lD1CIJJII GbMt&O>JI t.alillt -w<IIC• 1 Arts & Humanüles News & Media , LM:rasw:t fltnfestt,..iX-• Euaecrausý litJttAllltU n.. I Business & Economy Recreation & Sports 1CO&PNPC:£F\u0012\u0013. \u0014 Imd,,\u0015Qn1t1ern,. I Computers & Internet Reference 1 llllsaal. WWW. Softot-0.U. · Lllom .. PIEPllltGI<, QuotQPN I I Education Regional I ContutodlfQlYtfn'ty Kill Counlrio Ruw!£.USS1♦1u 1 Entertainment Science I Cogllanl<! l,mw. lh!!w, l!lYfil þ ,,..,.,..,,,,. MCIO!!OOI. Matkttplact ÿXIAWMOOt·Ā Po\\:mon, rnmeulut • Fret ¥K h31PPr\\ AS1itU Xllwo' 9Jll Pu. &eel­ lus:ldt Yahoo! • Ytb<tol 0.eQilltA • b\\lild your 6-H hotr1• P•8• • Plly fru FdttJSoi:rtt Ytbool Qo.be. auu YOOf A glance of modern search engine Go gle I uva Demand of understanding Web Maps Images News Shopping More• Search tools • About 103,000,000 results (0.65 seconds) I ----------- Demand of efficiency_ Demand of convenience lr\u000fw.vwi www. virginia.edL The University of' Jefferson. The corn University of en.wikipedia.org/v The University of\\ research university Google University of colleges.usnews.r Is University of Vii University of Virgi Google Search VIRGINIASPO. ... _____ ., ... _ ----·-·-··, -· ... \" .... _ -.. ·-·---·· www.virginiasports.com/ • Virginia Cavaliers • The University of Virginia Official Athletic Site, partner of CBSSports.com College Network. The most comprehensive coverage of UVa Cavaliers Athletics on the ... Images for university of virginia Report im2ges More images for university of virginia ,nParkAve C¯t!Googl9 irections I'm Feeling Lucky ottesville, President Acceptance rate: 28.3% (2013) Enrollment: 21,095 (2012) Mascot: University of Virginia Cavalier Founder: Thomas Jefferson Founded: 1819, Charlottesville, VA Colors: Blue. Orange Recent posts #UVA's Center for Politics and Politico have te2med up to offer interactive election ratings. #politics #elections #voting 1 hour ago IR is not just about web search • Web search is just one important area of information retrieval, but not all • Information retrieval also includes - Recommendation Recommended Based on Your Browsing History Linear Algebra and Its Applications ... > David C . Lay Hardcover Linear Algebra: A Modern Introduction > David Poole Hardcover Linear Algebra > G . E . Shi lov Paperback Introduction to Linear Algebra ... > Gi lbert Strang Hardcover Linear Algebra For Dummies > Mary Jane Sterling Paperback IR is not just about web search _ SECTIONS. / t HOME Q, SEARCH TECHNOLOGY. Google and Walmart Partner With Eye on Amazon By DAISUKE WAKABAYASHI. and MICHAEL CORKERY AUG. 23. 2017 Wal mart has also been trying to integrate its digital business with its vast network of more: than 4,690 stores. It partnered with Google to take on Amazon, the heavyweight of on line shopping. Roger Kisby tor The New 'rork Times ooeo [] RELATED. COVl'R5GE At Walmart Academy, Training Better Managers. But With a B etter Future? ALG. 8. 2017 Google, Lagging Amazon, Race.c Acro.c.c the Threshold Into the Home ocr 2. 2016 ECONOMIC. iRENOS The Amazon-Walmart Showdown That Explains the Modern Economy JUNE 10. 2011 Walmart Rewrites Its E-Commerce Strategy With $3.3 Billion Deal for Jet.com AlG. S, 2016 IR is not just about web search • Web search is just one important area of information retrieval, but not all • Information retrieval also includes - Question answering WolframAlpha \u0002\u0003\u0004\u0005t\u00060i;\u0005;;\u0006 how to calculate derivative of gamma function Derivative: = Examples :✓.: Random B' Step-by-step solution [(x) 1s the ga,nma function l/,11111(x) 1s the nth derivative of the digamma function I R is not just about web search • Web search is just one important area of information retrieval, but not all • Information retrieval also includes - Text mining customer t'.E,1t positiveo.3ining the ogsanalY,s1 lyti socialnegati45a daMntimenttni6e78 vo1cemecha aata::.base Information f .,,.\u0016 • ..r-niod\u0017Jnetwork e iev,\"{l(Jl>lr mt1t_1°iJ< ? \"\"' roco 2fn1faton f.actortzAtl:'tV m Llrl!l'dh;tion p, datasel topic - Wq-ue\\.'t:X1context . l ,.U,flt topic _ ,r-.c11Je1 VMm TOplcs ooeumonts Topic proporttoo5 and asilgnmonts (D.M Blei, Probabilistic Topic Models. Communications of the ACM, 2012) I R is not just about web search • Web search is just one important area of information retrieval, but not all • Information retrieval also includes Online advertising 1YAHOO! 1'----- ► View Movies ii Be.1uty • Shopping (I Tech m More on Y.:ihoo Tr:plllauouralion Tuas eleclOf:Trump'OOI WhyCarty Slmon rejecled Stars stiowofftlaws ln packages priced upto S1M bibliu1ty qua lilied' Mick Jagger stunnin,g newphOtos Poli1ics Michigan Certifies Trump as Winner of State's Presidential Race Trending Now 1 Amber Rose 6 Senior lndependen I 2- John Cena 7. Moriah Carey 3. Denver Broncos 8 Buy Auto Tires I I Microsoft fru 1hlpp1n9 \"\u0014e•yd•y I Surface Book M J(rosoft I Perf:!nce... Surface Dial I R is not just about web search • Web search is just one importa nt area of information retrieval, but not all • I nformation retrieva l also incl udes Enterprise search : web search + desktop sea rch 6Js1s QI.IVACOUAI!. QwAHOUE Discover lNa Sdloots Alhleæcs Arts Madleal Canter Researth Ubrttrlas Public Service New Madia Seard, U.Va. Web Search (Google) Soaa;h tho pooplo diroctorv I fmd poopl9 namN \"nongr11ng%2Qw?ng· • People UVa.W\u0011 Library 1 All results Current News /2006-oresent\\ News Archive /1998-2006) Athleti csllntramuials Human Resources Admission/Financial Aid About 9 results (0 59 seconds) liil1 urnversitv ofVirainia . Xi Wang -Thursday, May 1 2014 Synthesizing REST Web Services to Advance a Useful Science of Non-Functional System Properties and Tradeoffs, Chi wwwcs virginia edu/ Wang Honaning Assistant Professor Name, Hongning Wang. Computing ID, hwSx. Office Phone, (434) 982-2225. Email [edit]. (registered), [add alias] Title • Classification, Assistant Professor · rts virgmia edu/seardllpeople/?u=éx&n=Hongning++W11ng Jun 2, 2014 . . Hongning Wang, Yue L u and Chengxiang Zhai Latent Aspect Rating Analysis on Review Text Data A Rating Regression Approach. The www cs virginia edu/people/faculty/newtmes html Comouter Science co110011ia Hongning Wang, University of Illinois. Champaign-Urbana Tuesday, March 18. 2014. 3.30 PM. Rice Hall, Rm 130 {light refreshments after the seminar Rice . www cs virginia edu/events/colloquia/hw11ng html • Information retrieval - Computational approaches - Statistical (shallow) understanding of language - Handle large scale problems • Natural language processing - Cognitive, symbolic and computational approaches - Semantic (deep) understanding of language - (often times) small scale problems IR and NLP are getting closer • IR => NLP. - Larger data collections - Scalable/robust NLP techniques, e.g., translation models - Deep analysis of text documents and queries - Information extraction for structured IR tasks - Natural language based QA systems Text books • Introduction to Information Retrieval. Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schuetze, Cambridge University Press, 2007. • Search Engines: Information Retrieval in Practice. Bruce Croft, Donald Metzler, and Trevor Strohman, Pearson Education, 2009. What to read? Mathematics Machine Learnin I Web Applications, 1 Bioinformatics . . Pattern Recogni - ICML, NIPS,. Data Minin KDD, ICDM,. S ation Retrie WWW, WSDM,. are engineerin uter systems Algorithms Applications Library & Info atabase D, VLD Systems IR in future • Mobile search - Desktop search + location? Not exactly! ! • Interactive retrieval - Machine collaborates with human for information access • Personal assistant - Proactive information retrieval - Knowledge navigator • And many more - You name it! What you should know • IR originates from library science for handling unstructured data • IR has many important application areas, e.g., web search, recommendation, and question answering • IR is a highly interdisciplinary area with DBs, NLP, ML, HCI. "
  },
  {
    "index": 13,
    "title": "13. Data Mining and Society",
    "content": "Data Mining and Society. Based on Ch13 Slides from Jiawei Han, Micheline Kamber, and Jian Pei University of Illinois at Urbana-Champaign & Simon Fraser University ©2011 Han, Kamber & Pei. All rights reserved. Data Mining Trends and Research Frontiers ◼Data Mining Applications ◼Data Mining and Society ◼Data Mining Trends ◼Summary Data Mining Applications ◼Data mining: A young discipline with broad and diverse applications ◼There still exists a nontrivial gap between generic data mining methods and effective and scalable data mining tools for domain-specific applications ◼Some application domains (briefly discussed here) ◼Data Mining for Financial data analysis ◼Data Mining for Retail and Telecommunication Industries ◼Data Mining in Science and Engineering ◼Data Mining for Intrusion Detection and Prevention ◼Data Mining and Recommender Systems Data Mining for Financial Data Analysis (I) ◼Financial data collected in banks and financial institutions are often relatively complete, reliable, and of high quality ◼Design and construction of data warehouses for multidimensional data analysis and data mining ◼View the debt and revenue changes by month, by region, by sector, and by other factors ◼Access statistical information such as max, min, total, average, trend, etc. ◼Loan payment prediction/consumer credit policy analysis ◼feature selection and attribute relevance ranking ◼Loan payment performance ◼Consumer credit rating ◼Classification and clustering of customers for targeted marketing ◼multidimensional segmentation by nearest-neighbor, classification, decision trees, etc. to identify customer groups or associate a new customer to an appropriate customer group ◼Detection of money laundering and other financial crimes ◼integration of from multiple DBs (e.g., bank transactions, federal/state crime history DBs) ◼Tools: data visualization, linkage analysis, classification, clustering tools, outlier analysis, and sequential pattern analysis tools (find unusual access sequences) Data Mining for Financial Data Analysis (II) Data Mining for Retail & Telcomm. Industries (I) ◼Retail industry: huge amounts of data on sales, customer shopping history, e-commerce, etc. ◼Applications of retail data mining ◼Identify customer buying behaviors ◼Discover customer shopping patterns and trends ◼Improve the quality of customer service ◼Achieve better customer retention and satisfaction ◼Enhance goods consumption ratios ◼Design more effective goods transportation and distribution policies ◼Telcomm. and many other industries: Share many similar goals and expectations of retail data mining Data Mining Practice for Retail Industry Design and construction of data warehouses Multidimensional analysis of sales, customers, products, time, and region Analysis of the effectiveness of sales campaigns Customer retention: Analysis of customer loyalty ◼Use customer loyalty card information to register sequences of purchases of particular customers ◼Use sequential pattern mining to investigate changes in customer consumption or loyalty ◼Suggest adjustments on the pricing and variety of goods Product recommendation and cross-reference of items Fraudulent analysis and the identification of usual patterns Use of visualization tools in data analysis Data Mining in Science and Engineering Data warehouses and data preprocessing ◼Resolving inconsistencies or incompatible data collected in diverse environments and different periods (e.g. eco-system studies) Mining complex data types ◼Spatiotemporal, biological, diverse semantics and relationships Graph-based and network-based mining ◼Links, relationships, data flow, etc. Visualization tools and domain-specific knowledge Other issues ◼Data mining in social sciences and social studies: text and social media ◼Data mining in computer science: monitoring systems, software bugs, network intrusion Data Mining for Intrusion Detection and Prevention Majority of intrusion detection and prevention systems use ◼Signature-based detection: use signatures, attack patterns that are preconfigured and predetermined by domain experts ◼Anomaly-based detection: build profiles (models of normal behavior) and detect those that are substantially deviate from the profiles What data mining can help ◼New data mining algorithms for intrusion detection ◼Association, correlation, and discriminative pattern analysis help select and build discriminative classifiers ◼Analysis of stream data: outlier detection, clustering, model shifting ◼Distributed data mining ◼Visualization and querying tools Data Mining and Recommender Systems Recommender systems: Personalization, making product recommendations that are likely to be of interest to a user Approaches: Content-based, collaborative, or their hybrid ◼Content-based: Recommends items that are similar to items the user preferred or queried in the past ◼Collaborative filtering: Consider a user's social environment, opinions of other customers who have similar tastes or preferences Data mining and recommender systems ◼Users C × items S: extract from known to unknown ratings to predict user-item combinations ◼Memory-based method often uses k-nearest neighbor approach ◼Model-based method uses a collection of ratings to learn a model (e.g., probabilistic models, clustering, Bayesian networks, etc.) ◼Hybrid approaches integrate both to improve performance (e.g., using ensemble) Data Mining Trends and Research Frontiers ◼Data Mining Applications ◼Data Mining and Society ◼Data Mining Trends ◼Summary Ubiquitous and Invisible Data Mining Ubiquitous Data Mining ◼Data mining is used everywhere, e.g., online shopping ◼Ex. Customer relationship management (CRM) Invisible Data Mining ◼Invisible: Data mining functions are built in daily life operations ◼Ex. Google search: Users may be unaware that they are examining results returned by data ◼Invisible data mining is highly desirable ◼Invisible mining needs to consider efficiency and scalability, user interaction, incorporation of background knowledge and visualization techniques, finding interesting patterns, real-time, ... ◼Further work: Integration of data mining into existing business and scientific technologies to provide domain-specific data mining tools Privacy, Security and Social Impacts of Data Mining Many data mining applications do not touch personal data ◼E.g., meteorology, astronomy, geography, geology, biology, and other scientific and engineering data Many DM studies are on developing scalable algorithms to find general or statistically significant patterns, not touching individuals The real privacy concern: unconstrained access of individual records, especially privacy-sensitive information Method 1: Removing sensitive IDs associated with the data Method 2: Data security-enhancing methods ◼Multi-level security model: permit to access to only authorized level ◼Encryption: e.g., blind signatures, biometric encryption, and anonymous databases (personal information is encrypted and stored at different locations) Method 3: Privacy-preserving data mining methods Privacy-Preserving Data Mining Privacy-preserving (privacy-enhanced or privacy-sensitive) mining: ◼Obtaining valid mining results without disclosing the underlying sensitive data values ◼Often needs trade-off between information loss and privacy Privacy-preserving data mining methods: ◼Randomization (e.g., perturbation): Add noise to the data in order to mask some attribute values of records ◼K-anonymity and l-diversity: Alter individual records so that they cannot be uniquely identified ◼k-anonymity: Any given record maps onto at least k other records ◼l-diversity: enforcing intra-group diversity of sensitive values ◼Distributed privacy preservation: Data partitioned and distributed either horizontally, vertically, or a combination of both ◼Downgrading the effectiveness of data mining: The output of data mining may violate privacy ◼Modify data or mining results, e.g., hiding some association rules or slightly distorting some classification models K-Anonymity Example with suppression and Generalization 2-Anonymity w.r.t Age, Gender and State For any combination of the above at least 2 rows are there Chapter 13: Data Mining Trends and Research Frontiers ◼Other Methodologies of Data Mining ◼Data Mining Applications ◼Data Mining and Society ◼Data Mining Trends ◼Summary Trends of Data Mining Application exploration: Dealing with application-specific problems Scalable and interactive data mining methods Integration of data mining with Web search engines, database systems, data warehouse systems and cloud computing systems Mining social and information networks Mining spatiotemporal, moving objects and cyber-physical systems Mining multimedia, text and web data Mining biological and biomedical data Data mining with software engineering and system engineering Visual and audio data mining Distributed data mining and real-time data stream mining Privacy protection and information security in data mining Data Mining Trends and Research Frontiers ◼Mining Complex Types of Data ◼Other Methodologies of Data Mining ◼Data Mining Applications ◼Data Mining and Society ◼Data Mining Trends ◼Summary Summary Application-based mining integrates domain-specific knowledge with data analysis techniques and provide mission-specific solutions Ubiquitous data mining and invisible data mining are penetrating our data lives Privacy and data security are importance issues in data mining, and privacy-preserving data mining has been developed recently Our discussion on trends in data mining shows that data mining is a promising, young field, with great, strategic importance"
  }
]