## 6. Computer Vision

Certainly! Below is a detailed, well-organized, and clear study note based on the lecture content you provided on **Computer Vision** with deep neural networks. I have expanded explanations, clarified key concepts, and structured the note with numbered emoji headings for easy navigation.



## Study Notes: Deep Neural Networks for Computer Vision



### 1. üëÅÔ∏è What is Computer Vision?

Computer Vision is a field of artificial intelligence that enables computers to interpret and understand visual information from the world, much like human vision. The goal is to enable machines to "see" and make sense of images or videos, extracting meaningful information such as identifying objects, understanding scenes, or even predicting actions.

Humans are naturally very good at recognizing objects, understanding scenes, and interpreting subtle visual cues like shading and texture. However, computers face many challenges in matching this ability because images can vary widely due to lighting, viewpoint, scale, and occlusion (objects blocking each other).

Despite these challenges, recent advances in **deep learning** have dramatically improved computer vision capabilities, especially in the last five years. Deep neural networks have accelerated progress, enabling computers to perform tasks like object detection, segmentation, and recognition with increasing accuracy.



### 2. üñºÔ∏è Core Tasks in Computer Vision

Computer vision involves several fundamental tasks, each with its own challenges and methods:

- **Image Classification:** Assigning a label to an entire image (e.g., "cat," "dog," "car").
- **Object Detection:** Identifying and locating multiple objects within an image by drawing bounding boxes around them.
- **Semantic Segmentation:** Labeling each pixel in an image with a category (e.g., all pixels belonging to "grass" or "cat"), but without distinguishing between different instances of the same object.
- **Instance Segmentation:** Like semantic segmentation, but also differentiates between individual instances of objects (e.g., two separate cats).
- **Localization:** Finding the position of a single object within an image.
- **3D Shape Prediction:** Understanding the three-dimensional structure of objects from 2D images.

Each of these tasks requires different approaches and architectures, often building on convolutional neural networks (CNNs).



### 3. üîç Why is Computer Vision Difficult?

Computer vision is challenging due to several factors:

- **Viewpoint Variation:** Objects look different from different angles.
- **Illumination Changes:** Lighting can drastically alter the appearance of objects.
- **Scale Variation:** Objects can appear at different sizes depending on distance.
- **Intra-class Variation:** Objects of the same class (e.g., dogs) can look very different.
- **Background Clutter:** Objects may be partially hidden or surrounded by distracting backgrounds.
- **Occlusion:** Objects may be partially blocked by other objects.
- **Motion Blur:** Moving objects or camera shake can blur images.

These factors make it hard for algorithms to consistently recognize and segment objects.



### 4. üß† Deep Learning Architectures for Computer Vision

Deep learning, especially convolutional neural networks (CNNs), has revolutionized computer vision. CNNs use layers of filters (kernels) that scan across images to detect patterns like edges, textures, and shapes. These features are combined in deeper layers to recognize complex objects.

Key architectures for object detection and segmentation include:

- **R-CNN (Region-based CNN):** Extracts region proposals (candidate object locations) and classifies each region separately.
- **Fast R-CNN:** Improves speed by running the CNN once on the whole image and then cropping features for each region.
- **Faster R-CNN:** Adds a Region Proposal Network (RPN) to generate proposals directly from CNN features, speeding up the process.
- **Mask R-CNN:** Extends Faster R-CNN by adding a branch to predict pixel-level masks for instance segmentation.
- **YOLO (You Only Look Once):** A single-stage detector that divides the image into a grid and predicts bounding boxes and class probabilities simultaneously, enabling real-time detection.
- **SSD (Single Shot Detector):** Similar to YOLO, it predicts bounding boxes and classes in one pass but uses multiple feature maps at different scales.



### 5. ü™ü Traditional Object Detection Methods

Before deep learning, object detection relied on:

- **Sliding Window Technique:** The image is scanned with a fixed-size window at multiple scales and locations. Each window is classified as object or background. This is computationally expensive because it requires evaluating thousands of windows.
- **Region Proposal Methods (Selective Search):** Instead of scanning every window, these methods generate a smaller set of candidate regions likely to contain objects by grouping similar pixels ("blobs"). These proposals are then classified.

These methods were slow and less accurate compared to modern deep learning approaches.



### 6. üñ•Ô∏è Deep Learning Object Detection: R-CNN Family

#### R-CNN

- Extracts ~2000 region proposals using selective search.
- Warps each region to a fixed size and passes it through a CNN.
- Classifies each region with a Support Vector Machine (SVM).
- Applies bounding box regression to refine object locations.
- **Problem:** Very slow because it runs CNN separately on each region.

#### Fast R-CNN

- Runs CNN once on the entire image to produce a feature map.
- Crops features corresponding to each region proposal (using RoI pooling).
- Classifies and refines bounding boxes in one network.
- Much faster than R-CNN but still relies on slow selective search for proposals.

#### Faster R-CNN

- Introduces a Region Proposal Network (RPN) that shares CNN features.
- RPN predicts object proposals directly, eliminating selective search.
- Two-stage detector: first propose regions, then classify and refine.
- Balances speed and accuracy well but is not real-time.



### 7. ‚ö° Single-Stage Detectors: YOLO and SSD

Single-stage detectors predict bounding boxes and class probabilities in one pass, making them faster and suitable for real-time applications.

#### YOLO (You Only Look Once)

- Divides the image into an SxS grid (commonly 7x7).
- Each grid cell predicts bounding boxes (center coordinates, width, height) and confidence scores.
- Also predicts class probabilities conditioned on the presence of an object.
- Uses a single CNN to predict all outputs simultaneously.
- Advantages: Extremely fast, global reasoning over the entire image.
- Limitations: Struggles with small objects, localization errors, and only one class per grid cell.

#### SSD (Single Shot Detector)

- Uses multiple feature maps at different scales to detect objects of various sizes.
- Predicts bounding boxes and class scores for multiple default boxes per location.
- Faster than two-stage detectors but slightly less accurate.



### 8. üß© Semantic and Instance Segmentation

#### Semantic Segmentation

- Assigns a class label to every pixel in the image.
- Does not distinguish between different instances of the same class.
- Useful for understanding the overall scene (e.g., labeling all grass pixels).
- Challenges: Requires pixel-level annotations for training, computationally expensive.

#### Instance Segmentation

- Extends semantic segmentation by differentiating between individual object instances.
- Mask R-CNN is a popular architecture that adds a mask prediction branch to Faster R-CNN.
- Outputs a binary mask for each detected object, enabling precise object boundaries.



### 9. üîÑ Techniques in Semantic Segmentation

- **Sliding Window:** Classify each pixel by extracting patches around it. Inefficient due to overlapping computations.
- **Fully Convolutional Networks (FCN):** Replace fully connected layers with convolutional layers to output pixel-wise predictions.
- **Downsampling and Upsampling:** CNNs reduce spatial resolution to learn abstract features (downsampling), then use upsampling (e.g., unpooling, transposed convolution) to restore resolution for pixel-level predictions.
- **RoI Align:** Improves accuracy in instance segmentation by precisely aligning features to regions of interest.



### 10. üìè Evaluation Metrics and Post-Processing

- **Intersection over Union (IoU):** Measures overlap between predicted and ground truth bounding boxes. Higher IoU means better localization.
- **Precision:** Percentage of predicted objects that are correct.
- **Recall:** Percentage of ground truth objects detected.
- **Non-Maximum Suppression (NMS):** Removes redundant overlapping bounding boxes by keeping only the highest scoring box per object.



### 11. üåç Applications of Computer Vision

Computer vision powers many real-world applications across diverse fields:

- **Autonomous Vehicles:** Detecting pedestrians, vehicles, and obstacles.
- **Medical Imaging:** Tumor detection, skin lesion analysis, X-ray classification.
- **Biometrics:** Face recognition and analysis for security.
- **Gaming and Augmented Reality:** Real-time object tracking and interaction.
- **Robotics:** Navigation and manipulation.
- **Environmental Monitoring:** Remote sensing and eco-sustainable informatics.
- **Industrial Automation:** Quality control, packing/unpacking processes.
- **Space Exploration:** Analyzing images from satellites and rovers.



### 12. üîÆ Future Directions and Ethical Considerations

- Computer vision is rapidly evolving with new architectures and generative AI methods.
- Emerging trends include 3D object detection, dense captioning (combining detection with natural language descriptions), and scene graph generation (understanding relationships between objects).
- Ethical concerns include privacy, surveillance misuse, and bias in datasets.
- Responsible development and deployment are crucial to maximize benefits and minimize harm.



## Summary

Computer vision uses deep neural networks to enable machines to interpret images and videos. Key tasks include classification, detection, and segmentation, each with unique challenges. Traditional methods like sliding windows have been replaced by deep learning architectures such as R-CNN variants and YOLO, which balance accuracy and speed. Semantic and instance segmentation provide pixel-level understanding of scenes. Applications span healthcare, autonomous driving, security, and more. The field is fast-moving, with exciting future prospects and important ethical considerations.



**Recommended Reading:**  
Rick Szeliski, *Computer Vision: Algorithms and Applications* (available online at http://szeliski.org/Book/)



If you want, I can also help with visual diagrams or example code snippets for any of these topics!

## Questions

Certainly! Here are 20 multiple-choice questions (MCQs) covering the key concepts and ideas from the lecture on Deep Neural Networks for Computer Vision. Some questions have one correct answer, others may have multiple correct answers to increase difficulty.



#### 1. What are the main challenges that make computer vision difficult?  
A) Viewpoint variation  
B) Illumination changes  
C) Intra-class variation  
D) Unlimited computational power  



#### 2. Which of the following tasks are considered core problems in computer vision?  
A) Image classification  
B) Object detection  
C) Semantic segmentation  
D) Text summarization  



#### 3. What is the primary difference between semantic segmentation and instance segmentation?  
A) Semantic segmentation labels each pixel with a category but does not distinguish object instances  
B) Instance segmentation differentiates between individual objects of the same class  
C) Semantic segmentation only works on grayscale images  
D) Instance segmentation does not provide pixel-level labels  



#### 4. Which of the following statements about convolutional neural networks (CNNs) is true?  
A) CNNs use fully connected layers exclusively  
B) CNNs apply filters that scan across the input image to detect patterns  
C) CNNs cannot be used for pixel-level predictions  
D) CNNs reduce spatial resolution through pooling or strided convolution  



#### 5. What is the main drawback of the sliding window technique in object detection?  
A) It requires a large number of CNN forward passes, making it computationally expensive  
B) It cannot detect multiple objects in an image  
C) It only works for grayscale images  
D) It does not require labeled data  



#### 6. Select all that apply: Which methods are part of the R-CNN family?  
A) R-CNN  
B) Fast R-CNN  
C) Faster R-CNN  
D) YOLO  



#### 7. How does Fast R-CNN improve over the original R-CNN?  
A) It runs the CNN once on the entire image instead of on each region proposal  
B) It removes the need for region proposals  
C) It uses selective search for region proposals  
D) It uses SVMs for classification  



#### 8. What is the role of the Region Proposal Network (RPN) in Faster R-CNN?  
A) To generate candidate object regions directly from CNN features  
B) To classify objects in the image  
C) To perform pixel-wise segmentation  
D) To replace the fully connected layers  



#### 9. Which of the following are advantages of YOLO compared to two-stage detectors like Faster R-CNN?  
A) Real-time detection speed  
B) Global reasoning over the entire image  
C) Higher accuracy on small objects  
D) End-to-end training  



#### 10. What are some limitations of the original YOLO (version 1)?  
A) Each grid cell can only predict one class  
B) Struggles with unusual bounding box aspect ratios  
C) Performs poorly on large objects  
D) Confidence scores for empty grid cells dominate training gradients  



#### 11. In semantic segmentation, why is it inefficient to classify each pixel using a sliding window approach?  
A) Because it requires classifying many overlapping patches independently  
B) Because it cannot handle multiple classes  
C) Because it only works on small images  
D) Because it does not use convolutional layers  



#### 12. What is the purpose of upsampling (e.g., unpooling or transposed convolution) in fully convolutional networks for segmentation?  
A) To restore the spatial resolution of feature maps to match the input image size  
B) To reduce the number of parameters in the network  
C) To perform classification on the entire image  
D) To generate region proposals  



#### 13. Which of the following statements about Non-Maximum Suppression (NMS) is true?  
A) NMS removes overlapping bounding boxes to keep only the most confident prediction per object  
B) NMS is used to increase the number of detected objects  
C) NMS is applied before generating region proposals  
D) NMS cannot be used when objects overlap  



#### 14. What does Intersection over Union (IoU) measure in object detection?  
A) The overlap between predicted and ground truth bounding boxes  
B) The confidence score of a predicted bounding box  
C) The number of objects detected in an image  
D) The pixel-wise accuracy of segmentation  



#### 15. Which of the following are typical applications of computer vision?  
A) Autonomous vehicles  
B) Tumor identification in medical images  
C) Natural language processing  
D) Face recognition and biometric analysis  



#### 16. Which backbone networks are commonly used in object detection architectures?  
A) VGG16  
B) ResNet-101  
C) Inception V3  
D) LSTM  



#### 17. What is the main difference between two-stage and single-stage object detectors?  
A) Two-stage detectors first generate region proposals, then classify them  
B) Single-stage detectors predict bounding boxes and classes in one pass  
C) Two-stage detectors are generally faster than single-stage detectors  
D) Single-stage detectors cannot detect multiple objects  



#### 18. Which of the following statements about Mask R-CNN are correct?  
A) It extends Faster R-CNN by adding a mask prediction branch for instance segmentation  
B) It uses RoI Align to improve feature alignment for mask prediction  
C) It only performs semantic segmentation  
D) It predicts a binary mask for each detected object  



#### 19. What are some challenges specific to 3D object detection compared to 2D detection?  
A) Predicting 3D bounding box parameters including orientation and depth  
B) Handling roll and pitch angles of objects  
C) It is a simpler problem than 2D detection  
D) Requires monocular or multi-view camera inputs  



#### 20. Which of the following techniques help improve training stability and avoid overfitting in deep learning-based object detection?  
A) Dropout  
B) Extensive data augmentation  
C) Using only fully connected layers  
D) Reducing the number of training samples  



These questions cover the fundamental concepts, architectures, challenges, evaluation metrics, and applications discussed in the lecture. They include straightforward and tricky questions to test deep understanding. Let me know if you want the answer key or explanations!

## Answers

Certainly! Below are the correct answers for each question along with brief explanations to clarify why those answers are correct.



#### 1. What are the main challenges that make computer vision difficult?  
**Correct answers:** A) Viewpoint variation, B) Illumination changes, C) Intra-class variation  
**Explanation:** These factors cause objects to appear differently in images, making recognition and detection challenging. Unlimited computational power (D) is not a challenge but a hypothetical resource.



#### 2. Which of the following tasks are considered core problems in computer vision?  
**Correct answers:** A) Image classification, B) Object detection, C) Semantic segmentation  
**Explanation:** These are fundamental tasks in computer vision. Text summarization (D) is a natural language processing task, not computer vision.



#### 3. What is the primary difference between semantic segmentation and instance segmentation?  
**Correct answers:** A) Semantic segmentation labels each pixel with a category but does not distinguish object instances, B) Instance segmentation differentiates between individual objects of the same class  
**Explanation:** Semantic segmentation groups all pixels of the same class together, while instance segmentation separates individual objects.



#### 4. Which of the following statements about convolutional neural networks (CNNs) is true?  
**Correct answers:** B) CNNs apply filters that scan across the input image to detect patterns, D) CNNs reduce spatial resolution through pooling or strided convolution  
**Explanation:** CNNs use convolutional filters to detect features and often reduce spatial resolution to learn abstract representations. Fully connected layers are used but not exclusively (A), and CNNs can be adapted for pixel-level predictions (C).



#### 5. What is the main drawback of the sliding window technique in object detection?  
**Correct answer:** A) It requires a large number of CNN forward passes, making it computationally expensive  
**Explanation:** Sliding window scans many locations and scales, requiring many CNN evaluations, which is slow.



#### 6. Select all that apply: Which methods are part of the R-CNN family?  
**Correct answers:** A) R-CNN, B) Fast R-CNN, C) Faster R-CNN  
**Explanation:** YOLO is a different, single-stage detector, not part of the R-CNN family.



#### 7. How does Fast R-CNN improve over the original R-CNN?  
**Correct answers:** A) It runs the CNN once on the entire image instead of on each region proposal, C) It uses selective search for region proposals  
**Explanation:** Fast R-CNN shares computation by running CNN once and crops features for proposals. It still uses selective search but no longer runs CNN per region. It does not remove region proposals (B) or use SVMs (D).



#### 8. What is the role of the Region Proposal Network (RPN) in Faster R-CNN?  
**Correct answer:** A) To generate candidate object regions directly from CNN features  
**Explanation:** RPN replaces slow selective search by predicting proposals from CNN features, speeding up detection.



#### 9. Which of the following are advantages of YOLO compared to two-stage detectors like Faster R-CNN?  
**Correct answers:** A) Real-time detection speed, B) Global reasoning over the entire image, D) End-to-end training  
**Explanation:** YOLO is fast, reasons globally, and is trained end-to-end. However, it struggles with small objects (C).



#### 10. What are some limitations of the original YOLO (version 1)?  
**Correct answers:** A) Each grid cell can only predict one class, B) Struggles with unusual bounding box aspect ratios, D) Confidence scores for empty grid cells dominate training gradients  
**Explanation:** YOLO v1 has these limitations affecting accuracy. It performs better on large objects, so (C) is incorrect.



#### 11. In semantic segmentation, why is it inefficient to classify each pixel using a sliding window approach?  
**Correct answer:** A) Because it requires classifying many overlapping patches independently  
**Explanation:** Overlapping patches cause redundant computation, making sliding window inefficient.



#### 12. What is the purpose of upsampling (e.g., unpooling or transposed convolution) in fully convolutional networks for segmentation?  
**Correct answer:** A) To restore the spatial resolution of feature maps to match the input image size  
**Explanation:** Downsampling reduces resolution; upsampling restores it for pixel-wise predictions.



#### 13. Which of the following statements about Non-Maximum Suppression (NMS) is true?  
**Correct answer:** A) NMS removes overlapping bounding boxes to keep only the most confident prediction per object  
**Explanation:** NMS filters redundant detections to avoid multiple boxes for the same object.



#### 14. What does Intersection over Union (IoU) measure in object detection?  
**Correct answer:** A) The overlap between predicted and ground truth bounding boxes  
**Explanation:** IoU quantifies how well predicted boxes match ground truth.



#### 15. Which of the following are typical applications of computer vision?  
**Correct answers:** A) Autonomous vehicles, B) Tumor identification in medical images, D) Face recognition and biometric analysis  
**Explanation:** These are common CV applications. Natural language processing (C) is unrelated.



#### 16. Which backbone networks are commonly used in object detection architectures?  
**Correct answers:** A) VGG16, B) ResNet-101, C) Inception V3  
**Explanation:** These CNN architectures are popular backbones. LSTM (D) is for sequential data, not images.



#### 17. What is the main difference between two-stage and single-stage object detectors?  
**Correct answers:** A) Two-stage detectors first generate region proposals, then classify them, B) Single-stage detectors predict bounding boxes and classes in one pass  
**Explanation:** Two-stage detectors separate proposal and classification; single-stage do both simultaneously. Two-stage are generally slower (C), and single-stage can detect multiple objects (D).



#### 18. Which of the following statements about Mask R-CNN are correct?  
**Correct answers:** A) It extends Faster R-CNN by adding a mask prediction branch for instance segmentation, B) It uses RoI Align to improve feature alignment for mask prediction, D) It predicts a binary mask for each detected object  
**Explanation:** Mask R-CNN adds mask prediction and uses RoI Align for precise localization. It performs instance, not just semantic segmentation (C).



#### 19. What are some challenges specific to 3D object detection compared to 2D detection?  
**Correct answers:** A) Predicting 3D bounding box parameters including orientation and depth, B) Handling roll and pitch angles of objects, D) Requires monocular or multi-view camera inputs  
**Explanation:** 3D detection is more complex due to extra dimensions and orientation. It is harder, not simpler (C).



#### 20. Which of the following techniques help improve training stability and avoid overfitting in deep learning-based object detection?  
**Correct answers:** A) Dropout, B) Extensive data augmentation  
**Explanation:** Dropout and data augmentation reduce overfitting. Using only fully connected layers (C) or reducing training samples (D) would harm performance.



If you want, I can also provide explanations for incorrect options or suggest further reading!