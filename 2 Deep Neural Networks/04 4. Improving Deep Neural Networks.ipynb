{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e110e568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 4. Improving Deep Neural Networks\n",
       "\n",
       "Certainly! Below is a detailed, well-organized study note based on the lecture content you provided. It explains all key concepts clearly and thoroughly, using a friendly and accessible tone. Emojis and numbered main headings are included for clarity and structure.\n",
       "\n",
       "\n",
       "\n",
       "## Study Notes: Improving Deep Neural Networks üöÄ\n",
       "\n",
       "\n",
       "\n",
       "### 1. üß† Introduction to Improving Deep Neural Networks\n",
       "\n",
       "Deep neural networks (DNNs) have revolutionized many fields, but training them effectively and improving their performance remains a challenge. This study note covers several important techniques and architectural innovations that help make deep networks train faster, generalize better, and avoid common pitfalls like overfitting and vanishing gradients. We will explore normalization methods, dropout, early stopping, data augmentation, and advanced architectures like Residual Networks (ResNets) and DenseNets.\n",
       "\n",
       "\n",
       "\n",
       "### 2. ‚öñÔ∏è Normalization: Making Inputs and Layers Consistent\n",
       "\n",
       "##### Why Normalize Inputs?\n",
       "\n",
       "When training neural networks, the scale and distribution of input features can greatly affect learning. If features have very different ranges or distributions, the network may struggle to converge or learn efficiently. Normalizing inputs means adjusting them so they have a mean (average) of zero and a standard deviation (spread) of one. This standardization helps the network learn more quickly and reliably.\n",
       "\n",
       "**Important:** The same mean (Œº) and standard deviation (œÉ) used to normalize the training data must also be applied to the test data. This ensures consistency and prevents the model from seeing data in a different scale during testing.\n",
       "\n",
       "##### Normalization for Intermediate Layers: Batch Normalization\n",
       "\n",
       "Normalization isn‚Äôt just for inputs. It can also be applied inside the network, between layers, to stabilize and speed up training. This is called **Batch Normalization**.\n",
       "\n",
       "- **What it does:** Batch normalization normalizes the weighted sums (often called pre-activations or Y‚Äôs) of a layer before passing them through the activation function.\n",
       "- **How it works:** For each mini-batch during training, it computes the mean and standard deviation of the layer‚Äôs outputs and normalizes them to have zero mean and unit variance.\n",
       "- **Scaling and shifting:** Simply normalizing to zero mean and unit variance might limit the network‚Äôs ability to represent complex functions. So, batch normalization includes learnable parameters that scale and shift the normalized values, allowing the network to restore any necessary distribution.\n",
       "- **Placement:** It is common to apply batch normalization **before** the activation function, although some debate exists about whether it should be before or after.\n",
       "\n",
       "Batch normalization helps reduce the problem of internal covariate shift, where the distribution of inputs to each layer changes during training, making learning more stable and faster.\n",
       "\n",
       "\n",
       "\n",
       "### 3. ‚ùå Dropout: Preventing Overfitting by Randomly Dropping Neurons\n",
       "\n",
       "Overfitting happens when a network learns the training data too well, including noise, and performs poorly on new data. Dropout is a simple yet powerful technique to reduce overfitting.\n",
       "\n",
       "- **How dropout works:** During training, dropout randomly \"drops\" (sets to zero) a fraction of neurons in the network along with their connections. Each neuron is kept with a probability $ p $ (e.g., 0.8 means 80% chance to keep a neuron).\n",
       "- **Effect:** This forces the network to not rely too heavily on any single neuron and encourages redundancy and robustness.\n",
       "- **During testing:** Dropout is turned off, but the outgoing weights of neurons are scaled by the probability $ p $ to balance the expected output.\n",
       "\n",
       "Dropout was introduced by Srivastava et al. (2014) and remains a widely used regularization method.\n",
       "\n",
       "\n",
       "\n",
       "### 4. ‚èπÔ∏è Early Stopping: Avoiding Overtraining\n",
       "\n",
       "Training a neural network for too long can lead to overfitting. Early stopping is a simple way to prevent this:\n",
       "\n",
       "- **How it works:** During training, monitor the error (loss) on a separate validation set.\n",
       "- **Stop training:** As soon as the validation error starts to increase (even if training error keeps decreasing), stop training.\n",
       "- This means stopping **before** the training error reaches its minimum, which helps keep the model generalizable.\n",
       "\n",
       "\n",
       "\n",
       "### 5. üîÑ Data Augmentation: Creating More Training Data\n",
       "\n",
       "More data usually means better models, but collecting new data can be expensive or impractical. Data augmentation artificially increases the size of the training set by applying transformations to existing data.\n",
       "\n",
       "- **Examples:** Rotations, flips, scaling, cropping, color changes, noise addition.\n",
       "- **Benefits:** Augmented data helps the model generalize better by exposing it to more varied examples.\n",
       "- **Limitations:** Augmented data is not as good as real new data but is a cost-effective alternative.\n",
       "\n",
       "\n",
       "\n",
       "### 6. üèóÔ∏è Problems with Typical Deep CNNs\n",
       "\n",
       "Before diving into advanced architectures, it‚Äôs important to understand the challenges faced by traditional deep convolutional neural networks (CNNs):\n",
       "\n",
       "- **Exploding/Vanishing Gradients:** Gradients used for learning can become too large or too small as they propagate back through many layers, making training unstable or slow.\n",
       "- **Degradation Problem:** As networks get deeper, accuracy can saturate and then degrade, even on training data. This is not due to overfitting but because deeper layers sometimes fail to learn useful transformations.\n",
       "\n",
       "##### Solutions to these problems include:\n",
       "\n",
       "- Normalized weight initialization (Glorot & Bengio, 2010)\n",
       "- Batch normalization (Ioffe & Szegedy, 2015)\n",
       "- New architectures like ResNets and DenseNets\n",
       "\n",
       "\n",
       "\n",
       "### 7. üîó Residual Networks (ResNets): Learning Residuals Instead of Direct Mappings\n",
       "\n",
       "ResNets were introduced by He et al. (2016) to solve the degradation problem by making it easier for layers to learn identity mappings.\n",
       "\n",
       "##### The core idea:\n",
       "\n",
       "- Instead of learning a direct mapping $ H(X) $ from input $ X $ to output, the network learns a **residual function** $ F(X) = H(X) - X $.\n",
       "- The output of the block is then $ Y = F(X) + X $.\n",
       "- If the optimal function is close to identity, learning the residual $ F(X) $ (which might be zero or small) is easier than learning $ H(X) $ from scratch.\n",
       "\n",
       "##### Why this helps:\n",
       "\n",
       "- The identity shortcut connections allow gradients to flow directly through the network, reducing vanishing gradient problems.\n",
       "- If the residual is zero, the block simply passes the input forward unchanged, preventing degradation.\n",
       "- This design enables very deep networks (e.g., 50, 101, 152 layers) to be trained effectively.\n",
       "\n",
       "##### Architecture details:\n",
       "\n",
       "- Residual blocks typically have 2 or 3 layers.\n",
       "- If the input and output dimensions differ, a linear projection $ W_s $ is applied to $ X $ to match dimensions before addition.\n",
       "\n",
       "##### Experimental results:\n",
       "\n",
       "- ResNets outperform traditional deep networks on benchmarks like ImageNet.\n",
       "- They achieve lower training and validation errors without increasing the number of parameters.\n",
       "\n",
       "\n",
       "\n",
       "### 8. üåê Densely Connected Convolutional Networks (DenseNets): Connecting All Layers\n",
       "\n",
       "DenseNets, introduced by Huang et al. (2017), take connectivity a step further by connecting **every layer to every other layer** in a feed-forward fashion.\n",
       "\n",
       "##### How DenseNets work:\n",
       "\n",
       "- Each layer receives as input the concatenation of the outputs of all preceding layers.\n",
       "- Formally, the output of layer $ l $ is:\n",
       "  $$\n",
       "  X_l = H_l([X_0, X_1, ..., X_{l-1}])\n",
       "  $$\n",
       "  where $ H_l $ is a composite function (batch norm, ReLU, convolution), and $[ \\cdot ]$ denotes concatenation, not addition.\n",
       "\n",
       "##### Advantages of DenseNets:\n",
       "\n",
       "- **Fewer parameters:** Because each layer only needs to produce a small number of new feature maps (channels), the total number of parameters is reduced.\n",
       "- **Improved gradient flow:** Direct connections allow gradients to flow easily backward, alleviating vanishing gradients.\n",
       "- **Feature reuse:** Layers can reuse features from all previous layers, improving efficiency.\n",
       "\n",
       "##### Why fewer parameters?\n",
       "\n",
       "- Traditional networks must pass all information through each layer, requiring many feature maps.\n",
       "- DenseNets separate preserved information (from previous layers) and new information (computed by the current layer).\n",
       "- This reduces the need for large numbers of feature maps per layer.\n",
       "\n",
       "##### Architecture details:\n",
       "\n",
       "- DenseNets are organized into **dense blocks**, where layers are densely connected.\n",
       "- Between dense blocks, **transition layers** reduce the size of feature maps using batch normalization, 1x1 convolutions, and 2x2 average pooling.\n",
       "- This allows the network to reduce spatial dimensions while maintaining dense connectivity within blocks.\n",
       "\n",
       "##### Experimental results:\n",
       "\n",
       "- DenseNets achieve competitive or better accuracy than ResNets on ImageNet with fewer parameters and less computation (FLOPs).\n",
       "\n",
       "\n",
       "\n",
       "### 9. üìö Summary and References\n",
       "\n",
       "##### Summary\n",
       "\n",
       "- **Normalization** (input and batch normalization) stabilizes and speeds up training.\n",
       "- **Dropout** prevents overfitting by randomly dropping neurons during training.\n",
       "- **Early stopping** avoids overtraining by monitoring validation error.\n",
       "- **Data augmentation** artificially increases training data diversity.\n",
       "- **ResNets** solve degradation by learning residual functions with identity shortcuts.\n",
       "- **DenseNets** connect all layers directly, improving gradient flow and parameter efficiency.\n",
       "\n",
       "##### Key References\n",
       "\n",
       "- Ioffe & Szegedy (2015): Batch Normalization\n",
       "- Srivastava et al. (2014): Dropout\n",
       "- He et al. (2016): Deep Residual Learning (ResNet)\n",
       "- Huang et al. (2017): Densely Connected Convolutional Networks (DenseNet)\n",
       "- Glorot & Bengio (2010): Weight Initialization\n",
       "\n",
       "\n",
       "\n",
       "If you want, I can also help with diagrams or code examples to illustrate these concepts further! Just let me know.\n",
       "\n",
       "## Questions\n",
       "\n",
       "Certainly! Here are 20 multiple-choice questions (MCQs) that comprehensively cover the topic of improving deep neural networks based on the lecture content you provided. Each question may have one or more correct answers, but answers are **not** marked.\n",
       "\n",
       "\n",
       "\n",
       "##### 1. Which of the following are reasons to normalize input data before training a neural network?  \n",
       "A) To ensure consistent scale across features  \n",
       "B) To speed up convergence during training  \n",
       "C) To increase the model‚Äôs capacity  \n",
       "D) To prevent overfitting  \n",
       "\n",
       "\n",
       "\n",
       "##### 2. Batch normalization is typically applied:  \n",
       "A) Before the activation function  \n",
       "B) After the activation function  \n",
       "C) Only on the input layer  \n",
       "D) On the weighted sums (pre-activations) of a layer  \n",
       "\n",
       "\n",
       "\n",
       "##### 3. What is the main purpose of learnable scaling and shifting parameters in batch normalization?  \n",
       "A) To allow the network to restore any necessary distribution after normalization  \n",
       "B) To reduce the number of parameters in the network  \n",
       "C) To prevent vanishing gradients  \n",
       "D) To increase the batch size  \n",
       "\n",
       "\n",
       "\n",
       "##### 4. Dropout during training:  \n",
       "A) Randomly removes neurons and their connections with a fixed probability  \n",
       "B) Is applied only during testing  \n",
       "C) Helps prevent overfitting by forcing redundancy  \n",
       "D) Increases the number of neurons in the network  \n",
       "\n",
       "\n",
       "\n",
       "##### 5. During testing, how are the weights adjusted in a network trained with dropout?  \n",
       "A) Weights are multiplied by the dropout probability $ p $  \n",
       "B) Weights are multiplied by $ 1-p $  \n",
       "C) Dropout is applied as during training  \n",
       "D) Weights remain unchanged  \n",
       "\n",
       "\n",
       "\n",
       "##### 6. Early stopping is used to:  \n",
       "A) Stop training when training error reaches zero  \n",
       "B) Stop training when validation error starts to increase  \n",
       "C) Prevent overfitting by stopping before training error is minimized  \n",
       "D) Increase the training time for better accuracy  \n",
       "\n",
       "\n",
       "\n",
       "##### 7. Which of the following are true about data augmentation?  \n",
       "A) It creates synthetic data by transforming existing data  \n",
       "B) It is always better than collecting new real data  \n",
       "C) It helps produce more generalized models  \n",
       "D) It is computationally expensive compared to collecting new data  \n",
       "\n",
       "\n",
       "\n",
       "##### 8. The degradation problem in deep CNNs refers to:  \n",
       "A) Overfitting due to too many parameters  \n",
       "B) Saturation and then decrease of accuracy as network depth increases  \n",
       "C) Vanishing gradients causing training to fail  \n",
       "D) Training accuracy decreasing as more layers are added  \n",
       "\n",
       "\n",
       "\n",
       "##### 9. Why can adding more layers to a network theoretically not degrade training accuracy?  \n",
       "A) Because extra layers can learn identity mappings  \n",
       "B) Because deeper networks always have more capacity  \n",
       "C) Because the optimizer ignores extra layers  \n",
       "D) Because additional layers always improve feature extraction  \n",
       "\n",
       "\n",
       "\n",
       "##### 10. In ResNet, the residual function $ F(X) $ is defined as:  \n",
       "A) $ F(X) = H(X) + X $  \n",
       "B) $ F(X) = H(X) - X $  \n",
       "C) $ F(X) = X - H(X) $  \n",
       "D) $ F(X) = H(X) \\times X $  \n",
       "\n",
       "\n",
       "\n",
       "##### 11. The main hypothesis behind ResNet is that:  \n",
       "A) Learning the residual $ F(X) $ is easier than learning the original mapping $ H(X) $  \n",
       "B) Identity mappings are impossible to learn  \n",
       "C) Residual connections increase the number of parameters significantly  \n",
       "D) Residual connections reduce vanishing gradient problems  \n",
       "\n",
       "\n",
       "\n",
       "##### 12. When the dimensions of $ F(X) $ and $ X $ differ in ResNet, what is done?  \n",
       "A) The residual is discarded  \n",
       "B) A linear projection $ W_s $ is applied to $ X $ to match dimensions  \n",
       "C) The network throws an error  \n",
       "D) The dimensions are ignored and added directly  \n",
       "\n",
       "\n",
       "\n",
       "##### 13. Which of the following statements about DenseNets are true?  \n",
       "A) Each layer receives input from all preceding layers via concatenation  \n",
       "B) DenseNets use addition of feature maps like ResNets  \n",
       "C) DenseNets have fewer parameters than traditional CNNs with similar depth  \n",
       "D) DenseNets alleviate vanishing gradient problems by direct connections  \n",
       "\n",
       "\n",
       "\n",
       "##### 14. Why do DenseNets require fewer parameters despite dense connectivity?  \n",
       "A) Because each layer outputs fewer feature maps  \n",
       "B) Because information from previous layers is passed directly, reducing redundancy  \n",
       "C) Because DenseNets use smaller batch sizes  \n",
       "D) Because DenseNets do not use convolutional layers  \n",
       "\n",
       "\n",
       "\n",
       "##### 15. In DenseNet, the output of layer $ l $ is computed as:  \n",
       "A) $ X_l = H_l(X_{l-1}) $  \n",
       "B) $ X_l = H_l(\\sum_{i=0}^{l-1} X_i) $  \n",
       "C) $ X_l = H_l([X_0, X_1, ..., X_{l-1}]) $ (concatenation)  \n",
       "D) $ X_l = H_l(X_0) $  \n",
       "\n",
       "\n",
       "\n",
       "##### 16. What is the role of transition layers in DenseNets?  \n",
       "A) To increase the size of feature maps  \n",
       "B) To reduce the size of feature maps between dense blocks  \n",
       "C) To add dropout layers  \n",
       "D) To concatenate outputs of all layers  \n",
       "\n",
       "\n",
       "\n",
       "##### 17. Which of the following techniques help alleviate the vanishing gradient problem?  \n",
       "A) Batch normalization  \n",
       "B) Residual connections in ResNets  \n",
       "C) Dense connectivity in DenseNets  \n",
       "D) Early stopping  \n",
       "\n",
       "\n",
       "\n",
       "##### 18. Which of the following statements about batch normalization are correct?  \n",
       "A) It normalizes layer outputs to zero mean and unit variance within a mini-batch  \n",
       "B) It always reduces the number of parameters in the network  \n",
       "C) It can be applied before or after the activation function, but before is more common  \n",
       "D) It eliminates the need for dropout  \n",
       "\n",
       "\n",
       "\n",
       "##### 19. What happens if dropout is applied during testing?  \n",
       "A) The model‚Äôs performance improves  \n",
       "B) The output becomes unstable and inconsistent  \n",
       "C) The model behaves as if it is trained without dropout  \n",
       "D) The weights are scaled by the dropout probability  \n",
       "\n",
       "\n",
       "\n",
       "##### 20. Which of the following are true about the degradation problem and ResNet‚Äôs solution?  \n",
       "A) Degradation occurs because deeper layers fail to learn identity mappings  \n",
       "B) ResNet solves degradation by forcing layers to learn identity mappings explicitly  \n",
       "C) ResNet allows layers to learn residuals, which can be zero if identity is optimal  \n",
       "D) Degradation is caused by overfitting and can be fixed by dropout alone  \n",
       "\n",
       "\n",
       "\n",
       "These questions cover all the key concepts from normalization, dropout, early stopping, data augmentation, to advanced architectures like ResNet and DenseNet, including tricky and conceptual points. Let me know if you want me to provide answer keys or explanations!\n",
       "\n",
       "## Answers\n",
       "\n",
       "Certainly! Below are the correct answers for each question along with brief explanations.\n",
       "\n",
       "\n",
       "\n",
       "##### 1. Which of the following are reasons to normalize input data before training a neural network?  \n",
       "**Correct:** A) To ensure consistent scale across features  \n",
       "B) To speed up convergence during training  \n",
       "**Explanation:** Normalizing inputs ensures features have similar scales, which helps the network learn more efficiently and converge faster. It does not directly increase model capacity or prevent overfitting.\n",
       "\n",
       "\n",
       "\n",
       "##### 2. Batch normalization is typically applied:  \n",
       "**Correct:** A) Before the activation function  \n",
       "D) On the weighted sums (pre-activations) of a layer  \n",
       "**Explanation:** Batch normalization normalizes the pre-activation outputs (weighted sums) before applying the activation function, which stabilizes training.\n",
       "\n",
       "\n",
       "\n",
       "##### 3. What is the main purpose of learnable scaling and shifting parameters in batch normalization?  \n",
       "**Correct:** A) To allow the network to restore any necessary distribution after normalization  \n",
       "**Explanation:** These parameters let the network adjust normalized outputs to any scale or shift needed, preserving representational power.\n",
       "\n",
       "\n",
       "\n",
       "##### 4. Dropout during training:  \n",
       "**Correct:** A) Randomly removes neurons and their connections with a fixed probability  \n",
       "C) Helps prevent overfitting by forcing redundancy  \n",
       "**Explanation:** Dropout randomly drops neurons during training to prevent co-adaptation and overfitting.\n",
       "\n",
       "\n",
       "\n",
       "##### 5. During testing, how are the weights adjusted in a network trained with dropout?  \n",
       "**Correct:** A) Weights are multiplied by the dropout probability $ p $  \n",
       "**Explanation:** To compensate for dropout during training, weights are scaled by $ p $ during testing to maintain expected output.\n",
       "\n",
       "\n",
       "\n",
       "##### 6. Early stopping is used to:  \n",
       "**Correct:** B) Stop training when validation error starts to increase  \n",
       "C) Prevent overfitting by stopping before training error is minimized  \n",
       "**Explanation:** Early stopping monitors validation error and halts training once it worsens, preventing overfitting.\n",
       "\n",
       "\n",
       "\n",
       "##### 7. Which of the following are true about data augmentation?  \n",
       "**Correct:** A) It creates synthetic data by transforming existing data  \n",
       "C) It helps produce more generalized models  \n",
       "**Explanation:** Data augmentation generates new training examples via transformations, improving generalization. It is not always better than real data and is usually less expensive.\n",
       "\n",
       "\n",
       "\n",
       "##### 8. The degradation problem in deep CNNs refers to:  \n",
       "**Correct:** B) Saturation and then decrease of accuracy as network depth increases  \n",
       "D) Training accuracy decreasing as more layers are added  \n",
       "**Explanation:** Degradation means deeper networks sometimes perform worse even on training data, not due to overfitting.\n",
       "\n",
       "\n",
       "\n",
       "##### 9. Why can adding more layers to a network theoretically not degrade training accuracy?  \n",
       "**Correct:** A) Because extra layers can learn identity mappings  \n",
       "**Explanation:** If extra layers learn identity functions, they do not harm performance, so training accuracy should not degrade.\n",
       "\n",
       "\n",
       "\n",
       "##### 10. In ResNet, the residual function $ F(X) $ is defined as:  \n",
       "**Correct:** B) $ F(X) = H(X) - X $  \n",
       "**Explanation:** ResNet learns the residual $ F(X) $ such that $ H(X) = F(X) + X $.\n",
       "\n",
       "\n",
       "\n",
       "##### 11. The main hypothesis behind ResNet is that:  \n",
       "**Correct:** A) Learning the residual $ F(X) $ is easier than learning the original mapping $ H(X) $  \n",
       "D) Residual connections reduce vanishing gradient problems  \n",
       "**Explanation:** Learning residuals simplifies optimization and shortcut connections help gradients flow better.\n",
       "\n",
       "\n",
       "\n",
       "##### 12. When the dimensions of $ F(X) $ and $ X $ differ in ResNet, what is done?  \n",
       "**Correct:** B) A linear projection $ W_s $ is applied to $ X $ to match dimensions  \n",
       "**Explanation:** To add $ F(X) $ and $ X $, their dimensions must match; a projection aligns them.\n",
       "\n",
       "\n",
       "\n",
       "##### 13. Which of the following statements about DenseNets are true?  \n",
       "**Correct:** A) Each layer receives input from all preceding layers via concatenation  \n",
       "C) DenseNets have fewer parameters than traditional CNNs with similar depth  \n",
       "D) DenseNets alleviate vanishing gradient problems by direct connections  \n",
       "**Explanation:** DenseNets concatenate all previous outputs, improving gradient flow and parameter efficiency.\n",
       "\n",
       "\n",
       "\n",
       "##### 14. Why do DenseNets require fewer parameters despite dense connectivity?  \n",
       "**Correct:** A) Because each layer outputs fewer feature maps  \n",
       "B) Because information from previous layers is passed directly, reducing redundancy  \n",
       "**Explanation:** DenseNets reuse features directly, so each layer only needs to add a small number of new feature maps.\n",
       "\n",
       "\n",
       "\n",
       "##### 15. In DenseNet, the output of layer $ l $ is computed as:  \n",
       "**Correct:** C) $ X_l = H_l([X_0, X_1, ..., X_{l-1}]) $ (concatenation)  \n",
       "**Explanation:** DenseNet concatenates all previous layer outputs as input to the current layer.\n",
       "\n",
       "\n",
       "\n",
       "##### 16. What is the role of transition layers in DenseNets?  \n",
       "**Correct:** B) To reduce the size of feature maps between dense blocks  \n",
       "**Explanation:** Transition layers downsample feature maps to reduce spatial dimensions and control model size.\n",
       "\n",
       "\n",
       "\n",
       "##### 17. Which of the following techniques help alleviate the vanishing gradient problem?  \n",
       "**Correct:** A) Batch normalization  \n",
       "B) Residual connections in ResNets  \n",
       "C) Dense connectivity in DenseNets  \n",
       "**Explanation:** All these techniques improve gradient flow, reducing vanishing gradients.\n",
       "\n",
       "\n",
       "\n",
       "##### 18. Which of the following statements about batch normalization are correct?  \n",
       "**Correct:** A) It normalizes layer outputs to zero mean and unit variance within a mini-batch  \n",
       "C) It can be applied before or after the activation function, but before is more common  \n",
       "**Explanation:** Batch norm normalizes activations per mini-batch and is usually applied before activation.\n",
       "\n",
       "\n",
       "\n",
       "##### 19. What happens if dropout is applied during testing?  \n",
       "**Correct:** B) The output becomes unstable and inconsistent  \n",
       "**Explanation:** Dropout is only for training; applying it during testing causes unpredictable outputs.\n",
       "\n",
       "\n",
       "\n",
       "##### 20. Which of the following are true about the degradation problem and ResNet‚Äôs solution?  \n",
       "**Correct:** A) Degradation occurs because deeper layers fail to learn identity mappings  \n",
       "C) ResNet allows layers to learn residuals, which can be zero if identity is optimal  \n",
       "**Explanation:** Degradation arises when layers cannot learn identity; ResNet‚Äôs residual learning helps overcome this.\n",
       "\n",
       "\n",
       "\n",
       "If you want, I can also provide detailed explanations or references for any specific question!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from source.scripts import render\n",
    "render(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea7ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
