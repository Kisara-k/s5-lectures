{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cbf6fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 2\\. Loss Functions Regularization\n",
       "\n",
       "Certainly! Below is a detailed, well-organized study note based on your lecture content about **Loss Functions and Regularization**. The explanations are clear and thorough, designed to help you understand the key concepts step-by-step.\n",
       "\n",
       "## Study Notes on Loss Functions Regularization **üìö**\n",
       "\n",
       "### 1\\. üéØ Introduction to Loss Functions and Regularization\n",
       "\n",
       "In machine learning, especially in classification problems, **loss functions** and **regularization** are fundamental concepts that help us train models effectively. Loss functions measure how well our model‚Äôs predictions match the actual data, while regularization techniques help prevent the model from overfitting, ensuring it performs well on new, unseen data.\n",
       "\n",
       "This note will cover:\n",
       "\n",
       "*   Loss functions for binary and multi-class classification\n",
       "*   Why some loss functions work better than others\n",
       "*   The concept and importance of regularization\n",
       "*   Different types of regularization and their effects\n",
       "\n",
       "### 2\\. ‚öñÔ∏è Loss Functions for Classification Problems\n",
       "\n",
       "When training a classification model, the goal is to make predictions as close as possible to the true class labels. To do this, we need a way to measure the difference between the predicted outputs and the actual labels. This measurement is done using **loss functions** (also called error functions).\n",
       "\n",
       "##### Binary Classification Setup\n",
       "\n",
       "*   We have two classes, usually labeled as 0 and 1.\n",
       "*   For each data sample ( n ), the true class label is ( t\\_n ), which is either 0 or 1.\n",
       "*   The model outputs a prediction ( y\\_n ), which represents the probability that the sample belongs to class 1, i.e., ( y\\_n = P(t\\_n = 1 | \\\\text{inputs}\\_n) ).\n",
       "\n",
       "The goal is to train the model so that ( y\\_n ) is as close as possible to the true probability of the class label.\n",
       "\n",
       "### 3\\. üßÆ Loss Functions for Binary Classification\n",
       "\n",
       "##### Sigmoid Activation and Probability Output\n",
       "\n",
       "*   The output neuron often uses a **sigmoid activation function**, which squashes any input into a value between 0 and 1.\n",
       "*   This output can be interpreted as a probability, making it suitable for binary classification.\n",
       "\n",
       "##### Common Loss Functions\n",
       "\n",
       "3.1 Squared Error (SE)\n",
       "\n",
       "*   The squared error loss measures the square of the difference between the predicted probability ( y\\_n ) and the true label ( t\\_n ).\n",
       "*   Formula for a single sample:  \n",
       "    $$  \n",
       "    SE = (t\\_n - y\\_n)^2  \n",
       "    $$\n",
       "*   While simple, squared error is **not ideal for classification** because:\n",
       "    *   It treats errors symmetrically, which is not always appropriate for probabilities.\n",
       "    *   It is sensitive to outliers, which can disproportionately affect training.\n",
       "\n",
       "3.2 Cross-Entropy Loss (Logistic Loss)\n",
       "\n",
       "*   Cross-entropy measures the difference between two probability distributions: the true distribution ( p ) (actual labels) and the predicted distribution ( q ) (model outputs).\n",
       "*   It is defined as:  \n",
       "    $$  \n",
       "    H(p, q) = - \\\\sum p(x) \\\\log q(x)  \n",
       "    $$\n",
       "*   Intuitively, cross-entropy is **low when the predicted probabilities are close to the true labels** and high when they are far apart.\n",
       "*   For binary classification, the cross-entropy loss for a single sample is:  \n",
       "    $$  \n",
       "    CE = - \\\\left( t\\_n \\\\log y\\_n + (1 - t\\_n) \\\\log (1 - y\\_n) \\\\right)  \n",
       "    $$\n",
       "*   This loss function is preferred because it:\n",
       "    *   Aligns well with the probabilistic interpretation of the output.\n",
       "    *   Penalizes wrong confident predictions more heavily.\n",
       "    *   Leads to better convergence during training.\n",
       "\n",
       "### 4\\. üîÑ Cross-Entropy Intuition with an Example\n",
       "\n",
       "Imagine tossing a coin:\n",
       "\n",
       "*   ( X ) can be Heads (H) or Tails (T).\n",
       "*   Suppose ( p ) is the true distribution of a fair coin: ( p(H) = 0.5 ), ( p(T) = 0.5 ).\n",
       "*   Suppose ( q ) is the predicted distribution of a biased coin: ( q(H) = 0.7 ), ( q(T) = 0.3 ).\n",
       "\n",
       "Cross-entropy measures how different ( q ) is from ( p ). If ( q ) matches ( p ) exactly (both fair coins), cross-entropy is minimal. If ( q ) is very different (biased coin), cross-entropy is higher.\n",
       "\n",
       "This analogy helps understand why cross-entropy is a good loss function for classification: it measures how well the predicted probabilities match the true labels.\n",
       "\n",
       "### 5\\. üß© Multi-Class Classification and Softmax\n",
       "\n",
       "When there are more than two classes, the problem becomes **multi-class classification**.\n",
       "\n",
       "*   Each sample belongs to exactly one class out of ( K ) classes.\n",
       "*   The model outputs a vector of scores, one for each class.\n",
       "*   To convert these scores into probabilities, we use the **softmax function**.\n",
       "\n",
       "##### Softmax Function\n",
       "\n",
       "*   Softmax converts raw scores ( z\\_i ) into probabilities ( y\\_i ) that sum to 1:  \n",
       "    $$  \n",
       "    y\\_i = \\\\frac{e^{z\\_i}}{\\\\sum\\_{j=1}^K e^{z\\_j}}  \n",
       "    $$\n",
       "*   This ensures the output can be interpreted as a probability distribution over classes.\n",
       "\n",
       "##### Cross-Entropy Loss for Multi-Class\n",
       "\n",
       "*   The cross-entropy loss generalizes to multi-class by comparing the true class distribution (usually a one-hot vector) with the predicted softmax probabilities.\n",
       "*   For a single sample with true class ( c ), the loss is:  \n",
       "    $$  \n",
       "    CE = - \\\\log y\\_c  \n",
       "    $$\n",
       "*   This penalizes the model if the predicted probability for the true class is low.\n",
       "\n",
       "### 6\\. üõ°Ô∏è Regularization: Why and How?\n",
       "\n",
       "##### What is Regularization?\n",
       "\n",
       "Regularization is a set of techniques used to improve a model‚Äôs ability to generalize to new, unseen data. It helps prevent **overfitting**, where the model performs very well on training data but poorly on test data.\n",
       "\n",
       "*   Overfitting happens when the model learns noise or irrelevant details in the training data.\n",
       "*   Regularization encourages the model to be simpler and more robust.\n",
       "\n",
       "##### How Regularization Works\n",
       "\n",
       "*   Instead of minimizing just the loss ( J(w) ) (where ( w ) are the model parameters), we minimize:  \n",
       "    $$  \n",
       "    J(w) + \\\\lambda \\\\Omega(w)  \n",
       "    $$\n",
       "*   Here:\n",
       "    *   ( \\\\Omega(w) ) is a penalty term that measures the complexity of the model parameters.\n",
       "    *   ( \\\\lambda ) is a hyperparameter controlling the strength of regularization.\n",
       "\n",
       "### 7\\. üßÆ Types of Regularization Penalties\n",
       "\n",
       "##### 7.1 L2 Regularization (Ridge)\n",
       "\n",
       "*   The penalty is the sum of the squares of the weights:  \n",
       "    $$  \n",
       "    \\\\Omega(w) = \\\\sum\\_i w\\_i^2  \n",
       "    $$\n",
       "*   This encourages weights to be small but rarely exactly zero.\n",
       "*   It adds **numerical stability** especially when data is high-dimensional or poorly conditioned.\n",
       "*   Geometrically, it shrinks parameters towards zero smoothly.\n",
       "\n",
       "##### 7.2 L1 Regularization (Lasso)\n",
       "\n",
       "*   The penalty is the sum of the absolute values of the weights:  \n",
       "    $$  \n",
       "    \\\\Omega(w) = \\\\sum\\_i |w\\_i|  \n",
       "    $$\n",
       "*   This encourages sparsity, meaning many weights become exactly zero.\n",
       "*   It effectively performs **feature selection** by eliminating unimportant features.\n",
       "*   Geometrically, it tends to push parameters to the axes, creating sparse solutions.\n",
       "\n",
       "### 8\\. ‚öñÔ∏è Comparing L1 and L2 Regularization\n",
       "\n",
       "*   Both L1 and L2 regularization aim to simplify the model by reducing the magnitude of parameters.\n",
       "*   **L2 regularization** tends to shrink weights evenly, keeping all features but with smaller influence.\n",
       "*   **L1 regularization** aggressively pushes some weights to zero, effectively removing those features from the model.\n",
       "*   Choice depends on the problem:\n",
       "    *   Use L1 if you want a sparse model (feature selection).\n",
       "    *   Use L2 if you want to keep all features but reduce overfitting.\n",
       "\n",
       "### 9\\. üìà Summary and Practical Tips\n",
       "\n",
       "*   For **binary classification**, use **cross-entropy loss** with sigmoid activation for better performance.\n",
       "*   For **multi-class classification**, use **softmax activation** with cross-entropy loss.\n",
       "*   Avoid using squared error loss for classification tasks because it is less effective.\n",
       "*   Always consider **regularization** to improve generalization:\n",
       "    *   Use **L2 regularization** for smooth shrinkage of weights.\n",
       "    *   Use **L1 regularization** for sparse models and feature selection.\n",
       "*   Tune the regularization strength ( \\\\lambda ) carefully to balance bias and variance.\n",
       "\n",
       "### 10\\. üé• Additional Resources\n",
       "\n",
       "For deeper understanding, you can watch these video lectures:\n",
       "\n",
       "*   [Loss Functions and Regularization (YouTube)](https://youtu.be/hrIad1RVFV0?list=PLE6Wd9F_R--EdyJ5lbFl8UuGjecvVw66F6&t=2116)\n",
       "*   [Cross-Entropy Explained](https://www.youtube.com/watch?v=PKXpaLUi_gA8)\n",
       "*   [Regularization Techniques](https://www.youtube.com/watch?v=KvtGD37Rm5I&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=40)\n",
       "\n",
       "If you want, I can also help you with examples or coding implementations of these concepts!\n",
       "\n",
       "## Questions\n",
       "\n",
       "Certainly! Here are 20 multiple-choice questions (MCQs) that comprehensively cover the topic of **Loss Functions and Regularization** based on the lecture content. Some questions have one correct answer, others may have multiple correct answers to challenge understanding.\n",
       "\n",
       "##### 1\\. In binary classification, what does the output ( y\\_n ) of a model with a sigmoid activation represent?\n",
       "\n",
       "A) The raw score before activation  \n",
       "B) The probability that the input belongs to class 1  \n",
       "C) The predicted class label (0 or 1)  \n",
       "D) The error between prediction and true label\n",
       "\n",
       "##### 2\\. Why is squared error loss generally not preferred for binary classification problems?\n",
       "\n",
       "A) It is computationally expensive  \n",
       "B) It treats errors symmetrically, which is not ideal for probabilities  \n",
       "C) It is sensitive to outliers  \n",
       "D) It cannot be used with sigmoid activation\n",
       "\n",
       "##### 3\\. Cross-entropy loss is preferred over squared error in classification because:\n",
       "\n",
       "A) It penalizes confident wrong predictions more heavily  \n",
       "B) It measures the difference between two probability distributions  \n",
       "C) It always produces lower loss values  \n",
       "D) It aligns well with the probabilistic interpretation of outputs\n",
       "\n",
       "##### 4\\. The cross-entropy loss for a single binary classification sample with true label ( t\\_n ) and predicted probability ( y\\_n ) is:\n",
       "\n",
       "A) ( (t\\_n - y\\_n)^2 )  \n",
       "B) ( - \\\\left( t\\_n \\\\log y\\_n + (1 - t\\_n) \\\\log (1 - y\\_n) \\\\right) )  \n",
       "C) ( \\\\log(y\\_n) )  \n",
       "D) ( y\\_n - t\\_n )\n",
       "\n",
       "##### 5\\. What is the main purpose of the softmax function in multi-class classification?\n",
       "\n",
       "A) To normalize raw scores into probabilities that sum to 1  \n",
       "B) To select the class with the highest raw score  \n",
       "C) To convert probabilities into binary labels  \n",
       "D) To reduce overfitting\n",
       "\n",
       "##### 6\\. Which of the following statements about softmax activation is true?\n",
       "\n",
       "A) It outputs values between 0 and 1 for each class independently  \n",
       "B) The sum of all softmax outputs for a sample equals 1  \n",
       "C) It is used only in binary classification  \n",
       "D) It can be combined with cross-entropy loss for multi-class problems\n",
       "\n",
       "##### 7\\. In the context of regularization, what does the term ( \\\\lambda ) represent?\n",
       "\n",
       "A) The model‚Äôs learning rate  \n",
       "B) The strength of the regularization penalty  \n",
       "C) The number of model parameters  \n",
       "D) The error on the training set\n",
       "\n",
       "##### 8\\. Which of the following best describes the effect of L2 regularization on model parameters?\n",
       "\n",
       "A) It pushes some parameters exactly to zero  \n",
       "B) It shrinks all parameters smoothly towards zero  \n",
       "C) It increases the magnitude of parameters  \n",
       "D) It removes irrelevant features completely\n",
       "\n",
       "##### 9\\. How does L1 regularization differ from L2 regularization?\n",
       "\n",
       "A) L1 tends to produce sparse models by setting some weights to zero  \n",
       "B) L2 tends to produce sparse models by setting some weights to zero  \n",
       "C) L1 penalizes the sum of absolute values of weights  \n",
       "D) L2 penalizes the sum of squared weights\n",
       "\n",
       "##### 10\\. Which of the following are benefits of using regularization in machine learning models?\n",
       "\n",
       "A) Reduces overfitting  \n",
       "B) Improves generalization to unseen data  \n",
       "C) Guarantees zero training error  \n",
       "D) Helps in feature selection (for some types)\n",
       "\n",
       "##### 11\\. What happens if the regularization parameter ( \\\\lambda ) is set too high?\n",
       "\n",
       "A) The model may underfit the data  \n",
       "B) The model may overfit the data  \n",
       "C) The model parameters may become too large  \n",
       "D) The training error will always be zero\n",
       "\n",
       "##### 12\\. Cross-entropy loss between two probability distributions ( p ) and ( q ) is:\n",
       "\n",
       "A) Low when ( p ) and ( q ) are similar  \n",
       "B) High when ( p ) and ( q ) are similar  \n",
       "C) Independent of the similarity between ( p ) and ( q )  \n",
       "D) Used to measure the distance between predicted and true distributions\n",
       "\n",
       "##### 13\\. Which of the following are true about the sigmoid activation function?\n",
       "\n",
       "A) It outputs values strictly between 0 and 1  \n",
       "B) It is suitable for producing probabilities in binary classification  \n",
       "C) It outputs discrete class labels directly  \n",
       "D) It can cause vanishing gradient problems in deep networks\n",
       "\n",
       "##### 14\\. Why is cross-entropy loss considered a better fit for classification than squared error?\n",
       "\n",
       "A) Because it is differentiable  \n",
       "B) Because it models the likelihood of the data under a Bernoulli distribution  \n",
       "C) Because it penalizes wrong confident predictions more than small errors  \n",
       "D) Because it is easier to compute\n",
       "\n",
       "##### 15\\. In multi-class classification, what does the cross-entropy loss encourage the model to do?\n",
       "\n",
       "A) Maximize the predicted probability of the true class  \n",
       "B) Minimize the predicted probability of all classes  \n",
       "C) Assign equal probability to all classes  \n",
       "D) Maximize the sum of predicted probabilities\n",
       "\n",
       "##### 16\\. Which of the following statements about overfitting is correct?\n",
       "\n",
       "A) Overfitting means the model performs well on training data but poorly on test data  \n",
       "B) Overfitting means the model performs poorly on both training and test data  \n",
       "C) Regularization techniques help reduce overfitting  \n",
       "D) Increasing model complexity always reduces overfitting\n",
       "\n",
       "##### 17\\. What is the geometric interpretation of L1 regularization compared to L2?\n",
       "\n",
       "A) L1 regularization contours are diamond-shaped, encouraging sparsity  \n",
       "B) L2 regularization contours are circular, encouraging smooth shrinkage  \n",
       "C) L1 regularization contours are circular  \n",
       "D) L2 regularization contours are diamond-shaped\n",
       "\n",
       "##### 18\\. Which of the following loss functions can be used with a sigmoid output neuron?\n",
       "\n",
       "A) Squared error loss  \n",
       "B) Cross-entropy loss  \n",
       "C) Hinge loss  \n",
       "D) Mean absolute error\n",
       "\n",
       "##### 19\\. In the context of classification, what does it mean for a model to generalize well?\n",
       "\n",
       "A) It achieves low error on training data only  \n",
       "B) It performs well on new, unseen data  \n",
       "C) It has zero training error  \n",
       "D) It uses complex models with many parameters\n",
       "\n",
       "##### 20\\. Which of the following are true about the relationship between loss functions and activation functions?\n",
       "\n",
       "A) The choice of loss function depends on the activation function used in the output layer  \n",
       "B) Cross-entropy loss pairs naturally with sigmoid or softmax activations  \n",
       "C) Squared error loss is always better regardless of activation  \n",
       "D) Softmax activation requires a compatible loss function like cross-entropy for effective training\n",
       "\n",
       "If you want, I can also provide answer keys or explanations for these questions!\n",
       "\n",
       "## Answers\n",
       "\n",
       "Certainly! Below are the correct answers for each question along with brief explanations.\n",
       "\n",
       "##### 1\\. In binary classification, what does the output ( y\\_n ) of a model with a sigmoid activation represent?\n",
       "\n",
       "**Correct Answer:**  \n",
       "B) The probability that the input belongs to class 1\n",
       "\n",
       "**Explanation:**  \n",
       "The sigmoid activation outputs a value between 0 and 1, which can be interpreted as the probability that the input belongs to class 1.\n",
       "\n",
       "##### 2\\. Why is squared error loss generally not preferred for binary classification problems?\n",
       "\n",
       "**Correct Answers:**  \n",
       "B) It treats errors symmetrically, which is not ideal for probabilities  \n",
       "C) It is sensitive to outliers\n",
       "\n",
       "**Explanation:**  \n",
       "Squared error treats deviations equally regardless of direction, which is not ideal for probability outputs. It is also sensitive to outliers, which can distort training.\n",
       "\n",
       "##### 3\\. Cross-entropy loss is preferred over squared error in classification because:\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) It penalizes confident wrong predictions more heavily  \n",
       "B) It measures the difference between two probability distributions  \n",
       "D) It aligns well with the probabilistic interpretation of outputs\n",
       "\n",
       "**Explanation:**  \n",
       "Cross-entropy compares predicted and true probability distributions and penalizes wrong confident predictions more, making it better suited for classification.\n",
       "\n",
       "##### 4\\. The cross-entropy loss for a single binary classification sample with true label ( t\\_n ) and predicted probability ( y\\_n ) is:\n",
       "\n",
       "**Correct Answer:**  \n",
       "B) ( - \\\\left( t\\_n \\\\log y\\_n + (1 - t\\_n) \\\\log (1 - y\\_n) \\\\right) )\n",
       "\n",
       "**Explanation:**  \n",
       "This is the standard formula for binary cross-entropy loss.\n",
       "\n",
       "##### 5\\. What is the main purpose of the softmax function in multi-class classification?\n",
       "\n",
       "**Correct Answer:**  \n",
       "A) To normalize raw scores into probabilities that sum to 1\n",
       "\n",
       "**Explanation:**  \n",
       "Softmax converts raw output scores into a probability distribution over classes, ensuring all probabilities sum to 1.\n",
       "\n",
       "##### 6\\. Which of the following statements about softmax activation is true?\n",
       "\n",
       "**Correct Answers:**  \n",
       "B) The sum of all softmax outputs for a sample equals 1  \n",
       "D) It can be combined with cross-entropy loss for multi-class problems\n",
       "\n",
       "**Explanation:**  \n",
       "Softmax outputs sum to 1, making it suitable for multi-class classification, and it pairs naturally with cross-entropy loss.\n",
       "\n",
       "##### 7\\. In the context of regularization, what does the term ( \\\\lambda ) represent?\n",
       "\n",
       "**Correct Answer:**  \n",
       "B) The strength of the regularization penalty\n",
       "\n",
       "**Explanation:**  \n",
       "( \\\\lambda ) controls how much the regularization term influences the total loss.\n",
       "\n",
       "##### 8\\. Which of the following best describes the effect of L2 regularization on model parameters?\n",
       "\n",
       "**Correct Answer:**  \n",
       "B) It shrinks all parameters smoothly towards zero\n",
       "\n",
       "**Explanation:**  \n",
       "L2 regularization penalizes large weights by adding the sum of their squares, encouraging smaller but non-zero weights.\n",
       "\n",
       "##### 9\\. How does L1 regularization differ from L2 regularization?\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) L1 tends to produce sparse models by setting some weights to zero  \n",
       "C) L1 penalizes the sum of absolute values of weights  \n",
       "D) L2 penalizes the sum of squared weights\n",
       "\n",
       "**Explanation:**  \n",
       "L1 uses absolute values and encourages sparsity; L2 uses squares and encourages smooth shrinkage.\n",
       "\n",
       "##### 10\\. Which of the following are benefits of using regularization in machine learning models?\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) Reduces overfitting  \n",
       "B) Improves generalization to unseen data  \n",
       "D) Helps in feature selection (for some types)\n",
       "\n",
       "**Explanation:**  \n",
       "Regularization reduces overfitting and improves generalization; L1 regularization can also perform feature selection.\n",
       "\n",
       "##### 11\\. What happens if the regularization parameter ( \\\\lambda ) is set too high?\n",
       "\n",
       "**Correct Answer:**  \n",
       "A) The model may underfit the data\n",
       "\n",
       "**Explanation:**  \n",
       "Too much regularization forces weights to be too small or zero, causing the model to be too simple and underfit.\n",
       "\n",
       "##### 12\\. Cross-entropy loss between two probability distributions ( p ) and ( q ) is:\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) Low when ( p ) and ( q ) are similar  \n",
       "D) Used to measure the distance between predicted and true distributions\n",
       "\n",
       "**Explanation:**  \n",
       "Cross-entropy quantifies how close two distributions are; it is low when they are similar.\n",
       "\n",
       "##### 13\\. Which of the following are true about the sigmoid activation function?\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) It outputs values strictly between 0 and 1  \n",
       "B) It is suitable for producing probabilities in binary classification  \n",
       "D) It can cause vanishing gradient problems in deep networks\n",
       "\n",
       "**Explanation:**  \n",
       "Sigmoid outputs probabilities but can cause vanishing gradients in deep networks.\n",
       "\n",
       "##### 14\\. Why is cross-entropy loss considered a better fit for classification than squared error?\n",
       "\n",
       "**Correct Answers:**  \n",
       "B) Because it models the likelihood of the data under a Bernoulli distribution  \n",
       "C) Because it penalizes wrong confident predictions more than small errors\n",
       "\n",
       "**Explanation:**  \n",
       "Cross-entropy corresponds to maximizing likelihood under Bernoulli assumptions and penalizes confident wrong predictions heavily.\n",
       "\n",
       "##### 15\\. In multi-class classification, what does the cross-entropy loss encourage the model to do?\n",
       "\n",
       "**Correct Answer:**  \n",
       "A) Maximize the predicted probability of the true class\n",
       "\n",
       "**Explanation:**  \n",
       "Cross-entropy loss pushes the model to assign high probability to the correct class.\n",
       "\n",
       "##### 16\\. Which of the following statements about overfitting is correct?\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) Overfitting means the model performs well on training data but poorly on test data  \n",
       "C) Regularization techniques help reduce overfitting\n",
       "\n",
       "**Explanation:**  \n",
       "Overfitting is when the model memorizes training data but fails to generalize; regularization helps prevent this.\n",
       "\n",
       "##### 17\\. What is the geometric interpretation of L1 regularization compared to L2?\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) L1 regularization contours are diamond-shaped, encouraging sparsity  \n",
       "B) L2 regularization contours are circular, encouraging smooth shrinkage\n",
       "\n",
       "**Explanation:**  \n",
       "L1‚Äôs diamond-shaped contours encourage solutions on axes (sparse), L2‚Äôs circular contours shrink weights evenly.\n",
       "\n",
       "##### 18\\. Which of the following loss functions can be used with a sigmoid output neuron?\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) Squared error loss  \n",
       "B) Cross-entropy loss\n",
       "\n",
       "**Explanation:**  \n",
       "Both squared error and cross-entropy can be used with sigmoid outputs, but cross-entropy is preferred.\n",
       "\n",
       "##### 19\\. In the context of classification, what does it mean for a model to generalize well?\n",
       "\n",
       "**Correct Answer:**  \n",
       "B) It performs well on new, unseen data\n",
       "\n",
       "**Explanation:**  \n",
       "Generalization means the model‚Äôs performance extends beyond the training data to new data.\n",
       "\n",
       "##### 20\\. Which of the following are true about the relationship between loss functions and activation functions?\n",
       "\n",
       "**Correct Answers:**  \n",
       "A) The choice of loss function depends on the activation function used in the output layer  \n",
       "B) Cross-entropy loss pairs naturally with sigmoid or softmax activations  \n",
       "D) Softmax activation requires a compatible loss function like cross-entropy for effective training\n",
       "\n",
       "**Explanation:**  \n",
       "Loss functions and output activations must be compatible for effective training; cross-entropy pairs well with sigmoid and softmax.\n",
       "\n",
       "If you want, I can also help create explanations for incorrect options or provide more detailed examples!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from source.scripts import render\n",
    "render(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7542c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
