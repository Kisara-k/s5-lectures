## 8. Recurrent Neural Networks Part 2

Certainly! Below is a **detailed, well-organized, and clear study note** based on your lecture content on Recurrent Neural Networks (RNNs) Part 2. I have expanded explanations, clarified key concepts, and structured the note with numbered main headings and emojis for clarity.



## Study Notes: Recurrent Neural Networks Part 2 üìö



### 1. üîÑ Recap of Recurrent Neural Networks (RNNs)

To understand the advanced concepts in this lecture, let's first revisit the basics of RNNs.

RNNs are a type of neural network designed specifically for **sequential data** ‚Äî data where the order of inputs matters, such as text, speech, or time series. Unlike traditional neural networks that treat inputs independently, RNNs **remember information from previous inputs** to influence the current output. This memory is stored in what we call the **hidden state**.

#### Why do we need RNNs for sequences?

When analyzing sequences, the current input often depends on previous inputs. For example, in language, the meaning of a word depends on the words before it. To capture this, RNNs use **recursion** ‚Äî they process data step-by-step, passing information forward through time.

#### How do RNNs work mathematically?

At each time step \( t \), the RNN updates its hidden state \( h_t \) based on the current input \( x_t \) and the previous hidden state \( h_{t-1} \):

\[
h_t = \tanh(W_h h_{t-1} + W_x x_t)
\]

Here, \( W_h \) and \( W_x \) are weight matrices learned during training, and \( \tanh \) is a nonlinear activation function that squashes values between -1 and 1.

The output probability for the next element in the sequence \( y_{t+1} \) is computed using a softmax function:

\[
P(y_{t+1}) = \text{Softmax}(W_y h_t)
\]

The hidden state \( h_t \) acts as a **summary of all past inputs** up to time \( t \). The initial hidden state \( h_0 \) is usually set to zero or learned.

#### Types of RNN architectures

- **One-to-Many:** One input, multiple outputs (e.g., image captioning).
- **Many-to-One:** Multiple inputs, one output (e.g., sentiment analysis).
- **Many-to-Many:** Multiple inputs, multiple outputs (e.g., machine translation).

#### Bidirectional RNNs

Sometimes, the output at time \( t \) depends not only on past inputs but also on future inputs. For example, in speech recognition, understanding a sound might require knowing the next few sounds. **Bidirectional RNNs** address this by processing the sequence both forward and backward, combining information from past and future.



### 2. ‚ö†Ô∏è Gradient Problems in RNNs: Vanishing and Exploding Gradients

Training RNNs involves backpropagation through time (BPTT), where gradients are propagated backward through many time steps. However, this process often suffers from two major problems:

- **Vanishing gradients:** Gradients become very small as they are multiplied repeatedly through time steps, causing the network to stop learning long-range dependencies.
- **Exploding gradients:** Gradients grow exponentially large, causing unstable updates.

Mathematically, the gradient of the loss with respect to weights involves a product of many terms like \( \frac{\partial h_t}{\partial h_{t-1}} \), which can be less than 1 (leading to vanishing) or greater than 1 (leading to exploding).

This limits vanilla RNNs' ability to learn from long sequences because the influence of early inputs fades away or becomes unstable.



### 3. üß† Long Short-Term Memory (LSTM) Networks

To solve the gradient problems, **LSTM networks** were introduced. LSTMs are a special kind of RNN designed to **remember information for long periods** and **allow gradients to flow more easily** during training.

#### What makes LSTMs different?

Unlike vanilla RNNs that use a single layer with a simple tanh activation, LSTMs have a more complex repeating module with **four interacting layers** and **gates** that control the flow of information.

#### The key component: Cell State

The LSTM introduces a **cell state** \( C_t \), which acts like a conveyor belt running through the sequence with only minor linear interactions. This design allows information to flow unchanged unless explicitly modified, enabling the network to maintain long-term memory.

#### Gates in LSTM

Gates are sigmoid-activated layers that output values between 0 and 1, deciding how much information to let through. There are three main gates:

- **Forget gate:** Decides what information to discard from the cell state.
- **Input gate:** Decides what new information to add to the cell state.
- **Output gate:** Decides what information from the cell state to output as the hidden state.



### 4. üîç Step-by-Step Walkthrough of LSTM Operation

Let's understand how an LSTM processes information using a simple example:

> "Saman studies at UOM. His sister studies at UOC. She studies law."

#### Step 1: Forget Gate

The forget gate looks at the previous hidden state \( h_{t-1} \) and the current input \( x_t \) and outputs a number between 0 and 1 for each element in the previous cell state \( C_{t-1} \). A value of 1 means "keep this information," and 0 means "forget it."

For example, when the LSTM sees the word "sister," it might decide to forget information related to "Saman" because the focus shifts.

#### Step 2: Input Gate and Candidate Values

The input gate decides which new information to add to the cell state. It works together with a **tanh layer** that creates a vector of candidate values \( \tilde{C}_t \) that could be added.

In our example, the LSTM might add the gender information of "sister" to replace the old information it is forgetting.

#### Step 3: Update Cell State

The new cell state \( C_t \) is updated by combining the old cell state filtered by the forget gate and the new candidate values filtered by the input gate:

\[
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
\]

This additive update helps maintain gradient flow and long-term memory.

#### Step 4: Output Gate and Hidden State

Finally, the output gate decides what part of the cell state to output as the hidden state \( h_t \):

\[
h_t = o_t * \tanh(C_t)
\]

The \( \tanh \) function filters the cell state to highlight significant information (values close to ¬±1) and suppress minor details (values near 0).

In the example, after processing "sister," the LSTM might output whether the subject is singular or plural, which is useful for downstream tasks.



### 5. üîÑ Gradient Flow in LSTM

Thanks to the additive nature of the cell state updates, LSTMs allow gradients to flow more smoothly through time, avoiding the vanishing gradient problem that plagues vanilla RNNs.

The gradient of the loss with respect to the weights involves terms like:

\[
\frac{\partial C_t}{\partial C_{t-1}} = A_t + B_t + D_t + E_t
\]

This complex but stable gradient flow enables LSTMs to learn long-term dependencies effectively.



### 6. üîÄ Variants of LSTM: Gated Recurrent Unit (GRU)

Another popular variant of LSTM is the **Gated Recurrent Unit (GRU)**, which simplifies the LSTM architecture by:

- Combining the forget and input gates into a single **update gate** \( z_t \).
- Merging the cell state and hidden state into one.
- Using a **reset gate** \( r_t \) to control how much past information to forget.

GRUs have fewer parameters and often train faster than LSTMs, while achieving similar performance on many tasks.



### 7. üîç Beam Search: Improving Sequence Generation

When generating sequences (like text), simply picking the most likely next word at each step can lead to suboptimal results. **Beam search** is a heuristic search algorithm that keeps track of multiple candidate sequences (beams) at each step, exploring several possibilities before choosing the best overall sequence.

This approach balances between greedy search (fast but shortsighted) and exhaustive search (slow but thorough), improving the quality of generated sequences.



### 8. üéØ Attention Mechanism: Focusing on Important Parts

RNNs and LSTMs try to encode the entire input sequence into a fixed-size hidden state, which can be problematic for long sequences. The **attention mechanism** addresses this by allowing the model to focus on specific parts of the input when generating each output.

#### How does attention work?

For each output word \( y_j \), the model computes a set of **attention weights** \( \alpha_{ij} \) that indicate how much attention to pay to each input hidden state \( h_i \). These weights are normalized to sum to 1 and are used to compute a **context vector** \( C_j \), a weighted sum of the input states:

\[
C_j = \sum_i \alpha_{ij} h_i
\]

This context vector helps the model focus on relevant parts of the input when producing each output word.

#### Computing attention weights

Attention weights are computed using a small neural network (often a feedforward network) that scores the relevance of each input hidden state to the current output step. The scores are passed through a softmax to get normalized weights.

#### Example: Machine Translation

When translating a sentence from English to French, instead of encoding the entire English sentence into one vector, the attention mechanism allows the model to look at different English words when generating each French word, improving translation quality, especially for long sentences.



### 9. ‚ö†Ô∏è Limitations of LSTM and the Rise of Transformers

Despite their strengths, LSTMs have some limitations:

- They struggle with **very long sequences** because even with gates, the memory is limited.
- They cannot be trained in parallel efficiently because of their sequential nature.

To overcome these issues, newer architectures like **Transformers** have been developed. Transformers rely entirely on attention mechanisms and allow for parallel processing of sequences, leading to state-of-the-art performance in many sequence modeling tasks.



## Summary

- RNNs are designed for sequential data but suffer from vanishing/exploding gradients.
- LSTMs solve these problems using cell states and gates to control information flow.
- GRUs simplify LSTMs with fewer gates and parameters.
- Beam search improves sequence generation by exploring multiple candidate outputs.
- Attention mechanisms allow models to focus on relevant parts of the input, improving performance on long sequences.
- Transformers are the next step beyond LSTMs, enabling parallel training and better handling of long sequences.



If you want, I can also provide diagrams or code examples to further clarify these concepts!

## Questions

Certainly! Here are 20 multiple-choice questions (MCQs) covering the full scope of the lecture content on Recurrent Neural Networks Part 2. Some questions have one correct answer, others may have multiple correct answers. Answers are **not marked** so you can use these for self-testing or quizzes.



#### 1. Which of the following statements about vanilla RNNs are true?  
A) They process sequential data by maintaining a hidden state that summarizes past inputs.  
B) They use gates to control information flow through time.  
C) They suffer from vanishing and exploding gradient problems during training.  
D) They can easily learn long-term dependencies without any modifications.



#### 2. The hidden state \( h_t \) in an RNN at time step \( t \) is computed using which of the following?  
A) The current input \( x_t \) and the previous hidden state \( h_{t-1} \).  
B) Only the current input \( x_t \).  
C) The output at time \( t-1 \).  
D) The cell state \( C_t \).



#### 3. What is the main purpose of bidirectional RNNs?  
A) To process sequences in both forward and backward directions.  
B) To reduce the number of parameters in the model.  
C) To allow the output at time \( t \) to depend on both past and future inputs.  
D) To solve the vanishing gradient problem.



#### 4. Which of the following are causes of vanishing gradients in RNNs?  
A) Repeated multiplication of gradients less than 1 through many time steps.  
B) Use of sigmoid or tanh activation functions.  
C) Large weight matrices with values greater than 1.  
D) Additive updates to the cell state.



#### 5. How do LSTM networks primarily address the vanishing gradient problem?  
A) By using additive updates to the cell state instead of multiplicative updates.  
B) By removing the hidden state entirely.  
C) By introducing gates that control information flow.  
D) By using only linear activation functions.



#### 6. Which gates are present in a standard LSTM cell?  
A) Forget gate  
B) Input gate  
C) Output gate  
D) Reset gate



#### 7. What is the role of the forget gate in an LSTM?  
A) To decide which information to discard from the previous cell state.  
B) To decide which new information to add to the cell state.  
C) To control the output of the hidden state.  
D) To reset the hidden state to zero.



#### 8. In the LSTM update equation \( C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \), what do \( f_t \) and \( i_t \) represent?  
A) Forget gate output and input gate output, respectively.  
B) Input gate output and output gate output, respectively.  
C) Output gate output and forget gate output, respectively.  
D) Reset gate output and update gate output, respectively.



#### 9. Which of the following statements about the hidden state \( h_t \) in LSTM are true?  
A) It is computed by filtering the cell state \( C_t \) through a tanh function and multiplying by the output gate.  
B) It contains the long-term memory of the sequence.  
C) It is the same as the cell state \( C_t \).  
D) It is used to produce the output at time \( t \).



#### 10. How does the Gated Recurrent Unit (GRU) differ from the standard LSTM?  
A) It combines the forget and input gates into a single update gate.  
B) It merges the cell state and hidden state into one.  
C) It has more parameters than LSTM.  
D) It typically converges faster during training.



#### 11. Which of the following are true about beam search in sequence generation?  
A) It keeps track of multiple candidate sequences at each step.  
B) It always selects the single most probable next word at each step.  
C) It balances between greedy and exhaustive search.  
D) It guarantees finding the globally optimal sequence.



#### 12. What is the main limitation of vanilla RNNs when dealing with long sequences?  
A) They cannot process sequences longer than 10 time steps.  
B) They struggle to maintain information from distant past inputs due to vanishing gradients.  
C) They require bidirectional processing to work.  
D) They cannot be trained using backpropagation.



#### 13. The attention mechanism in sequence models helps by:  
A) Encoding the entire input sequence into a fixed-size vector.  
B) Allowing the model to focus on relevant parts of the input when generating each output.  
C) Computing a weighted sum of input hidden states based on learned attention weights.  
D) Eliminating the need for recurrent connections.



#### 14. How are attention weights \( \alpha_{ij} \) typically computed?  
A) Using a softmax over scores produced by a small neural network comparing output step \( j \) and input hidden state \( i \).  
B) By averaging all input hidden states.  
C) By multiplying the input and output embeddings directly.  
D) By using a fixed uniform distribution.



#### 15. Which of the following are challenges or limitations of LSTM networks?  
A) Difficulty handling very long sequences.  
B) Inability to train in parallel efficiently.  
C) They always outperform transformers on all tasks.  
D) Complexity due to multiple gates and parameters.



#### 16. Why are transformers considered an improvement over LSTMs?  
A) They rely entirely on attention mechanisms without recurrence.  
B) They allow parallel processing of sequences during training.  
C) They have fewer parameters than LSTMs.  
D) They solve the vanishing gradient problem by design.



#### 17. In the context of RNNs, what does the term "additive" processing mean?  
A) Updating the cell state by adding new information rather than multiplying.  
B) Adding the outputs of multiple RNN layers.  
C) Summing the gradients during backpropagation.  
D) Adding noise to the input at each time step.



#### 18. Which of the following best describes the function of the reset gate in a GRU?  
A) It determines how much of the previous hidden state to forget when computing the candidate state.  
B) It controls the output of the GRU cell.  
C) It merges the input and forget gates.  
D) It resets the entire network weights.



#### 19. In bidirectional RNNs, the hidden state at time \( t \) is computed by:  
A) Concatenating the forward hidden state and backward hidden state at time \( t \).  
B) Only using the forward hidden state.  
C) Only using the backward hidden state.  
D) Averaging the forward and backward hidden states.



#### 20. Which of the following are true about the softmax function used in RNN outputs?  
A) It converts raw scores into probabilities that sum to 1.  
B) It is applied to the hidden state directly.  
C) It is used to predict the next element in a sequence.  
D) It outputs values between -1 and 1.



These questions cover the core concepts of RNNs, LSTMs, GRUs, gradient issues, attention mechanisms, beam search, and the limitations leading to transformers. They include straightforward, tricky, and conceptual questions to test deep understanding.

## Answers

Certainly! Below are the **correct answers** for each question along with **brief explanations** to clarify why those answers are correct.



#### 1. Which of the following statements about vanilla RNNs are true?  
**Correct:** A, C  
**Explanation:**  
- (A) Vanilla RNNs maintain a hidden state that summarizes past inputs, enabling sequential data processing.  
- (C) They suffer from vanishing and exploding gradients during training, limiting their ability to learn long-term dependencies.  
- (B) is false because vanilla RNNs do **not** use gates; gates are introduced in LSTMs/GRUs.  
- (D) is false because vanilla RNNs struggle with long-term dependencies.



#### 2. The hidden state \( h_t \) in an RNN at time step \( t \) is computed using which of the following?  
**Correct:** A  
**Explanation:**  
The hidden state depends on the current input \( x_t \) and the previous hidden state \( h_{t-1} \). It does not depend solely on \( x_t \) or the output at \( t-1 \), and vanilla RNNs do not have a cell state \( C_t \).



#### 3. What is the main purpose of bidirectional RNNs?  
**Correct:** A, C  
**Explanation:**  
- (A) Bidirectional RNNs process sequences forward and backward.  
- (C) This allows outputs at time \( t \) to depend on both past and future inputs, useful in tasks like speech recognition.  
- (B) is false; bidirectional RNNs actually increase parameters.  
- (D) is false; they do not solve gradient problems directly.



#### 4. Which of the following are causes of vanishing gradients in RNNs?  
**Correct:** A, B  
**Explanation:**  
- (A) Repeated multiplication of gradients less than 1 causes gradients to shrink exponentially.  
- (B) Sigmoid and tanh activations squash values between 0 and 1, contributing to gradient shrinkage.  
- (C) Large weights >1 cause exploding gradients, not vanishing.  
- (D) Additive updates are a feature of LSTM, which help prevent vanishing gradients.



#### 5. How do LSTM networks primarily address the vanishing gradient problem?  
**Correct:** A, C  
**Explanation:**  
- (A) LSTMs use additive updates to the cell state, allowing gradients to flow unchanged.  
- (C) Gates control what information is kept or discarded, improving learning of long-term dependencies.  
- (B) and (D) are false; LSTMs keep hidden states and use nonlinearities.



#### 6. Which gates are present in a standard LSTM cell?  
**Correct:** A, B, C  
**Explanation:**  
Standard LSTM has three gates: forget, input, and output gates. The reset gate is part of GRUs, not LSTMs.



#### 7. What is the role of the forget gate in an LSTM?  
**Correct:** A  
**Explanation:**  
The forget gate decides which parts of the previous cell state to keep or discard. It does not add new information or control output directly.



#### 8. In the LSTM update equation \( C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \), what do \( f_t \) and \( i_t \) represent?  
**Correct:** A  
**Explanation:**  
\( f_t \) is the forget gate output controlling retention of old cell state, and \( i_t \) is the input gate output controlling how much new candidate information \( \tilde{C}_t \) to add.



#### 9. Which of the following statements about the hidden state \( h_t \) in LSTM are true?  
**Correct:** A, D  
**Explanation:**  
- (A) \( h_t \) is computed by filtering the cell state through tanh and modulating by the output gate.  
- (D) \( h_t \) is used to produce the output at time \( t \).  
- (B) is false because the cell state \( C_t \) holds long-term memory, not \( h_t \).  
- (C) is false; \( h_t \) and \( C_t \) are distinct.



#### 10. How does the Gated Recurrent Unit (GRU) differ from the standard LSTM?  
**Correct:** A, B, D  
**Explanation:**  
- (A) GRU combines forget and input gates into one update gate.  
- (B) GRU merges cell and hidden states.  
- (D) GRUs typically have fewer parameters and converge faster.  
- (C) is false; GRUs have fewer parameters than LSTMs.



#### 11. Which of the following are true about beam search in sequence generation?  
**Correct:** A, C  
**Explanation:**  
- (A) Beam search keeps multiple candidate sequences (beams) at each step.  
- (C) It balances between greedy (single best) and exhaustive search.  
- (B) is false; greedy search picks only the single best next word.  
- (D) is false; beam search is heuristic and does not guarantee global optimality.



#### 12. What is the main limitation of vanilla RNNs when dealing with long sequences?  
**Correct:** B  
**Explanation:**  
Vanilla RNNs struggle to maintain information from distant past inputs due to vanishing gradients. They can be trained with backpropagation and do not require bidirectional processing.



#### 13. The attention mechanism in sequence models helps by:  
**Correct:** B, C  
**Explanation:**  
- (B) Attention allows the model to focus on relevant parts of the input for each output.  
- (C) It computes a weighted sum of input hidden states using attention weights.  
- (A) is false; attention avoids encoding the entire input into a fixed vector.  
- (D) is false; attention is often combined with recurrence or transformers.



#### 14. How are attention weights \( \alpha_{ij} \) typically computed?  
**Correct:** A  
**Explanation:**  
Attention weights are computed by scoring relevance between output step \( j \) and input hidden state \( i \) using a small neural network, then normalized with softmax.



#### 15. Which of the following are challenges or limitations of LSTM networks?  
**Correct:** A, B, D  
**Explanation:**  
- (A) LSTMs still struggle with very long sequences.  
- (B) They cannot be efficiently trained in parallel due to sequential dependencies.  
- (D) LSTMs are complex with multiple gates and parameters.  
- (C) is false; transformers often outperform LSTMs.



#### 16. Why are transformers considered an improvement over LSTMs?  
**Correct:** A, B, D  
**Explanation:**  
- (A) Transformers rely entirely on attention, removing recurrence.  
- (B) They allow parallel processing of sequences during training.  
- (D) Their design avoids vanishing gradients by using attention instead of recurrence.  
- (C) is false; transformers often have more parameters.



#### 17. In the context of RNNs, what does the term "additive" processing mean?  
**Correct:** A  
**Explanation:**  
Additive processing refers to updating the cell state by adding new information rather than multiplying, which helps maintain gradient flow.



#### 18. Which of the following best describes the function of the reset gate in a GRU?  
**Correct:** A  
**Explanation:**  
The reset gate controls how much of the previous hidden state to forget when computing the candidate state, allowing the model to reset parts of its memory.



#### 19. In bidirectional RNNs, the hidden state at time \( t \) is computed by:  
**Correct:** A  
**Explanation:**  
The forward and backward hidden states at time \( t \) are concatenated to form the final hidden state, combining past and future context.



#### 20. Which of the following are true about the softmax function used in RNN outputs?  
**Correct:** A, C  
**Explanation:**  
- (A) Softmax converts raw scores into probabilities summing to 1.  
- (C) It is used to predict the next element in a sequence by outputting a probability distribution.  
- (B) is false; softmax is applied to the output layer, not directly to hidden states.  
- (D) is false; softmax outputs probabilities between 0 and 1, not between -1 and 1.



If you want, I can also help create explanations for incorrect options or provide more detailed examples!